{
	"study_id": "10.29333/iji.2025.18218a",
	"no": "101",
	"first_author_year": "Hao 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "China (private EFL university)",
	"llm_type_brief": "ChatGPT (unspecified version)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly specifies examining ChatGPT’s impact on seven dimensions of academic writing and four types of engagement within a constructivist and engagement-theory framework and addresses a defined gap in the Chinese private-university context.",
	"q2_participant_info_score": "1",
	"q2_participant_info_notes": "Participant Information.educational_level, discipline, age, sex, learning_context, and target_language show that the sample is third-year English majors aged 18–22 in a Chinese private university EFL academic writing course, but Participant Information.language_proficiency and Participant Information.mother_tongue are NR, so key language background variables are incomplete.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe cluster random sampling of two intact classes from nine third-year classes with N=70 (Methodology.sample_size_and_effect) and acknowledge limited generalizability, but Methodology.power_analysis is NR and no explicit sample-size justification is provided.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method reports two intact classes used as experimental and control groups, Study Quality and Reporting.risk_of_bias_randomization notes intact-class assignment and potential selection bias, and Study Quality and Reporting.baseline_equivalence reports no significant pre-test difference (p=0.970), indicating clearly described allocation and baseline comparability with some bias discussion.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design and Outcome.assessment_timepoints describe a pre-test–post-test design over a 16-week semester with two timepoints (pre and post) and no delayed follow-up (Outcome.followup_length_and_type marked NA), so time-based change is examined only in the short term.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability indicate use of standardized PTE Academic writing tasks scored by two experienced PTE trainers and an engagement questionnaire adapted from prior studies, but no inter-rater reliability indices for writing scores or internal consistency statistics (e.g., Cronbach’s alpha) for the questionnaire are reported, and Methodology.scoring_procedure_and_rater_training mentions rater expertise without detailed training or reliability evidence.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation specify a 16-week course with weekly academic writing tasks, explicit pre-test, training on ChatGPT, supervised ChatGPT-based feedback plus teacher feedback in the experimental class, and parallel teacher-only feedback in the control class; Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.llm_safety_guardrails, Intervention.writing_stage, Intervention.writing_task_type, and Intervention.setting collectively provide sufficient detail on LLM use, roles, and classroom context for basic replication despite unspecified model version and prompt templates.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports independent samples t-tests for between-group comparisons and paired samples t-tests for within-group pre–post changes on overall and component writing scores, with individual students as the analysis unit (Methodology.unit_of_analysis) and group/time factors defined in Outcome.independent_variables_and_factors; these analyses align with the quasi-experimental two-group pre–post design and the continuous outcome variables.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Methodology.sample_size_and_effect present t-values, p-values, and mean differences for overall and component scores, but Outcome.effect_size_summary notes that standardized effect sizes (e.g., Cohen’s d) were not reported, and Study Quality and Reporting.assumption_check_and_data_diagnostics indicates that formal checks of normality and homogeneity beyond Levene’s test are not described, with Study Quality and Reporting.data_preprocessing_and_transformation providing no additional diagnostics.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, while Outcome.limitation, Outcome.challenge, and Outcome.implication discuss small-sample, single-institution, non-random assignment, VPN access issues, and risks of overreliance within constructivist and engagement-theory perspectives (Methodology.theoretical_foundation), but no explicit outlier handling or sensitivity analyses are reported.",
	"total_quality_score": "14",
	"quality_category": "Moderate",
	"key_quality_concerns": "Quasi-experimental intact-class design without individual randomization and no reported reliability indices or standardized effect sizes for key writing and engagement measures, limiting causal inference and precision of effect estimates.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "Students in the experimental class directly used ChatGPT for academic writing feedback under integrity statements and teacher supervision despite VPN-based access constraints, but the model version, detailed prompt templates, and exact access conditions were not reported, which constrains reproducibility and interpretation of LLM-related effects."
}