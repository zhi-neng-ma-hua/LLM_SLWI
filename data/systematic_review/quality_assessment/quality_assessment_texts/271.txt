{
	"study_id": "10.3389/feduc.2025.1614673",
	"no": "271",
	"first_author_year": "Alnemrat 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Jordan (data collected at a large public university in Jordan)",
	"llm_type_brief": "ChatGPT (GPT-4)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly specifies three comparative aims about AI versus teacher feedback and proficiency levels, identifies a contextual and methodological gap in Arab EFL higher education, and situates the work within established feedback and AI-supported scaffolding theories.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.language_proficiency, Participant Information.learning_context, Participant Information.target_language, Participant Information.mother_tongue, Participant Information.sex, and Participant Information.discipline indicate undergraduate EFL students in Jordan, classified as Intermediate-Low or Advanced-Low on ACTFL-based assessments, all native Arabic speakers learning English with reported gender distribution, although age is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.group_assignment_method describe stratified sampling by proficiency with intact classroom sections assigned to AI or teacher feedback, and Methodology.sample_size_and_effect reports N=120 with detailed effect sizes, while Methodology.sampling_frame_and_representativeness notes a single large public Jordanian university context; however, Methodology.power_analysis is NR and no explicit a priori sample size rationale is given.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization indicate non-random allocation based on intact classes with stratification by proficiency level, and Study Quality and Reporting.baseline_equivalence reports similar pre-test means across AI and teacher groups without formal statistical tests, so allocation is described but non-random and potential selection bias and baseline imbalance cannot be fully ruled out.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, and Outcome.assessment_timepoints describe a pretest–posttest design with one week between Draft 1 and Draft 2 and analyses of pre–post gains, while Outcome.followup_length_and_type confirms that no delayed follow-up or additional time points were included, so only short-term change is examined.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability state that the analytic rubric was adapted from prior EFL writing research and reviewed by two L2 writing experts, and Methodology.scoring_procedure_and_rater_training reports double rating of a 30 percent stratified subsample with Pearson correlations, mean absolute differences, and ICC for post-test scores, providing explicit reliability and content validity evidence despite limited detail on rater training.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation specify a single timed argumentative task, feedback provision, and a one-week revision window, while Intervention.writing_stage, Intervention.writing_task_type, Intervention.setting, Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, and Intervention.llm_safety_guardrails together detail the GPT-4 mentor prompt, privacy mode, feedback focus, learner instructions, and teacher roles in sufficient depth to enable approximate replication.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method describes descriptive statistics, paired-samples t-tests within each subgroup, an independent-samples t-test on gain scores, and a 2×2 ANOVA on post-test scores, which align with Outcome.primary_outcome_variables, Outcome.independent_variables_and_factors, and Methodology.unit_of_analysis by appropriately modeling pre–post change and the factorial structure (Feedback Type by Proficiency Level) at the student level.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report t-values, F-values, p-values, and Cohen’s d (with mention of partial eta squared), while Study Quality and Reporting.assumption_check_and_data_diagnostics notes that Shapiro–Wilk tests and Levene’s tests were used to check normality and homogeneity of variance, and Study Quality and Reporting.data_preprocessing_and_transformation explains computation of total scores and gain scores without additional transformations, indicating that key assumptions, tests, and effect sizes were addressed.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR and no robustness checks are described, but Outcome.limitation, Outcome.challenge, Outcome.implication, and Methodology.theoretical_foundation show that the authors discuss design and scoring limitations, short-term and single-task constraints, and embed their findings within feedback engagement and LLM pedagogy frameworks, offering theoretical and methodological interpretation without explicit outlier or sensitivity analyses.",
	"total_quality_score": "16",
	"quality_category": "High",
	"key_quality_concerns": "Non-random intact-class allocation with only informal baseline comparability, partial inter-rater reliability on a subsample, and no explicit outlier or attrition handling may constrain internal validity and robustness of the single-task, short-term findings on AI versus teacher feedback.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "AI feedback was delivered via ChatGPT GPT-4 using a standardized rubric-aligned mentor prompt with chat history disabled and learner training to critically engage with feedback, but group assignment was non-random and model configuration details and full interaction logs were not reported, limiting exact reproducibility of the LLM condition."
}