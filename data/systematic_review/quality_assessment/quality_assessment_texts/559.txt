{
	"study_id": "NR",
	"no": "559",
	"first_author_year": "Polakova 2024",
	"title_short": "NR",
	"year": "2024",
	"country_region": "Czech Republic",
	"llm_type_brief": "ChatGPT (OpenAI; specific model version NR)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states its purpose (improvement of writing via ChatGPT feedback and exploration of perceptions), identifies an under-researched role of ChatGPT in EFL writing among Gen Z university students, and situates this within Education 4.0/5.0 and AI-in-education frameworks.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, language_proficiency, learning_context, target_language, discipline, and age provide tertiary educational stage, EFL status, university EFL context, L2 English, study programs, and an age range of 19–23, although mother_tongue, sex, and standardized proficiency scores are NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe a convenience sample of 110 Professional English students from one university with limited claims about broader generalisability, and Methodology.sample_size_and_effect reports N and pre/post counts, but Methodology.power_analysis is NR and no explicit sample size justification is provided.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization indicate a single convenience group with no control group and no random assignment, and Study Quality and Reporting.baseline_equivalence notes that only within-group baseline descriptions were needed, so potential selection and confounding biases are acknowledged but not analytically addressed.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, Outcome.assessment_timepoints, and Outcome.followup_length_and_type show a one-group pre-test–post-test design across two sessions with no delayed follow-up, allowing only short-term pre/post comparison rather than examination of longer-term effects.",
	"q6_measurement_reliability_validity_score": "0",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability indicate that questionnaires and focus group data underwent content analysis and that ChatGPT itself was used as both feedback provider and evaluator of writing, but no formal validity or reliability evidence for these instruments, no human scoring benchmarks, and no inter-rater reliability or rater training details are reported in Methodology.scoring_procedure_and_rater_training.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation describe a three-session procedure (questionnaire and pre-test summary plus ChatGPT feedback, post-test summary plus second ChatGPT evaluation, and focus group), while Intervention.writing_stage, Intervention.writing_task_type, Intervention.setting, Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.llm_model_configuration, and Intervention.llm_safety_guardrails together provide sufficient detail about the ChatGPT feedback cycles, classroom context, and roles to enable approximate replication despite NR for explicit AI literacy training.",
	"q8_statistical_method_appropriateness_score": "1",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports descriptive quantitative analysis of ChatGPT-generated pre/post classifications (strengths/weaknesses) for aspects such as conciseness and grammar, aligned with Outcome.primary_outcome_variables and Methodology.unit_of_analysis, but no specific inferential tests or modelling are described, which limits alignment with the quasi-experimental pre-test–post-test design.",
	"q9_assumption_and_effect_size_score": "0",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance states that improvements were significant without specifying test types, statistics, or p-values, Outcome.effect_size_summary is NR, and Study Quality and Reporting.assumption_check_and_data_diagnostics and Study Quality and Reporting.data_preprocessing_and_transformation do not report assumption checks, diagnostics, or standardised effect sizes beyond descriptive counts.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, but Outcome.limitation, Outcome.challenge, Outcome.implication, and Methodology.theoretical_foundation discuss limitations of the convenience one-group design, short duration, reliance on ChatGPT as evaluator, and risks of overreliance and misinformation, and they conceptually integrate findings with Education 4.0/5.0 and AI-in-education perspectives.",
	"total_quality_score": "11",
	"quality_category": "Moderate",
	"key_quality_concerns": "One-group convenience design without a control group and reliance on unvalidated ChatGPT-based scoring (no human rating reliability or formal inferential statistics) weaken internal validity and the strength of causal claims about writing improvement.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "No",
	"design_note_llm_specific": "ChatGPT was used directly by students as both feedback provider and primary evaluator of summary quality with an unspecified model version and no reported AI-literacy training or technical safety guardrails, limiting reproducibility and independent validation of LLM effects."
}