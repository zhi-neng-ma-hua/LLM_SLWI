{
	"study_id": "NR",
	"no": "461",
	"first_author_year": "Zhang 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Hong Kong, China",
	"llm_type_brief": "ChatGPT (GPT-4-Turbo)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly formulates aims about non-academic socialisation with ChatGPT and its effects on logical knowledge, logical quality, and self-efficacy, identifies a specific gap in GPT-4-assisted learning of argumentative writing logic, and is grounded in an explicit multi-theoretical framework.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, learning_context, target_language, mother_tongue, age, sex, discipline, and language_proficiency describe Chinese EFL university students in Hong Kong (age 18–27, mostly female) using English as L2 for argumentative writing, although no standardised proficiency scores are reported.",
	"q3_sampling_and_power_score": "2",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe a voluntarily recruited and screened convenience sample of 40 eligible Chinese EFL students from one university with noted limits to generalisability, Methodology.sample_size_and_effect reports detailed Ns and large effect sizes for key outcomes, and Methodology.power_analysis explains that although no formal a priori power analysis was run, the sample size was justified via rules of thumb and post hoc power considerations.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization clarify that all 40 participants received the ChatGPT-assisted intervention in a single-group design recruited through voluntary sampling with exclusion criteria, and although potential selection bias is acknowledged, there is no control group or baseline equivalence analysis across groups.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, Outcome.assessment_timepoints, and Outcome.followup_length_and_type specify a three-week sequence with pre-test, immediate post-test, and one-week delayed post-test for logical knowledge plus pre–post measures of logical quality and self-efficacy, and analyses explicitly compare scores across these time points.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report piloting and content checking of LogicalHamster, adaptation of logical-knowledge tests and rubrics from prior work, high inter-rater reliability for test and essay scores (Cohen’s κ≈0.85–0.96), acceptable internal consistency for self-efficacy and relatedness scales (Cronbach’s α≈0.80–0.86), and satisfactory PLS-SEM measurement properties, while Methodology.scoring_procedure_and_rater_training details blind independent scoring and consensus procedures.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation provide a stepwise timeline of the three-week procedure, and Intervention.writing_stage, Intervention.writing_task_type, Intervention.setting, Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, and Intervention.llm_safety_guardrails jointly describe the GPT-4 LogicalHamster setup, learning activities, writing and revision tasks, and researcher roles in sufficient detail to support approximate replication.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method indicates the use of paired-samples t-tests for pre–post–delayed comparisons, PLS-SEM for modelling relationships among non-academic socialisation and outcome constructs, and hierarchical regressions for predictive analyses, which align with the single-group repeated-measures design, Outcome.primary_outcome_variables and Outcome.independent_variables_and_factors, and the individual-learner unit described in Methodology.unit_of_analysis.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Study Quality and Reporting.assumption_check_and_data_diagnostics notes checks of linearity, homoscedasticity, normality, multicollinearity, and PLS-SEM measurement validity, Outcome.statistical_significance reports t-values, p-values, path coefficients, and R² changes, Outcome.effect_size_summary provides Cohen’s d and PLS-SEM f² estimates, and Study Quality and Reporting.data_preprocessing_and_transformation explains how test, essay, questionnaire, and prompt data were scored and aggregated without additional undocumented transformations.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, but Outcome.limitation, Outcome.challenge, and Outcome.implication together with Methodology.theoretical_foundation offer an integrated discussion linking findings to self-regulated learning, social presence, anthropomorphisation, cognitive load, and the absence of control groups, without reporting explicit outlier handling or sensitivity analyses.",
	"total_quality_score": "18",
	"quality_category": "High",
	"key_quality_concerns": "Single-group voluntary sample from one institution without a control group and no reported outlier or sensitivity analyses limit causal attribution and generalisability of the observed GPT-4-assisted learning effects.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "A custom GPT-4-Turbo bot (LogicalHamster) on POE was configured via system prompts to teach seven logical fallacies and foster friendly social interaction in a single 45–75-min lab session, but exact model identifiers, parameter settings, and automated safety or academic integrity guardrails beyond manual piloting were not reported, which constrains precise reproducibility and interpretation of LLM behaviour over time."
}