{
	"study_id": "10.1016/j.system.2025.103805",
	"no": "284",
	"first_author_year": "Muñoz Muñoz 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Chile (Chilean university, EFL context)",
	"llm_type_brief": "ChatGPT (GPT-4o)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims and Basic Identification.research_gap_or_novelty, the study clearly formulates comparative aims for ChatGPT versus teacher WCF and specifies a context-specific research gap in Chile, while Methodology.theoretical_foundation situates the work within written corrective feedback and AI/AWE literature.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.discipline, Participant Information.language_proficiency, Participant Information.learning_context, and Participant Information.target_language describe the educational stage, program, CEFR band, and EFL context, and Participant Information.age and Participant Information.sex report age range and gender distribution, although mother tongue is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sample_size_and_effect state that 44 English Pedagogy students in two intact sections were used as a convenience sample with group sizes and test statistics reported, and Methodology.sampling_frame_and_representativeness acknowledges limited generalisability, but Methodology.power_analysis is NR and no explicit sample size justification is provided.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method explains that intact course sections were randomly assigned to ChatGPT or teacher feedback conditions, while Study Quality and Reporting.risk_of_bias_randomization and Study Quality and Reporting.baseline_equivalence report a significant pre-test difference favoring the teacher group, explicitly noting non-equivalent groups and potential selection bias.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design and Intervention.duration describe four IELTS-style tasks over about two months with T1 as pre-test and T4 as post-test, and Outcome.assessment_timepoints plus Outcome.statistical_significance and Outcome.other_outcomes_measure show analyses of score changes across T1–T4, though no delayed follow-up beyond the fourth task is included.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report use of the official IELTS rubric, expert review and Kendall’s W for prompt validation, and pilot testing, but Methodology.scoring_procedure_and_rater_training does not provide inter-rater reliability indices or detailed rater training for rubric scoring, especially for the ChatGPT group.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation outline a four-session timeline with 40-minute essays and 10-minute in-class feedback review, and LLM-related fields (Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.role_llm, Intervention.llm_access_policy, Intervention.training_support_llm_literacy) specify GPT-4o’s role, prompting, researcher-mediated use, and student review procedures in sufficient detail to approximate the feedback intervention.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method describes using Wilcoxon Signed-Rank, Mann–Whitney U, and Friedman tests with eta squared after normality and variance checks, which is consistent with Outcome.primary_outcome_variables (IELTS scores), Outcome.independent_variables_and_factors (feedback source and task/time), and Methodology.unit_of_analysis (individual essay scores and within-student change).",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Study Quality and Reporting.assumption_check_and_data_diagnostics reports Shapiro–Wilk and Levene tests guiding the choice of non-parametric analyses, Outcome.statistical_significance provides detailed test statistics and p-values, Outcome.effect_size_summary reports eta squared for key Wilcoxon comparisons, and Study Quality and Reporting.data_preprocessing_and_transformation notes that raw rubric scores were analyzed without additional transformations.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, but Outcome.limitation and Outcome.challenge discuss short intervention length, single-genre focus, external exposure, variability in ChatGPT output, and baseline non-equivalence, while Outcome.implication links the results to WCF theory and AI literacy; nonetheless, there is no explicit treatment of outliers or sensitivity analyses.",
	"total_quality_score": "17",
	"quality_category": "High",
	"key_quality_concerns": "Non-equivalent intact groups without individual randomization and absence of inter-rater reliability reporting for IELTS scoring, combined with a small convenience sample and no power analysis.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "Researcher-operated ChatGPT GPT-4o provided standardized direct WCF via a fixed prompt and paper-based feedback, with one chat per essay and model version specified but access mode, detailed scoring procedures for the AI group, and inherent variability of LLM outputs limiting full reproducibility."
}