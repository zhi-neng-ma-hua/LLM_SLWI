{
	"study_id": "https://doi.org/10.1007/s40299-024-00930-6",
	"no": "382",
	"first_author_year": "Hwang 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "South Korea (Korean university with multinational EFL learners)",
	"llm_type_brief": "ChatGPT (OpenAI LLM)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty and Methodology.theoretical_foundation, the study clearly states its aims regarding learners’ prompting behaviors, objective–approach alignment and writing outcomes, identifies a specific gap about prompt literacy in ChatGPT-assisted revision, and situates the work within an assemblage framework and prompt-engineering literature.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.language_proficiency, Participant Information.learning_context, Participant Information.target_language, Participant Information.age, Participant Information.sex and Participant Information.discipline describe first-year undergraduates (19–21 years) in an English-medium Korean international program, CEFR B2+ EFL learners with multiple national backgrounds and clear target language/context information.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness indicate volunteer convenience sampling of 11 students from a single advanced writing class with acknowledged limits to generalisability, and Methodology.sample_size_and_effect reports N and effect sizes, but Methodology.power_analysis is NR and there is no formal a priori power or detailed sample-size justification.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization describe a single-cohort one-group pretest–posttest design in which all 11 students received the ChatGPT-assisted revision intervention without randomisation or a control group, and Study Quality and Reporting.baseline_equivalence is not applicable, so the design and associated selection/maturation biases are acknowledged but not fully controlled.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration and Outcome.assessment_timepoints show a three-week sequence with a pretest essay in week 2 and a post-revision assessment in week 3 after ChatGPT-assisted revision, but only two time points and no delayed or longer-term follow-up on retention or transfer are reported.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability specify use of the TEEP attribute writing scale with two independent raters and substantial-to-almost-perfect Cohen’s Kappa values for all attributes and total scores, plus Kappa=1.0 for test–retest coding reliability of objectives and prompt approaches, and Methodology.scoring_procedure_and_rater_training reports qualified raters and clearly defined scoring procedures, supporting strong reliability and L2-appropriate validity.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation provide a detailed three-week timeline (ChatGPT workshop, tool-free pretest writing, two ChatGPT-assisted revision sessions with specified timing and turns, and collection of logs), while LLM-related fields (Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.setting) clearly describe writing tasks, revision stage use of ChatGPT, workshop content and the decision to omit explicit prompt training, enabling replication of the instructional sequence aside from unspecified model configuration and safety settings.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports Wilcoxon signed-rank tests with matched pairs rank biserial correlations for pre–post TEEP scores in a very small one-group sample, supported by descriptive statistics, which is appropriate for the within-subject design, ordinal 0–3 attribute scores and N=11 described in Methodology.sample_size_and_effect, Outcome.primary_outcome_variables, Outcome.independent_variables_and_factors and Methodology.unit_of_analysis.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary provide Wilcoxon z-values, p-values and rank biserial correlation effect sizes for each attribute, but Study Quality and Reporting.assumption_check_and_data_diagnostics notes that only the small sample size is cited to justify nonparametric tests and no further distributional diagnostics or data-preprocessing details are reported in Study Quality and Reporting.data_preprocessing_and_transformation.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR for any outlier handling or sensitivity checks, while Outcome.limitation, Outcome.challenge, Outcome.implication and Methodology.theoretical_foundation show that findings (surface-level gains vs limited higher-order improvement) are interpreted in relation to prompt literacy, assemblage perspectives and small-sample one-group design constraints but without explicit discussion of anomalous data patterns.",
	"total_quality_score": "15",
	"quality_category": "Moderate",
	"key_quality_concerns": "Very small convenience sample in a one-group pretest–posttest design without a control group and lack of detailed assumption checks, outlier handling and attrition/missing-data reporting limit causal inference and generalisability.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT was used only at the revision stage after a brief introductory workshop with no explicit prompt-writing training, and the study does not report the specific model version, configuration or safety guardrails, so AI behaviour and learners’ prior ChatGPT experience may constrain reproducibility and interpretation of LLM-related effects."
}