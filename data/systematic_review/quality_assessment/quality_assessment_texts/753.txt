{
	"study_id": "NR",
	"no": "753",
	"first_author_year": "Chan 2024",
	"title_short": "NR",
	"year": "2024",
	"country_region": "Hong Kong, China",
	"llm_type_brief": "GPT-3.5 Turbo",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims and Basic Identification.research_gap_or_novelty, the study clearly specifies examining the impact of GPT-3.5-turbo feedback on argumentative writing quality and affective variables and positions this within gaps in empirical work on LLM feedback in authentic higher education contexts; Methodology.theoretical_foundation links the work to AI-in-education, feedback, and AWE/LLM literature, providing a recognizable theoretical backdrop.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.discipline, Participant Information.learning_context, Participant Information.language_proficiency, and Participant Information.target_language describe first-year undergraduates in an EFL Hong Kong university writing course using IELTS-type tasks, while Participant Information.sex and Study Quality and Reporting.baseline_equivalence provide overall gender distribution; Participant Information.age and Participant Information.mother_tongue are NR but key educational and language background variables are mostly reported.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness state that convenience sampling from one Hong Kong university course was used and acknowledge limited generalizability; Methodology.sample_size_and_effect reports a large total N=918 and summarizes group means and t-test results, but Methodology.power_analysis is NR and there is no explicit rationale for sample size adequacy.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method describes random assignment of eligible students to feedback and control groups with attention to gender ratio, and Study Quality and Reporting.risk_of_bias_randomization notes the randomized controlled trial framing, while Study Quality and Reporting.baseline_equivalence indicates similar initial task scores; however, details of the randomization procedure, allocation concealment, and formal baseline tests are not reported.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design and Intervention.duration describe a single two-hour session with an initial essay and a revision about 5–25 minutes later, and Outcome.assessment_timepoints clarifies that all measures (original score, revised score, post-task questionnaire) occurred within this session; Outcome.followup_length_and_type is NA, so only short-term pre–post change is examined without any delayed follow-up.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.scoring_procedure_and_rater_training explain that original and revised essays were double marked by separate instructors using a course rubric and averaged, and that a brief three-item questionnaire and semi-structured interviews were used, but Methodology.data_collection_validity_reliability does not report inter-rater reliability coefficients for writing scores or any reliability/validity indices for the questionnaire, yielding only partial evidence on measurement quality.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation specify a two-hour lab session with a 30-minute initial essay, researcher-mediated GPT-3.5 feedback generation, a 5-minute feedback-reading period, and a 20-minute revision, while Intervention.writing_stage and Intervention.writing_task_type situate the intervention at the revision stage of a specific argumentative prompt; Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.role_instructor, Intervention.role_llm, Intervention.llm_access_policy, and Intervention.training_support_llm_literacy together describe teacher-mediated GPT-3.5-turbo feedback, the structured prompt, and the lack of direct student LLM use, allowing the core procedure to be broadly replicated.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports descriptive statistics, independent-samples t-tests for group comparisons, point biserial and Pearson correlations, and simple linear regression relating motivation and engagement to score improvements, with alpha=.05; Outcome.primary_outcome_variables, Outcome.independent_variables_and_factors, and Methodology.unit_of_analysis indicate that these parametric methods at the individual-student level reasonably match the two-group experimental design and continuous outcome measures, even though potential clustering and partial use of the large sample (df=192) are not fully explained.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary provide p-values for t-tests and regression models and report effect size indicators such as point biserial correlations, Pearson correlations, and R-squared values for regressions, but Study Quality and Reporting.assumption_check_and_data_diagnostics notes that no explicit checks of normality, homoscedasticity, or outliers were reported and no data transformations beyond averaging rater scores were described.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, so there is no discussion of outlier handling or sensitivity analyses; however, Outcome.limitation and Outcome.challenge reflect on contextual constraints, single-prompt design, affective and motivational confounds, and unmodeled variables, while Outcome.implication and Methodology.theoretical_foundation connect findings to broader feedback, motivation, and AI-in-education literature, providing some theoretical integration without explicit data-robustness analyses.",
	"total_quality_score": "14",
	"quality_category": "Moderate",
	"key_quality_concerns": "Convenience sampling from a single university with a single-session non-credit task, combined with limited reporting on measurement reliability, assumption checks, outlier handling, and missing-data use, constrains internal validity and generalizability.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "GPT-3.5-turbo was used only by the researcher to generate up-to-500-word written feedback between drafts in a single revision-focused session, with students barred from direct LLM use and with limited reporting of prompt details, model parameters, and LLM literacy support, which may restrict reproducibility and interpretation of the AI feedback mechanism."
}