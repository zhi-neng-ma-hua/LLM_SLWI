{
	"study_id": "NR",
	"no": "619",
	"first_author_year": "Almashy 2024",
	"title_short": "NR",
	"year": "2024",
	"country_region": "India (12th-grade students at an Indian public school)",
	"llm_type_brief": "ChatGPT 3.5 + Grammarly + Google Translate",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states its goal of comparing three CALL tools (ChatGPT, Grammarly, Google Translate) and a traditional control on writing error correction, highlights a specific gap in comparative CALL research, and situates the tools within established CALL and L2 writing perspectives.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.age, Participant Information.discipline, Participant Information.learning_context, Participant Information.target_language, Participant Information.mother_tongue, Participant Information.language_proficiency, Participant Information.sex, and Participant Information.prior_experience_llm together provide detailed information on 12th grade Hindi L1 ESL/EFL learners, including schooling level, age, streams, L1, L2 context, general proficiency background, and gender distribution, although standardized proficiency scores are not reported.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe cluster random sampling of four intact 12th grade classes at a single public school into three experimental CALL conditions and one control, and note limited generalisability beyond this setting, while Methodology.sample_size_and_effect reports total N=213 and MANOVA effect sizes; however, Methodology.power_analysis is NR and no explicit a priori sample size justification is provided.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization indicate that four intact classes were randomly allocated to ChatGPT, Grammarly, Google Translate, and control conditions without individual-level randomisation or allocation concealment, and Study Quality and Reporting.baseline_equivalence is NR so baseline comparability across classes is not statistically demonstrated or discussed.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, and Outcome.assessment_timepoints describe a 12 week pretest–posttest design with picture-based narrative writing tasks before and after the interventions, but Outcome.followup_length_and_type is NR and there is no delayed posttest or analysis of longer term retention, so time effects are limited to short term change.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability explain that pre and post narratives were analyzed by an enhanced Grammarly Automated Essay Scoring system to count eight error types, with the tool described as capable of detecting L2 errors, yet Methodology.scoring_procedure_and_rater_training does not report any human rater involvement, rater training, or reliability coefficients, and no empirical validity or reliability indices for the AES in this sample are provided.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation give a detailed 12 week schedule with four 60 minute sessions per week, describing pretest, tool specific training programs for each class (ChatGPT in Class A, Grammarly in Class B, Google Translate in Class C, and traditional instruction in Class D), and a posttest, while Intervention.writing_genre, Intervention.writing_stage, Intervention.writing_task_type, Intervention.setting, Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, and Intervention.llm_access_policy together specify the narrative tasks, focus on revision, standardized ChatGPT 3.5 prompt, student and teacher roles, and in class CALL usage sufficiently for approximate replication despite Intervention.llm_model_configuration and Intervention.llm_safety_guardrails being NR.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports use of MANOVA with class (four groups) and test time (pretest, posttest) as independent variables and frequencies of eight error categories as dependent variables, which aligns with Methodology.research_design as a four group pretest–posttest quasi experiment, Outcome.primary_outcome_variables as multivariate error counts, Outcome.independent_variables_and_factors specifying class and time, and Methodology.unit_of_analysis indicating individual learner texts as the analysis unit.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report MANOVA F values, p values, and partial eta squared for main effects and interactions, and Study Quality and Reporting.data_preprocessing_and_transformation notes automated extraction of error counts via Grammarly AES, but Study Quality and Reporting.assumption_check_and_data_diagnostics indicates that checks of MANOVA assumptions such as normality and homogeneity of variance covariance matrices were not reported and no corrective procedures were described.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR so there is no description of outlier handling or sensitivity analyses, while Outcome.limitation, Outcome.challenge, and Outcome.implication acknowledge constraints related to a single school, a restricted set of CALL tools, and short term error focus and link findings to broader CALL and L2 writing practice at a general level rather than deeply integrating results with the theoretical distinctions outlined in Methodology.theoretical_foundation.",
	"total_quality_score": "14",
	"quality_category": "Moderate",
	"key_quality_concerns": "Cluster-randomized intact classes with no reported baseline equivalence and exclusive reliance on Grammarly AES automated error counts (without human rating reliability or MANOVA assumption checks) limit causal inference and the validity of the observed differences among CALL tools.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT 3.5 was used only in one experimental class during supervised revision with a fixed error-correction prompt, with no reported model parameters, access date, or safety guardrails and outcomes evaluated solely via Grammarly AES error counts, which constrains reproducibility and may not reflect broader writing quality or unsupervised LLM use."
}