{
	"study_id": "NR",
	"no": "896",
	"first_author_year": "Escalante 2023",
	"title_short": "NR",
	"year": "2023",
	"country_region": "Asia–Pacific region (small liberal arts university)",
	"llm_type_brief": "ChatGPT (GPT-4, OpenAI)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states two aims (comparing GPT-4 versus human tutor feedback and examining learner preferences) and situates them within gaps in AWE/LLM and ENL writing research.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.language_proficiency, Participant Information.learning_context, and Participant Information.target_language identify university ENL students with at least CEFR B1 English proficiency in a higher-education ENL writing context, while Participant Information.age and Participant Information.sex provide age ranges and gender composition, even though Participant Information.mother_tongue is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe a self-selected convenience sample of ENL students from one liberal arts university and acknowledge limited representativeness, and Methodology.sample_size_and_effect reports N=48 for Study 1 and N=43 for Study 2 with detailed test statistics, but Methodology.power_analysis is NR and no explicit sample-size or power justification is provided.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method identifies Study 1 as a quasi-experimental non-randomized comparison of AI-feedback and human-tutor groups, Study Quality and Reporting.risk_of_bias_randomization notes the absence of randomization and potential selection/confounding biases, and Study Quality and Reporting.baseline_equivalence reports non-significant pretest differences, clarifying allocation and baseline comparability.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, and Outcome.assessment_timepoints describe a pretest–posttest design over roughly eight weeks with six weeks of weekly assignments but no delayed follow-up, and Outcome.followup_length_and_type is NR/NA, so only short- to medium-term change is examined without longer-term maintenance.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report that pretest and posttest essays were rated by two experienced instructors using an analytic rubric with excellent to good intraclass correlation coefficients (ICC≈0.816–0.932), and Methodology.scoring_procedure_and_rater_training describes rater expertise and procedures, although no reliability indices are given for the Study 2 Likert questionnaire.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation outline an eight-week sequence with a diagnostic pretest, six weekly 300-word source-based assignments, and a posttest under clearly distinguished human-tutor and GPT-4 feedback conditions, while LLM-related fields (Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.llm_access_policy, Intervention.role_instructor, Intervention.role_llm, Intervention.writing_stage, Intervention.writing_task_type) provide sufficient detail on the ChatGPT-mediated feedback workflow for replication.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method states that Study 1 used Shapiro–Wilk and Levene’s tests followed by a 2×2 mixed repeated-measures ANOVA with time (pre vs post) and group (AI vs human feedback), plus independent-samples t-tests and effect sizes, while Study 2 used descriptive statistics, t-tests, and thematic analysis, which is consistent with Outcome.primary_outcome_variables, Outcome.independent_variables_and_factors, and Methodology.unit_of_analysis at the individual-student level.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report F-values, t-values, p-values, partial eta-squared, and Cohen’s d for key analyses, and Study Quality and Reporting.assumption_check_and_data_diagnostics notes Shapiro–Wilk tests, Levene’s tests, Greenhouse–Geisser corrections, and preliminary checks for missing data and assumption violations, with Study Quality and Reporting.data_preprocessing_and_transformation indicating de-identification of texts and analysis of untransformed scores.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis mentions exclusion of one posttest-only case but does not detail broader outlier diagnostics or sensitivity analyses, whereas Outcome.limitation and Outcome.challenge acknowledge the quasi-experimental design, short duration, modest sample sizes, and unexamined proficiency differences, and Outcome.implication together with Methodology.theoretical_foundation integrate findings into discussions of GenAI-enabled feedback and blended human–AI pedagogy.",
	"total_quality_score": "17",
	"quality_category": "High",
	"key_quality_concerns": "Quasi-experimental non-randomized grouping from a single university with modest sample sizes and no long-term follow-up limits causal inference and generalizability.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "GPT-4 via ChatGPT was accessed only by a teaching assistant using a standardized multi-part feedback prompt on de-identified student texts, and the lack of detailed model versioning, access dates, and parameter settings constrains exact reproducibility and interpretation of the AI feedback mechanism."
}