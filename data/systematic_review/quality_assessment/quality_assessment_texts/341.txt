{
	"study_id": "NR",
	"no": "341",
	"first_author_year": "Deygers 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Belgium (major Belgian university)",
	"llm_type_brief": "ChatGPT3",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty and Methodology.theoretical_foundation, the study clearly formulates aims about ChatGPT3-supported automated writing evaluation effects on syntactic and lexical complexity over nine weeks and on sustainment after removing ChatGPT access, and situates these questions within AWE and writing development research addressing a longitudinal evidence gap.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, language_proficiency, learning_context, target_language and discipline specify first-year EFL English translation students at a Belgian university, and Participant Information.age, sex and prior_experience_llm report mean age, gender distribution and similarly low initial familiarity with ChatGPT, although mother_tongue is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe random assignment within a cohort of first-year English translation students at one Belgian university and acknowledge limited generalizability, and Methodology.sample_size_and_effect reports N=105 and key statistics, but Methodology.power_analysis is NR and no explicit justification of sample size is given.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization state that students were randomly assigned to experimental (teacher feedback plus ChatGPT) and control (teacher feedback only) conditions across four instructor-led groups, while Study Quality and Reporting.baseline_equivalence reports no significant baseline differences in gender, age, writing enjoyment or ChatGPT familiarity and baseline writing complexity was controlled via covariates in the multilevel models, with instructor-related variance noted as a potential limitation.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration and Outcome.assessment_timepoints describe a nine-week longitudinal two-group design with a baseline essay, two key essays during the ChatGPT intervention (weeks 6 and 8), and a final essay without ChatGPT (week 9), and Outcome.followup_length_and_type indicates this short-term follow-up within the instructional period to examine changes over time and the persistence of effects once AI access is removed.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability describe controlled in-class timed essays with topic factsheets, use of a baseline hand-written essay as a control measure, and reliance on established NLP tools TAALES and TAASSC for lexical and syntactic indices, but no explicit reliability coefficients or detailed validation evidence for the specific indices in this sample are reported and Methodology.scoring_procedure_and_rater_training notes that no human ratings or rater training were used.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation provide a week-by-week account of the nine-week sequence (baseline, ChatGPT training for the experimental group, practice, two differential-feedback essays and a final essay without ChatGPT), while Intervention.writing_stage, Intervention.writing_task_type and Intervention.setting specify timed in-class expository essays under controlled conditions, and LLM-related fields (Intervention.llm_model_type, llm_model_configuration, llm_integration_mode, prompting_strategy, training_support_llm_literacy, role_llm, role_instructor, llm_access_policy) together explain how, when and for which feedback purposes ChatGPT3 was used alongside teacher feedback, allowing approximate replication of the instructional and AI-integration procedure.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports Wilcoxon tests and independent t-tests for selected comparisons and multilevel linear regression models with random intercepts and slopes for essays nested within students to estimate effects of time, condition and their interaction while controlling baseline complexity, which matches Outcome.primary_outcome_variables and Outcome.independent_variables_and_factors and aligns with Methodology.unit_of_analysis describing essays nested in students.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary provide p-values and several effect size estimates (rank-biserial correlations for Wilcoxon tests and Cohenâ€™s d for interim t-tests) and Study Quality and Reporting.data_preprocessing_and_transformation notes text cleaning and use of baseline complexity covariates, but Study Quality and Reporting.assumption_check_and_data_diagnostics mentions only informal inspection of plots and descriptive statistics and does not report formal residual or homoscedasticity checks for the multilevel models.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, indicating no reported procedures for outlier detection or sensitivity analyses, yet Outcome.limitation and Outcome.implication, together with Methodology.theoretical_foundation, discuss constraints such as the nine-week timeframe, focus on syntactic and lexical complexity only, limited control over real-world ChatGPT usage and the role of ChatGPT as a supplement to teacher feedback, integrating findings with AWE and writing development perspectives despite limited treatment of data anomalies.",
	"total_quality_score": "16",
	"quality_category": "High",
	"key_quality_concerns": "Limited outcome scope focused only on syntactic and lexical complexity indices and minimal reporting on statistical assumptions, outlier handling and detailed ChatGPT3 configuration constrain interpretation, causal inference and reproducibility of the AI-supported writing effects.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT3 was available only to the experimental group during in-class writing after a brief prompt-engineering course, but exact model build, interface settings and detailed usage logs (including any out-of-class use) were not reported, limiting precise quantification and replication of ChatGPT exposure."
}