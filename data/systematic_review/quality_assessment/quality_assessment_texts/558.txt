{
	"study_id": "NR",
	"no": "558",
	"first_author_year": "Mahapatra 2024",
	"title_short": "NR",
	"year": "2024",
	"country_region": "India (elite private-run university)",
	"llm_type_brief": "ChatGPT (version NR)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states its dual aims (impact of ChatGPT-based formative feedback on academic writing and student perceptions), identifies a specific empirical gap in ESL/EFL large-class contexts in the Global South, and is explicitly grounded in dialogic feedback theory and a framework of ChatGPT as a reliable writing tool.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.discipline, Participant Information.language_proficiency, Participant Information.learning_context, Participant Information.target_language, and Participant Information.age provide clear information on first-year science and engineering undergraduates in an ESL higher-education context with English-medium schooling and institutional English screening, although Participant Information.mother_tongue and Participant Information.sex are NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe an intact class of first-year science and engineering students randomly assigned to experimental and comparison sections at one elite private-run Indian university, with contextual discussion of generalisability, and Methodology.sample_size_and_effect reports final Ns and large effects, but Methodology.power_analysis is NR and no a priori sample-size justification is given.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization state that an intact class was randomly assigned to experimental vs comparison sections without individual-level randomisation or allocation concealment, and Study Quality and Reporting.baseline_equivalence notes that although both groups took a pre-test, explicit statistical comparison of baseline writing scores between groups was not reported.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, Outcome.assessment_timepoints, and Outcome.followup_length_and_type describe a quasi-experimental design with three time points (pre-test, post-test, delayed post-test about two months later) for both groups, and Outcome.statistical_significance plus Outcome.effect_size_summary show analysis and interpretation of change over time and maintenance of gains.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report researcher-developed analytic rubrics validated by two applied linguistics experts and double-rating of all scripts with Cohen’s Kappa exceeding 0.8, and Methodology.scoring_procedure_and_rater_training notes systematic rubric-based scoring by two teachers, providing clear evidence of validity and inter-rater reliability for the L2 writing measures.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation give a detailed timeline (one-hour ChatGPT training plus six hours of genre-focused instruction across process, comparison, and cause-effect writing with pre-, post-, and delayed post-tests), and Intervention.writing_genre, Intervention.writing_stage, Intervention.writing_task_type, Intervention.prompting_strategy, Intervention.role_instructor, Intervention.role_llm, Intervention.llm_integration_mode, and Intervention.training_support_llm_literacy specify how ChatGPT was used for brainstorming, genre feature checking, self- and peer assessment, and revision, so that the pedagogical and LLM-related procedures are largely replicable despite Intervention.llm_model_configuration and Intervention.llm_safety_guardrails being NR.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method describes one-way repeated measures ANOVA with Bonferroni post-hoc tests for the experimental group’s three time points and one-tailed independent-samples t-tests for experimental vs comparison groups at post-test and delayed post-test, all at the individual-student level as per Methodology.unit_of_analysis and Outcome.independent_variables_and_factors, which aligns with the quasi-experimental group × time structure of the design.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Methodology.sample_size_and_effect report F, t, and p values along with η² and Cohen’s d for key contrasts, while Study Quality and Reporting.assumption_check_and_data_diagnostics notes use of Levene’s test, Shapiro–Wilk tests, and Grubb’s test to check homogeneity, normality, and outliers, and Study Quality and Reporting.data_preprocessing_and_transformation indicates that scores were analysed without additional transformations, showing that assumptions and effect sizes were explicitly addressed.",
	"q10_outliers_and_interpretation_score": "2",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis reports use of Grubb’s test with no outliers detected, and Outcome.limitation, Outcome.challenge, Outcome.implication, together with Methodology.theoretical_foundation, show that results were interpreted in light of dialogic feedback theory and ChatGPT-as-tool frameworks while acknowledging genre scope, duration limits, classroom constraints, and risks such as dependence and reduced independent thinking.",
	"total_quality_score": "18",
	"quality_category": "High",
	"key_quality_concerns": "Quasi-experimental intact-class allocation without individual-level randomisation or fully tested baseline equivalence, and no a priori power analysis or detailed reporting of attrition patterns.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT was used as a formative feedback tool for self- and peer assessment across writing stages with explicit student training, but Study Quality and Reporting.llm_version_reproducibility and Intervention.llm_model_configuration indicate that the specific ChatGPT version, access date, and configuration were not reported and no additional LLM safety guardrails were described, which constrains exact replication and interpretation of AI behaviour over time."
}