{
	"study_id": "10.1080/17501229.2025.2525341",
	"no": "95",
	"first_author_year": "Lu 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Southeast China (mainland Chinese high school)",
	"llm_type_brief": "ChatGPT (GPT-4o)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty and Methodology.theoretical_foundation, the study clearly formulates two research questions on whether ChatGPT-generated model texts enhance argumentative text quality and how students perceive them, explicitly framed within the Output Hypothesis, Noticing Hypothesis and model-text feedback literature.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, language_proficiency, learning_context, mother_tongue, target_language, age, sex and discipline together describe 104 Grade 11 EFL students in a mainland Chinese high school (CEFR B1–B2, Mandarin/Cantonese L1, 55 males and 49 females), providing detailed educational, linguistic and demographic background.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness indicate convenience sampling of three intact Grade 11 classes from one high school, and Methodology.sample_size_and_effect reports N=104 with group sizes and large interaction effect sizes, but Methodology.power_analysis is NR and there is no explicit discussion of sample size rationale or broader representativeness.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization state that after confirming no proficiency differences via OPT, the three intact classes were randomly assigned to control, teacher-model and ChatGPT-model conditions, and Study Quality and Reporting.baseline_equivalence reports non-significant pretest writing and OPT differences, supporting clear allocation and baseline comparability despite class-level randomisation.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design and Intervention.duration describe a four-week pretest–post-test design with three stages, and Outcome.assessment_timepoints and Outcome.followup_length_and_type show only two time points (pretest in Week 2 and rewritten post-test in Week 4) without any delayed follow-up, so only short-term change is examined.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report a pilot-tested writing task, comprehensibility checks for both model texts, an analytical rubric adopted from prior work, high inter-rater reliability for writing scores (r=0.89 for drafts, r=0.93 for rewrites), validated questionnaire sources and substantial intercoder agreement for qualitative coding (k=0.84), and Methodology.scoring_procedure_and_rater_training describes independent double rating and averaging of scores.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation provide a detailed week-by-week account of the three-stage procedure (drafting, model/self comparison, rewriting) for all three groups, while LLM-related fields (Intervention.llm_model_type, llm_model_configuration, llm_integration_mode, prompting_strategy, role_llm, role_instructor, llm_access_policy, training_support_llm_literacy, setting and writing_stage) clearly specify that ChatGPT 4o generated a single contextualised model essay delivered on paper, with teacher-mediated prompt design and no direct student–LLM interaction, enabling basic replication.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports using mixed-design ANOVA with time (pretest vs post-test) and group (CG vs EG1 vs EG2) plus post-hoc and within-group t-tests with Cohen’s d and partial η², which aligns with Outcome.independent_variables_and_factors, Outcome.primary_outcome_variables and Methodology.unit_of_analysis that define individual student writing scores across two time points and three instructional conditions.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Study Quality and Reporting.assumption_check_and_data_diagnostics notes that normality, homogeneity of variance and sphericity were checked and met for the mixed ANOVA, Outcome.statistical_significance provides F, p and detailed interaction tests, Outcome.effect_size_summary reports partial η² for interactions and Cohen’s d for pairwise and within-group comparisons, and Study Quality and Reporting.data_preprocessing_and_transformation explains averaging of double-rated scores without additional transformations.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR and no robustness or sensitivity checks are described, although Outcome.limitation, Outcome.challenge and Outcome.implication interpret findings in relation to Output and Noticing Hypotheses and model-text feedback theory (Methodology.theoretical_foundation), discussing constraints such as single-topic focus, single model text and limited generalisability.",
	"total_quality_score": "17",
	"quality_category": "High",
	"key_quality_concerns": "Convenience sample of three intact classes from a single high school with no power analysis or outlier/sensitivity checks, and only short-term pretest–post-test outcomes without longitudinal follow-up.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT 4o was used only by the teacher to generate a single printed model essay via one contextual prompt, with students never accessing the LLM directly and future replicability limited by potential model updates despite the prompt being reported."
}