{
	"study_id": "NR",
	"no": "441",
	"first_author_year": "Zhang 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Hong Kong Special Administrative Region, China",
	"llm_type_brief": "ChatGPT (GPT-4, LogicalHamster)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims and Basic Identification.research_gap_or_novelty, the study clearly formulates aims about GPT-4-based logic learning with LogicalHamster and identifies a specific gap on ChatGPT-based logic learning strategies and their links to logic knowledge and writing. Methodology.theoretical_foundation details a coherent framework drawing on logic learning strategies, Bloom’s taxonomy, experiential learning theories, and AI/generative-AI writing research.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.learning_context, Participant Information.target_language, and Participant Information.mother_tongue specify Chinese EFL university students using English as the medium of instruction in Hong Kong. Participant Information.age and Participant Information.sex report age range and gender distribution, and Participant Information.language_proficiency describes frequent EFL argumentative writing tasks even though standardised proficiency scores are not provided.",
	"q3_sampling_and_power_score": "2",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe voluntary response sampling from a single Hong Kong university with explicit inclusion/exclusion criteria and note restricted generalisability. Methodology.sample_size_and_effect reports N=40 with detailed outcome statistics, and Methodology.power_analysis explains that although no a priori analysis was run, the sample met general rules of thumb and G Power post hoc checks indicated adequate power for some R² tests.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method indicates a single-group design without random allocation to different conditions, and Study Quality and Reporting.risk_of_bias_randomization notes self-selection and absence of randomisation while using baseline measures as covariates. Because there is no control group and baseline equivalence between groups is NA, potential selection and maturation biases remain despite clear reporting.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design and Intervention.duration describe a three-week procedure with pre-test, immediate post-test, and one-week delayed post-test for knowledge of logic plus pre-learning and post-learning revision essays. Outcome.assessment_timepoints and Outcome.followup_length_and_type show that changes over time and short-term retention were explicitly assessed and interpreted.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report piloting of LogicalHamster, blind scoring of knowledge tests with high inter-rater correlation (r≈0.96), blind scoring of writing logic with high inter-rater correlations (r≈0.87 and r≈0.85), substantial Cohen’s kappa values for prompt and interview coding, and acceptable reliability and validity indices in the PLS SEM measurement model. Methodology.scoring_procedure_and_rater_training further details rubric-based scoring and resolution of discrepancies through discussion.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation provide a stepwise three-week timeline covering pre-treatment survey, orientation, pre-test writing, logic tests, a 45–75 minute ChatGPT-based learning session, immediate revision, and delayed post-test. LLM-related fields (Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.setting) describe GPT-4/POE configuration, targeted reasoning errors, supported strategies, lab setting, and learner–bot interaction in sufficient detail to enable approximate replication of the intervention.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports descriptive statistics, Welch one-way ANOVA on prompt sub-strategy frequencies, multiple linear regressions predicting outcomes from strategy use while controlling for prior knowledge, and PLS SEM linking latent strategy and outcome constructs, which align with Outcome.primary_outcome_variables and Outcome.independent_variables_and_factors. Methodology.unit_of_analysis confirms the individual learner and essay as analysis units, matching the chosen models to the data structure and research questions.",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Study Quality and Reporting.assumption_check_and_data_diagnostics indicates that linearity, homoscedasticity, and approximate normality (using skewness and kurtosis) were checked and that multicollinearity was acceptable, matching the regression and ANOVA methods in Methodology.data_analysis_method. Outcome.effect_size_summary and Outcome.statistical_significance report omega-squared and mu-squared for ANOVA and f² values for PLS SEM paths, although effect sizes for pre–post changes such as Cohen’s d are not provided.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis notes no specific procedures for identifying or handling outliers or conducting sensitivity analyses. However, Outcome.limitation, Outcome.challenge, and Outcome.implication integrate findings with the conceptual framework in Methodology.theoretical_foundation, discussing sample and design constraints, cognitive demands of higher-level strategies, and the affordances and limits of ChatGPT-based logic learning.",
	"total_quality_score": "18",
	"quality_category": "High",
	"key_quality_concerns": "Single-group volunteer design without a control condition and no explicit outlier or sensitivity analyses, combined with a gender-imbalanced sample from one Hong Kong EMI university that limits generalisability.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "Logic learning relied on a GPT-4-based POE bot (LogicalHamster) configured to teach seven reasoning errors and support five strategy types in a single 45–75 minute lab session, but precise GPT-4 version details, system prompts, and parameter settings were not reported, which may constrain exact replication of the LLM behaviour."
}