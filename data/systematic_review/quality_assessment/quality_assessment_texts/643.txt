{
	"study_id": "NR",
	"no": "643",
	"first_author_year": "Jamshed 2024",
	"title_short": "NR",
	"year": "2024",
	"country_region": "India (senior secondary public school)",
	"llm_type_brief": "ChatGPT 3.5 (mobile app)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly specifies its aims (comparing ChatGPT mobile feedback with teacher feedback on grammar errors and examining attitudes), identifies a concrete empirical gap about AI-generated feedback in classroom ESL writing, and situates the work within AI-assisted language learning and error-analysis/action-research frameworks.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, Participant Information.age, Participant Information.discipline, Participant Information.language_proficiency, Participant Information.learning_context, Participant Information.mother_tongue, Participant Information.target_language, and Participant Information.prior_experience_llm provide detailed information on 12th-grade Hindi L1 ESL learners in an Indian senior secondary school, including schooling level, age range, L1/L2, learning context, and years of English study, although sex is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe convenience sampling from one senior secondary public school and purposeful division into experimental and control intact groups with acknowledgment of limited generalizability, and Methodology.sample_size_and_effect reports total N=132 and key ANOVA statistics, but Methodology.power_analysis is NR and no a priori sample size justification is provided.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization indicate non-random allocation of intact classes into experimental and control groups via purposeful sampling without individual-level randomization or allocation concealment, and Study Quality and Reporting.baseline_equivalence notes that although both groups took a pretest, no statistical baseline comparability tests are reported, implying potential selection bias despite clear description of procedures.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, and Outcome.assessment_timepoints describe a two-time-point pretest–posttest design over eight weeks with picture-based narrative writing tasks before and after the intervention, while Outcome.followup_length_and_type is NaN and no delayed posttest or longer-term follow-up of effects is reported.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability explain that grammatical errors were identified via the enhanced Grammarly AES system, described as widely recognized for L2 error detection, and that a 12-item Likert questionnaire measured attitudes, but no empirical reliability or validity indices (e.g., inter-rater reliability, test–retest, Cronbach’s alpha) are reported and Methodology.scoring_procedure_and_rater_training indicates exclusive reliance on automated scoring with no human rater training information.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation give a clear eight-week schedule with four one-hour sessions per week, describing pretest, repeated 200-word picture-based story writing, ChatGPT mobile feedback cycles for the experimental group, and parallel teacher-feedback procedures for the control group, while Intervention.writing_genre, Intervention.writing_stage, Intervention.writing_task_type, Intervention.setting, Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, and Intervention.llm_access_policy provide sufficient detail on tasks, stages, standardized ChatGPT 3.5 prompt use, mobile access, and teacher/LLM roles to enable approximate replication despite Intervention.llm_model_configuration and Intervention.llm_safety_guardrails being NR.",
	"q8_statistical_method_appropriateness_score": "1",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports repeated measures ANOVA with grammatical error type and test time as within-subject factors and group (ChatGPT vs teacher feedback) as a between-subject factor, which broadly aligns with Methodology.research_design, Outcome.primary_outcome_variables as AES-based error counts, Outcome.independent_variables_and_factors specifying group and time, and Methodology.unit_of_analysis at the individual-learner level, but the analyses do not account for intact-class grouping in Methodology.group_assignment_method and do not explicitly model the clustered structure, indicating a potential mismatch with the sampling design.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report repeated measures ANOVA F-values, p-values, and a large partial η² for the main effect of error type, and Study Quality and Reporting.data_preprocessing_and_transformation describes direct use of Grammarly AES error counts, but Study Quality and Reporting.assumption_check_and_data_diagnostics states that checks of ANOVA assumptions (e.g., normality, sphericity, homogeneity) are not described and effect sizes are not systematically reported for all key effects.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR so outlier handling and robustness checks are not discussed, while Outcome.limitation, Outcome.challenge, and Outcome.implication acknowledge constraints such as single-site sampling, short duration, limited error categories, and variability in ChatGPT feedback quality and broadly relate findings to AI-assisted feedback and classroom practice in line with Methodology.theoretical_foundation, but theoretical integration and data-issue discussion remain at a general level.",
	"total_quality_score": "13",
	"quality_category": "Moderate",
	"key_quality_concerns": "Convenience sampling with non-random intact-group allocation and untested baseline equivalence, together with exclusive reliance on Grammarly AES error counts without reported measurement reliability or ANOVA assumption checks, limits causal inference and the validity of the observed ChatGPT versus teacher feedback effects.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT 3.5 was used only via a supervised mobile app workflow with a fixed grammar-error prompt during in-class revision, with no reported model access date or safeguards against overreliance or plagiarism and outcomes restricted to AES-detected error counts in eight grammatical categories, which constrains both reproducibility and the breadth of L2 writing effects captured."
}