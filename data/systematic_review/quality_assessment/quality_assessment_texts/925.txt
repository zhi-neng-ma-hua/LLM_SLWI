{
	"study_id": "NR",
	"no": "925",
	"first_author_year": "Hwang 2022",
	"title_short": "NR",
	"year": "2022",
	"country_region": "Taiwan",
	"llm_type_brief": "Generative AI (unspecified model)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims and Basic Identification.research_gap_or_novelty, the study clearly specifies multiple aims (app development, group comparison, behavior correlations, perceptions) and a concrete gap around combining recognition technologies with generative AI, while Methodology.theoretical_foundation notes grounding in EFL writing, recognition technologies, and generative AI research, even though no formal learning theory is named.",
	"q2_participant_info_score": "1",
	"q2_participant_info_notes": "Participant Information.educational_level, learning_context, target_language, and discipline identify undergraduate engineering EFL learners, but Participant Information.language_proficiency is general and Participant Information.mother_tongue, age, and sex are NR, so key language background and demographic details are incomplete.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sample_size_and_effect describe a convenience sample of two intact engineering classes (N=71) with reported pre/post statistics, but Methodology.power_analysis is NR and Methodology.sampling_frame_and_representativeness only notes the single-faculty context without an explicit discussion of representativeness or sample size justification.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization describe a quasi-experimental design with two intact classes assigned to experimental and control groups without randomization, and Study Quality and Reporting.baseline_equivalence reports non-significant pre-test differences plus use of pre-test scores as ANCOVA covariates, making allocation procedures and baseline comparability explicit while acknowledging potential selection bias.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration, and Outcome.assessment_timepoints indicate a six-week pre-test/post-test design with measurements at two time points only, and Outcome.followup_length_and_type notes there was no delayed follow-up, so only short-term change is examined.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report pre- and post-test writing tasks scored with a TOEFL-based rubric by three experienced raters with high ICCs (0.890 pre-test, 0.799 post-test), and Methodology.scoring_procedure_and_rater_training confirms multi-rater scoring, providing clear evidence of reliability and basic validity for the L2 writing measures despite limited detail on rater training procedures.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation outline the six-week sequence of app-based descriptive writing tasks with specified pre-test, practice cycles, and post-test, while Intervention.writing_task_type and Intervention.writing_stage describe photo-based vocabulary listing, description, essay writing, and revision; LLM-related fields (Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.role_llm, Intervention.llm_access_policy, Intervention.training_support_llm_literacy) describe how AI-generated sample sentences and grammar feedback are embedded and accessed, giving sufficient detail for approximate replication even though the backend model is unspecified.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method states that ANOVA was used to compare pre-test scores, ANCOVA compared post-test writing scores between groups with pre-test scores as covariate, and Pearson correlations plus stepwise regression related learning behaviors to post-test performance within the experimental group, which is appropriate for the Outcome.primary_outcome_variables and Outcome.independent_variables_and_factors given the individual-learner unit described in Methodology.unit_of_analysis.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report F-values, p-values, partial eta squared, correlation coefficients, and R² for main analyses, and Study Quality and Reporting.assumption_check_and_data_diagnostics notes that homogeneity of variance was satisfied for ANCOVA, but other assumptions and data diagnostics, as well as data handling described in Study Quality and Reporting.data_preprocessing_and_transformation, are reported only in a general way without detailed tests or treatment of potential violations.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Outcome.limitation and Outcome.challenge acknowledge constraints such as holistic scoring, home-based learning during COVID-19, single-university engineering undergraduates, and technology-related practical issues, and Outcome.implication links the findings to the potential of combining recognition technologies with generative AI for EFL writing, yet Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR and no explicit handling of outliers or robustness checks against aberrant data patterns is described.",
	"total_quality_score": "15",
	"quality_category": "Moderate",
	"key_quality_concerns": "Quasi-experimental intact-class design without randomization and no reporting of attrition or outlier handling, with only short-term pre–post measurement and limited statistical diagnostics.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "Generative AI in Smart UEnglish is provided via an unspecified backend model accessed only through an automated AI-GS function for the experimental group, with no detailed model/version description and minimal explicit training in critical evaluation of AI outputs, which constrains reproducibility and interpretation of LLM-specific effects."
}