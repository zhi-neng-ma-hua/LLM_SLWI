{
	"study_id": "NR",
	"no": "372",
	"first_author_year": "Andewi 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Indonesia",
	"llm_type_brief": "ChatGPT",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states its aims (comparing ChatGPT prompting with lecturer interaction on proficiency, writing, and self-efficacy and examining prompting strategies and challenges), identifies a specific research gap, and grounds the work in sociocultural and self-regulated learning theories including the scaffolding inversion concept.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information fields provide detailed demographics and language background, including educational level (fourth-semester undergraduates), discipline (Information Systems), proficiency distribution (beginner/intermediate/advanced), EFL learning context, target language, mother tongue (Bahasa Indonesia), age distribution, and sex breakdown for each group.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe purposive sampling of 60 eligible students followed by stratified random assignment to ChatGPT and control groups, and Methodology.sample_size_and_effect reports group sizes and effect estimates, but Methodology.power_analysis is NR and no explicit sample size or power justification is provided even though representativeness limitations are acknowledged.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization state that students were stratified by proficiency and randomly assigned to experimental and control conditions taught by the same lecturer with parallel tasks, while Study Quality and Reporting.baseline_equivalence notes nearly identical pretest means and use of pretest scores as covariates, supporting clear allocation procedures and baseline comparability.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design and Intervention.duration describe a pretest–posttest design over approximately nine weeks, and Outcome.assessment_timepoints and Outcome.followup_length_and_type specify pre-intervention and immediate post-intervention assessments for proficiency, writing, and self-efficacy, with analyses using paired t-tests and ANCOVA to examine change over time, although no delayed follow-up was conducted.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability report expert review and piloting of the English Proficiency Test, essay rubric, and self-efficacy questionnaire, acceptable reliability for the proficiency test, good internal consistency for self-efficacy (Cronbach’s α=0.85), and substantial inter-rater reliability for essay scores (Cohen’s κ=0.81), with scoring procedures and consensus described in Methodology.scoring_procedure_and_rater_training.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation outline week-by-week procedures for pretests and the eight-week intervention, and Intervention.writing_stage, Intervention.writing_task_type, and Intervention.setting specify the types and stages of writing tasks in the course; LLM-related fields (Intervention.llm_model_type, Intervention.llm_model_configuration, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.llm_safety_guardrails) together give detailed information on how ChatGPT was accessed, what prompting strategies were used, and how teacher and AI roles were organized, enabling broad replication despite absence of an exact model version.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method describes the use of paired-samples t-tests for within-group pre–post comparisons and ANCOVA for between-group posttest comparisons with pretest scores as covariates, which aligns with the two-group pretest–posttest design; Outcome.primary_outcome_variables and Outcome.independent_variables_and_factors match these analyses, and Methodology.unit_of_analysis confirms that the student-level scores were the analysis unit appropriate to these methods.",
	"q9_assumption_and_effect_size_score": "1",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary report ANCOVA F values, p values, and partial η² for English proficiency, writing performance, and self-efficacy, but Study Quality and Reporting.assumption_check_and_data_diagnostics notes only general justification of using ANCOVA and parametric tests without detailing checks of homogeneity of regression slopes, residual normality, or other diagnostics, and Study Quality and Reporting.data_preprocessing_and_transformation does not describe additional data transformations or assumption-related processing.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, indicating no reported outlier handling or sensitivity analyses, although Outcome.limitation, Outcome.challenge, and Outcome.implication, together with Methodology.theoretical_foundation, provide an integrated interpretation that links quantitative and qualitative findings to sociocultural and self-regulated learning frameworks and discusses constraints such as cognitive offloading and limited generalizability.",
	"total_quality_score": "17",
	"quality_category": "High",
	"key_quality_concerns": "Single-institution, relatively small and predominantly beginner-level sample and lack of reported power analysis or detailed ANCOVA assumption and outlier diagnostics limit the generalizability and robustness of the quantitative conclusions.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "ChatGPT support involved substantial in- and out-of-class prompting with recorded chat logs but unspecified model version and parameters and only procedural (rather than technical) safety guardrails, which restricts exact reproducibility of the AI component and may affect interpretation of observed learning gains."
}