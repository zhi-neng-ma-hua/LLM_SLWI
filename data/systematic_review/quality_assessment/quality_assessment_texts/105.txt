{
	"study_id": "10.1007/s44217-025-00700-6",
	"no": "105",
	"first_author_year": "Jmaiel 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Saudi Arabia (Northern Border University, Arar)",
	"llm_type_brief": "ChatGPT (unspecified version)",
	"q1_research_aims_clarity_score": "2",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims, Basic Identification.research_gap_or_novelty, and Methodology.theoretical_foundation, the study clearly states its goals (overall ESP writing gains and gender/major differences), identifies a specific gap in ChatGPT-supported ESP writing in Saudi higher education, and situates the design within a self-directed learning framework.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, discipline, age, language_proficiency, learning_context, sex, and target_language show that the sample consists of 18–21-year-old Saudi undergraduates in ESP courses (E-commerce, Legal Assistant, IT) at CEFR A1–B1 with English as L2, although Participant Information.mother_tongue and prior_experience_llm are NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sampling_frame_and_representativeness describe non-probability convenience and purposive sampling of 117 ESP students at one university and acknowledge limited generalizability, while Methodology.sample_size_and_effect reports N and large effects but Methodology.power_analysis is NR and no explicit sample-size justification is provided.",
	"q4_group_allocation_and_bias_score": "2",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method specifies a one-group pretest–posttest design in which all 117 students received the ChatGPT-supported intervention and gender/major are naturally occurring factors, and Study Quality and Reporting.risk_of_bias_randomization explicitly notes high selection bias and the absence of a control group, so allocation and associated bias are clearly described.",
	"q5_longitudinal_design_score": "1",
	"q5_longitudinal_design_notes": "Methodology.research_design and Outcome.assessment_timepoints indicate a two-month one-group pre-test–post-test design with ESP writing assessed before and after ChatGPT-supported self-directed learning and Outcome.followup_length_and_type marked NA, so only short-term change is examined without delayed follow-up.",
	"q6_measurement_reliability_validity_score": "1",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability describe expert-reviewed ESP email tasks and an analytic rubric for content, structure, grammar, and cohesion, but no quantitative reliability indices (e.g., inter-rater reliability) are reported and Methodology.scoring_procedure_and_rater_training does not detail rater training or agreement statistics.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation describe a two-month period with specified ESP email scenarios, pre-test and post-test, iterative drafting and revision, peer review, and teacher guidance, while Intervention.llm_model_type, Intervention.llm_integration_mode, Intervention.prompting_strategy, Intervention.training_support_llm_literacy, Intervention.role_llm, Intervention.role_instructor, Intervention.llm_access_policy, Intervention.writing_stage, Intervention.writing_task_type, and Intervention.setting together provide detailed information on how ChatGPT was trained, accessed, and used during drafting and revising, allowing basic replication despite missing model configuration details.",
	"q8_statistical_method_appropriateness_score": "2",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method reports use of Wilcoxon signed-rank tests for pre–post comparisons and ANOVA/Welch’s ANOVA to examine time effects and interactions with gender and major, with individual student writing scores as the unit of analysis (Methodology.unit_of_analysis) and time, gender, and academic major as independent variables (Outcome.independent_variables_and_factors), which aligns with the one-group repeated-measures design and continuous subskill scores (Outcome.primary_outcome_variables).",
	"q9_assumption_and_effect_size_score": "2",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance and Outcome.effect_size_summary show that the study reports p-values, large rank-biserial correlations for Wilcoxon tests, and eta-squared for ANOVA, while Study Quality and Reporting.assumption_check_and_data_diagnostics notes Levene’s tests for homogeneity and the use of Welch’s ANOVA when variances were unequal; Study Quality and Reporting.data_preprocessing_and_transformation indicates direct analysis of rubric scores without additional transformations, so key assumptions and effect sizes are at least partially addressed.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, but Outcome.limitation, Outcome.challenge, and Outcome.implication discuss the one-group design, sample constraints, variance heterogeneity, and potential risks of ChatGPT, and link findings to self-directed learning and AI-supported instruction as outlined in Methodology.theoretical_foundation, providing some theoretical and methodological integration without explicit outlier handling or sensitivity analyses.",
	"total_quality_score": "16",
	"quality_category": "High",
	"key_quality_concerns": "One-group pretest–posttest design with convenience sampling and no control group, together with the absence of reported inter-rater reliability for writing scores, limits causal inference and measurement precision.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "Yes",
	"design_note_llm_specific": "Students used ChatGPT as a self-directed ESP writing assistant over two months with training in prompt creation, but the model version, detailed access conditions, and monitoring of actual ChatGPT usage were not reported, which constrains reproducibility and interpretation of AI exposure intensity."
}