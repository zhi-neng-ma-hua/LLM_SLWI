{
	"study_id": "NR",
	"no": "258",
	"first_author_year": "Al-Obaydi 2025",
	"title_short": "NR",
	"year": "2025",
	"country_region": "Iraq and Czech Republic",
	"llm_type_brief": "ChatGPT (GPT-4)",
	"q1_research_aims_clarity_score": "1",
	"q1_research_aims_clarity_notes": "Based on Basic Identification.research_aims and Basic Identification.research_gap_or_novelty, the study clearly aims to compare ChatGPT-based peer assessment with human peer assessment for EFL college writing and identifies a gap in cross-context comparisons, while Methodology.theoretical_foundation notes general grounding in feedback, peer assessment, AI in higher education and engagement literature but no single explicit theoretical framework.",
	"q2_participant_info_score": "2",
	"q2_participant_info_notes": "Participant Information.educational_level, language_proficiency, learning_context, age, sex, discipline and target_language describe fourth-year EFL undergraduates aged 22–23 in Iraq and the Czech Republic with balanced gender distribution in an EFL higher-education context, although mother_tongue is NR.",
	"q3_sampling_and_power_score": "1",
	"q3_sampling_and_power_notes": "Methodology.sampling_method and Methodology.sample_size_and_effect report purposive sampling of 16 fourth-year students from two universities, volunteers randomly chosen and assigned to two groups of n=8 with equal national representation, and Methodology.sampling_frame_and_representativeness acknowledges limited generalizability, but Methodology.power_analysis is NR and no formal sample size or power justification is provided.",
	"q4_group_allocation_and_bias_score": "1",
	"q4_group_allocation_and_bias_notes": "Methodology.group_assignment_method and Study Quality and Reporting.risk_of_bias_randomization indicate that volunteers were selected and then organised into Group A and Group B with matching demographic attributes but without clearly reported full randomisation, and Study Quality and Reporting.baseline_equivalence notes only general similarity of demographics without statistical tests of baseline writing ability, leaving residual selection and allocation bias concerns.",
	"q5_longitudinal_design_score": "2",
	"q5_longitudinal_design_notes": "Methodology.research_design, Intervention.duration and Outcome.assessment_timepoints show that four compositions (C1–C4) were written and assessed weekly over about one month, and Methodology.sample_size_and_effect plus Outcome.application_effectiveness_overview describe progressive score increases across these timepoints, although Outcome.followup_length_and_type confirms there was no long-term post-study follow-up.",
	"q6_measurement_reliability_validity_score": "2",
	"q6_measurement_reliability_validity_notes": "Methodology.data_collection_instrument and Methodology.data_collection_validity_reliability describe an analytic writing rubric and teacher evaluation sheet reviewed by international specialists for face and content validity, with rubric reliability coefficient reported as 0.879, and Methodology.scoring_procedure_and_rater_training details student training and calibration for rubric use and standardised ChatGPT-4 prompting, although reliability of the evaluation sheet itself is not quantified.",
	"q7_intervention_procedure_and_duration_score": "2",
	"q7_intervention_procedure_and_duration_notes": "Intervention.duration and Intervention.intervention_implementation clearly outline four weekly composition and assessment cycles, Intervention.writing_stage, writing_task_type and setting describe when and where writing and assessment occurred, and LLM-related fields (Intervention.llm_model_type, llm_model_configuration, llm_integration_mode, prompting_strategy, training_support_llm_literacy, role_llm, role_instructor, llm_access_policy, llm_safety_guardrails) provide concrete information on how ChatGPT-4 was used as an AI peer assessor, supporting basic replication.",
	"q8_statistical_method_appropriateness_score": "1",
	"q8_statistical_method_appropriateness_notes": "Methodology.data_analysis_method indicates that quantitative analysis relied on descriptive compilation and comparison of rubric scores across four compositions and groups without inferential tests, which is consistent with the small exploratory pilot but does not fully exploit the two-group repeated-measures design described in Methodology.research_design, Outcome.primary_outcome_variables and Methodology.unit_of_analysis.",
	"q9_assumption_and_effect_size_score": "0",
	"q9_assumption_and_effect_size_notes": "Outcome.statistical_significance states that no inferential statistics (e.g., t-tests, ANOVA) or p-values were reported, Outcome.effect_size_summary is NR, and Study Quality and Reporting.assumption_check_and_data_diagnostics together with Study Quality and Reporting.data_preprocessing_and_transformation indicate only straightforward tabulation of rubric scores without reported checks of statistical assumptions or effect size estimation.",
	"q10_outliers_and_interpretation_score": "1",
	"q10_outliers_and_interpretation_notes": "Study Quality and Reporting.outlier_treatment_and_sensitivity_analysis is NR, yet Outcome.limitation and Outcome.challenge discuss major design and implementation constraints (very small N, descriptive analysis, procedural differences and ethical issues), and Outcome.implication together with Methodology.theoretical_foundation link findings to broader work on feedback, peer assessment, AI in higher education and multidimensional engagement, providing some integrative interpretation without explicit treatment of outliers or robustness analyses.",
	"total_quality_score": "13",
	"quality_category": "Moderate",
	"key_quality_concerns": "Very small purposive pilot sample with unclear group randomisation and no inferential statistics or effect sizes, so estimated differences between AI-based and human peer assessment are descriptive and have limited internal validity and generalisability.",
	"include_in_main_synthesis": "Yes",
	"include_in_meta_analysis": "No",
	"design_note_llm_specific": "ChatGPT-4 was used as an AI peer assessor via a standardised rubric-aligned prompting sequence on students’ own compositions under classroom supervision, but detailed model configuration, calibration of AI scores against human ratings, and any control of out-of-class LLM use were not reported, limiting reproducibility and interpretation of AI feedback effects."
}