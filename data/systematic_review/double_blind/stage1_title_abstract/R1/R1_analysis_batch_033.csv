Title,Year,Decision,Notes
A Case Study of Implementing Generative Ai in University’s General English Courses,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are EFL learners in Korean university-level general English courses, clearly indicating L2 English learners in an EFL context.""}, ""c2"": {""status"": ""unclear"", ""evidence"": ""The abstract mentions a 'generative AI-based instruction model' but does not specify that it is an LLM (e.g., ChatGPT, GPT-4) or provide details on the specific tool or model architecture. It could be any generative AI, not necessarily an LLM.""}, ""c3"": {""status"": ""unclear"", ""evidence"": ""The study refers to 'English instruction for writing and speaking' and 'supporting linguistic proficiency,' but the primary stated focus is on affective factors (motivation, interest, confidence). It is not clear that writing competence or writing-related variables are the primary focus of the intervention or analysis.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The abstract specifies that the research investigated effects on affective factors (motivation, interest, confidence). It does not indicate that any quantifiable writing outcome metrics were collected or reported; writing is only mentioned as a domain of instruction, not as an assessed outcome.""}}"
Detecting and Assessing Ai-generated and Human-produced Texts: the Case of Second Language Writing Teachers,2024,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""The focus is on second language writing teachers evaluating texts written by various authors (NS lecturer, NS student, NNS student, NNS lecturer, ChatGPT). There is no indication that the participants are L2 English learners; rather, they are teachers assessing texts.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""ChatGPT is used to generate one of the essays, but there is no experimental or quasi-experimental integration of an LLM into writing instruction or learners\u2019 writing processes. The study examines detection and assessment of AI vs. human texts, not an LLM-mediated pedagogical intervention.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on teachers\u2019 ability to detect and assess AI-generated versus human-produced texts, not on improving learners\u2019 writing competence through an instructional intervention. It is an assessment/detection study rather than a writing pedagogy study.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Although scores are analyzed, they pertain to teacher ratings of different source texts (AI vs. human) and teachers\u2019 identification strategies, not to changes in L2 learners\u2019 writing outcomes following an LLM-based intervention. No quantifiable learner writing gains are reported.""}}"
The Impact of Chatgpt on English Language Learners’ Writing Skills: an Assessment of Ai Feedback on Mobile,2024,include,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are described as English as a second language (ESL) learners in a senior secondary public school in India. The focus is explicitly on English writing skills and common English grammatical errors.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study uses ChatGPT\u2019s mobile application as the intervention, providing AI feedback on students\u2019 writing. It employs a quasi-experimental design comparing ChatGPT feedback with traditional teacher feedback, clearly integrating an LLM into writing instruction.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The primary focus is on ESL writing skills, including grammar and composition proficiency, and the impact of ChatGPT feedback on common writing errors. This is a pedagogical intervention in writing, not an automated scoring study.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""Quantitative writing outcomes are reported: pre- and post-test story-writing tasks, with measured reduction in specific error types and improved writing proficiency in the experimental group compared to the control group.""}}"
Large Language Models and Automated Essay Scoring of English Language Learner Writing: Insights into Validity and Reliability,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The study uses 119 essays from an English language placement test written by English language learners, indicating an ELL/L2 English population.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""LLMs (PaLM 2, Claude 2, GPT-3.5, GPT-4) are used solely for automated essay scoring, not as part of an instructional or quasi-experimental writing intervention. There is no integration of LLMs into teaching or learners\u2019 writing processes.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on the validity and reliability of LLM-based automated essay scoring, not on improving writing competence or implementing a pedagogical writing intervention.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports reliability and validity metrics of LLM scoring, not quantifiable outcomes of learners\u2019 writing performance resulting from an LLM-mediated intervention.""}}"
Evaluating Cami Ai across Samr Stages: Students’ Achievement and Perceptions in Efl Writing Instruction,2024,unsure,"{""c1"": {""status"": ""pass"", ""evidence"": ""The abstract explicitly states that participants are 126 EFL (English as a Foreign Language) university students, indicating L2 English learners in an EFL context, with a focus on EFL writing instruction.""}, ""c2"": {""status"": ""unclear"", ""evidence"": ""The study uses \u201cCami, an AI-powered tool\u201d and refers to \u201cCami AI technology,\u201d but the abstract does not specify whether Cami is a large language model (e.g., ChatGPT-like, transformer-based generative model) or another type of AI (e.g., annotation, feedback, or grading tool). Without clarification that it is LLM-based, it is unclear if it meets the LLM intervention criterion.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is explicitly \u201cEFL writing instruction,\u201d and the study examines the impact of Cami AI-SAMR implementation on EFL students\u2019 writing achievement, indicating a primary focus on writing competence within a pedagogical intervention.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""The abstract reports that Cami AI-SAMR implementation \u201csignificantly impacted EFL students\u2019 writing achievement,\u201d implying quantitative measures of writing performance as outcomes, alongside qualitative perceptions.""}}"
Advancing Efl Writing Proficiency in Jordan: Addressing Challenges and Embedding Progressive Strategies,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 150 Jordanian EFL students majoring in English across three public universities, clearly an EFL (L2 English) context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study is descriptive and exploratory, using surveys and semi-structured interviews. AI, including potential LLM tools, is only mentioned as a prospective strategy; no actual AI/LLM-based intervention or experimental/quasi-experimental design is implemented.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although the focus is on EFL writing challenges and strategies, there is no implemented writing instruction or intervention using LLMs; AI is discussed conceptually as a possible future tool, not as a tested pedagogical context.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports challenges and proposed strategies but does not report quantifiable writing outcome metrics from an AI/LLM-mediated intervention. Data are survey statistics and thematic analysis, not experimental writing performance outcomes linked to LLM use.""}}"
Can Chatgpt Reliably and Accurately Apply a Rubric to L2 Writing Assessments? the Devil Is in the Prompt(s),2024,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract refers to 'second language (L2) writing' and is published in the Journal of Technology and Chinese Language Teaching. It is unclear whether the target language is English or Chinese, and no explicit mention of ESL/EFL/ELL or English writing is made.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study investigates ChatGPT as an assessment tool, focusing on accuracy and reliability of AI-generated scores compared to human raters and the impact of prompting strategies. There is no indication of an instructional or quasi-experimental pedagogical intervention integrating ChatGPT into L2 writing instruction or writing processes.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on L2 writing assessment\u2014using ChatGPT to score writing and examining reliability and accuracy. This aligns with automated essay scoring functionality rather than a pedagogical writing intervention aimed at improving learners\u2019 writing competence.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Outcomes reported concern the accuracy and reliability of ChatGPT\u2019s scores relative to human raters and across topics, not quantifiable changes in learners\u2019 writing performance following an LLM-mediated intervention.""}}"
Paraphrase or Plagiarism? Exploring Eap Students’ Use of Source Material in a Transnational University Context,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The abstract explicitly states that the study investigates English as a Second Language (ESL) student writers in an English for Academic Purposes (EAP) context, which fits the target L2 English learner population.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""Although the abstract mentions Generative AI and students\u2019 confidence in technological tools, there is no indication of an experimental or quasi-experimental intervention integrating an LLM (e.g., ChatGPT) into writing instruction or processes. The study is exploratory, focusing on students\u2019 paraphrasing practices and perceptions, not on an LLM-based instructional treatment.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The primary focus is on academic writing practices\u2014specifically paraphrasing, plagiarism, and use of source material in EAP writing\u2014clearly a writing-related context rather than automated scoring or non-pedagogical evaluation.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study uses a text-based interview method and a custom-designed writing task to explore how students make decisions about source use. The abstract reports qualitative insights (e.g., focus on sentence-level paraphrasing, low confidence in tools) but does not mention any quantifiable writing outcome metrics or experimental measures of intervention effectiveness.""}}"
Improving Writing Feedback for Struggling Writers: Generative Ai to the Rescue?,2024,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract mentions students with and without disabilities who struggled with writing and notes that AI feedback did not reflect student characteristics such as ELL status. However, it does not specify that the target population is L2 English learners in ESL/EFL/ELL contexts, nor that the analysis focuses specifically on English as an L2.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study imports existing student essays into two versions of ChatGPT and analyzes the feedback and instructional suggestions generated, comparing them with teachers\u2019 feedback. This is an evaluation of AI-generated feedback quality, not an experimental or quasi-experimental pedagogical intervention integrating LLMs into writing instruction or writing processes for learners.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although the context is writing instruction and feedback, the primary focus is on comparing AI and teacher feedback via thematic analysis, not on implementing an LLM-mediated instructional intervention aimed at improving learners\u2019 writing competence. It is more a functionality/feasibility study of AI feedback than a pedagogical intervention study.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study uses inductive thematic analysis of AI and teacher responses. There is no indication of quantitative writing outcome measures (e.g., pre/post writing scores, rubric-based gains) assessing the effectiveness of an LLM-mediated intervention on student writing performance.""}}"
Efl Learners’ Attitudes towards Utilizing Chatgpt for Acquiring Writing Skills in Higher Education: a Case Study of Computing Students,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are EFL learners: fifty-two students at a private university in Sharjah taking an English language course. The focus is on acquisition of English writing skills, clearly an EFL/ESL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""Although ChatGPT is mentioned, the study is described as exploring students\u2019 attitudes towards utilizing ChatGPT. There is no indication of an experimental or quasi-experimental instructional intervention integrating ChatGPT into writing instruction; rather, it is an attitudinal case study using a questionnaire and interviews.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is explicitly writing-focused: acquisition of writing skills in English as a foreign language and ChatGPT-based essay writing versus self-dependent essay writing. The primary focus is on writing competence and related skills.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Outcomes reported are students\u2019 attitudes and perceptions (e.g., time-saving, language accuracy, limited progress in higher-order thinking). There is no mention of quantifiable writing performance measures (e.g., writing scores, rubric-based assessments) used to evaluate the effectiveness of ChatGPT-mediated writing intervention.""}}"
Detecting Chatgpt-generated Essays in a Large-scale Writing Assessment: Is There a Bias Against Non-native English Speakers?,2024,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""Although the study involves GRE writing assessment data that likely includes non-native English speakers, the focus is not on L2 learners\u2019 development or instruction but on detector bias. Participants are not clearly framed as L2 English learners in ESL/EFL/ELL contexts for pedagogical purposes.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study develops detectors of ChatGPT-generated essays using linguistic and perplexity features. ChatGPT is only the source of generated texts; there is no experimental or quasi-experimental integration of an LLM into writing instruction or writing processes as an intervention.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on AI-generated text detection performance and bias, not on improving writing competence or writing-related pedagogical variables. It is essentially an assessment/detection study, not a writing intervention.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""No quantifiable writing outcome metrics related to learners\u2019 writing development or performance under an LLM-mediated intervention are reported. Outcomes concern detector accuracy and bias, not changes in writing quality or related constructs.""}}"
Testing the Viability of Chatgpt as a Companion in L2 Writing Accuracy Assessment,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The study uses the Cambridge Learner Corpus First Certificate in English (CLC FCE), which consists of L2 English learner writing. The focus is clearly on L2 English writing accuracy.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""ChatGPT is used as an automated evaluator of linguistic accuracy, not as part of an experimental or quasi-experimental instructional intervention in which learners use the LLM during writing or receive LLM-mediated instruction.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on validating ChatGPT as an automated accuracy assessment tool, comparing it to human raters and Grammarly. There is no pedagogical writing intervention; it is a functionality/assessment study rather than a teaching or learning context.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports correlations between ChatGPT\u2019s accuracy assessments and human ratings, but does not report writing outcome changes resulting from an LLM-mediated intervention. No experimental manipulation of instruction or learner use of ChatGPT is described.""}}"
"A Meta-analysis of Effects of Automated Writing Evaluation on Anxiety, Motivation, and Second Language Writing Skills",2024,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract refers broadly to second language (L2) writing skills but does not specify that the population is L2 English learners in ESL/EFL/ELL contexts, nor that the target language is English specifically.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study is a PRISMA-based meta-analysis of automated writing evaluation (AWE) technologies in general, not specifically of LLM-based tools (e.g., ChatGPT, GPT-4). It also does not describe an experimental or quasi-experimental intervention conducted by the authors; instead, it synthesizes prior studies.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although the focus includes L2 writing skills, the article is a meta-analysis rather than a primary study implementing a pedagogical intervention. Review and meta-analytic studies are to be excluded by design.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The article aggregates effects of AWE on anxiety, motivation, and L2 writing skills but does not itself report original experimental outcome data from an LLM-mediated writing intervention. As a meta-analysis, it falls under excluded publication types.""}}"
Understanding Efl Students’ Use of Self-made Ai Chatbots as Personalized Writing Assistance Tools: a Mixed Methods Study,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 69 Chinese undergraduate EFL students, clearly L2 English learners in an EFL context, and the focus is on English writing (\u2018learning to write\u2019, \u2018writing processes\u2019).""}, ""c2"": {""status"": ""pass"", ""evidence"": ""Students created self-made RAG chatbots using Poe to assist with their writing processes. These are LLM-based chatbots integrated as tools in a pedagogical workshop, fitting the LLM intervention criterion.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is explicitly writing-focused: chatbots assist with idea generation, outlines, and error identification in writing. The study examines chatbots as pedagogical tools for personalized writing assistance, not as scoring systems.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Reported outcomes are motivational and attitudinal: clearer writing goals, increased confidence, reinforced beliefs, and more positive attitudes. The abstract does not indicate any quantitative writing performance metrics (e.g., scores, quality ratings, accuracy measures); the focus is on motivation rather than measurable writing competence.""}}"
Chatgpt for L2 Learning: Current Status and Implications,2024,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""The article is a review of 44 studies on ChatGPT for L2 learning, not an empirical study with its own participant sample. Thus, it does not itself involve a defined population of L2 English learners as participants in an intervention.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""This is a systematic/overview-type study summarizing prior research on ChatGPT for L2 learning. It does not implement an experimental or quasi-experimental LLM-based writing intervention itself; it only reviews others\u2019 interventions.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although writing skills are mentioned as a major objective in the reviewed studies, the article\u2019s primary focus is synthesizing literature across multiple dimensions (roles, theories, methods, outcomes), not conducting a specific writing-focused pedagogical intervention.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study does not report original, quantifiable writing outcome metrics from an intervention; it summarizes benefits, challenges, and outcomes from other studies. As a review article, it falls under the exclusion criteria for non-primary research.""}}"
"The Metaphor of Ai in Writing in English: a Reflection on Efl Learners’ Motivation to Write, Enjoyment of Writing, Academic Buoyancy, and Academic Success in Writing",2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 86 intermediate EFL students from China in an English as a foreign language context, clearly L2 English learners.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The intervention is described as 'automated writing evaluation (AWE)'; no indication is given that this system is an LLM (e.g., ChatGPT, GPT-4, transformer-based generative model). AWE tools are typically non-LLM automated scoring/feedback systems, which fall outside the review\u2019s inclusion criteria.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The study focuses on fostering learners\u2019 writing skills and related constructs (motivation to write, enjoyment of writing, academic buoyancy, academic success in writing), which are writing-related variables in an instructional context.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""The abstract reports quantitative outcomes (one-way MANOVA) showing differences between groups in motivation to write, enjoyment in writing, academic buoyancy, and academic success in writing, indicating measurable writing-related outcomes.""}}"
Effect of Editgpt on the Learners` Autonomy and Learning Anxiety,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants were 30 Omani EFL learners, clearly indicating L2 English learners in an EFL context.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The intervention used EditGPT, described as an AI application and automated writing evaluation system, integrated into instruction for the experimental group. Given the name and context, it is reasonably inferred to be an LLM-based tool.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although EditGPT is used in the context of English writing instruction, the primary focus of the study is on learning anxiety and learner autonomy, not on writing competence or writing-related performance variables.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The outcomes measured are foreign language anxiety and learner autonomy via questionnaires and scales. No quantifiable writing performance or writing quality metrics are reported to assess the effectiveness of the LLM-mediated writing intervention on writing outcomes.""}}"
"Chatgpt-empowered Writing Strategies in Efl Students’ Academic Writing: Calibre, Challenges and Chances",2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 60 Chinese university juniors majoring in English (EFL context), and the focus is explicitly on English academic writing for EFL students.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study investigates current usage, potential applications, limitations, and perceptions of ChatGPT via a questionnaire and focus group interviews. There is no indication of an experimental or quasi-experimental instructional intervention where ChatGPT is systematically integrated into writing instruction or processes with controlled conditions.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is clearly English academic writing and writing strategies (planning, composing, revising) for EFL students, focusing on how ChatGPT can empower academic writing strategies.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Although mixed methods and regression analysis are mentioned, the abstract does not report quantifiable writing outcome metrics (e.g., changes in writing scores, quality, accuracy). The outcomes are about perceived empowerment of writing strategies and reported uses, not measured changes in writing performance following an intervention.""}}"
Exploring the Use of Chatgpt as a Tool for Written Corrective Feedback in an Efl Classroom,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are five Form Four students in a Band 2 secondary school in Hong Kong, clearly an EFL context with English as the target language.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study uses ChatGPT explicitly as a writing feedback-giving tool, integrated into students\u2019 writing and revision sessions, which fits an LLM-based writing intervention.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is EFL writing: students write tasks and revise them based on ChatGPT\u2019s written corrective feedback. The focus is on written corrective feedback and its effectiveness in writing.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The abstract describes exploration of cognitive factors and students\u2019 perceptions via stimulated recall interviews. It does not report any quantitative or experimental writing outcome measures (e.g., scores, accuracy gains, quality ratings) to assess effectiveness; the findings are qualitative (two factors related to effectiveness, students\u2019 valuing of feedback).""}}"
Tapping into the Pedagogical Potential of Infinigochatic: Evidence from Iwrite Scoring and Comments and Lu & Ai’s Linguistic Complexity Analyzer,2024,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are postgraduate students taking an English reading and writing course in Southwest China, clearly functioning as L2 English learners in an EFL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study compares L2 learner writings with versions polished by infinigoChatIC (a ChatGPT-based tool) using a single prompt by the teacher. There is no experimental or quasi-experimental pedagogical intervention where learners use the LLM as part of instruction or their writing process; it is essentially a text-transformation/comparison study.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The focus is on how LLM-polished texts differ from learner texts in quality and complexity, scored by iWrite and analyzed by a complexity analyzer. This is an evaluation of LLM output characteristics rather than a teaching/learning intervention aimed at developing learners\u2019 writing competence.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Although quantitative measures (iWrite scores, lexical and syntactic complexity) are reported, they assess the difference between original and LLM-polished texts, not outcomes of an LLM-mediated instructional intervention on learners\u2019 own writing performance over time.""}}"
