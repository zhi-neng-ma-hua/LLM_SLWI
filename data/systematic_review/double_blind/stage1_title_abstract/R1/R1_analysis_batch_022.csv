Title,Year,Decision,Notes
Integrating Automated Writing Evaluation into Saudi Efl Students’ Writing Practice,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 154 Arabic-speaking undergraduate EFL students at a Saudi university, clearly L2 English learners in an EFL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The intervention uses generic 'AWE systems' (Automated Writing Evaluation). The abstract does not indicate that these are LLM-based tools (e.g., ChatGPT, GPT-4) or transformer-based generative models; AWE is typically scoring/feedback software, not an LLM writing assistant.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The focus is on integrating AWE into EFL writing practice and its impact on writing skills, autonomy, and motivation, which are writing-related variables in a pedagogical context.""}, ""c4"": {""status"": ""unclear"", ""evidence"": ""The study mentions collecting 'samples of EFL writing assignments before and after utilising the AWE system,' but the abstract does not clearly state that quantifiable writing outcome metrics were analyzed; emphasis is on experiences, evaluations, challenges, satisfaction, and perceived effectiveness.""}}"
The Impact of Ai-generated Feedback Explicitness (generic Vs. Specific) on Efl Students' Use of Automated Written Corrective Feedback,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are EFL university students in Saudi Arabia (Arab schools and universities), clearly L2 English learners in an EFL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The intervention is described as AI-driven Automated Written Corrective Feedback and Automated Essay Scoring systems/AWE systems, but there is no indication these are large language model (LLM)-based tools (e.g., ChatGPT, GPT-4). They are generic AWCF/AWE systems, which are often non-LLM. The abstract does not specify transformer-based generative models.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The focus is on EFL writing, corrective feedback, and writing proficiency, clearly within writing competence and writing-related variables in a pedagogical context.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Although the main research question mentions writing proficiency, the reported methods and results focus on perceptions (questionnaires, interviews, exploratory factor analysis of ease of use, clarity, usefulness, perceptions, feedback explicitness). No concrete quantitative writing outcome measures (e.g., scores, accuracy gains) are reported in the abstract.""}}"
Saudi Efl Learners’ Perceptions of Using Artificial Intelligence and Its Impact on Their Writing Skills,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are female Saudi learners of English as a Foreign Language at the University of Jeddah, clearly an EFL context with English as the target language.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""Although ChatGPT and other AI tools are mentioned, the abstract frames the study as examining learners\u2019 perceptions and experiences, not as an experimental or quasi-experimental intervention integrating LLMs into instruction. No controlled or structured LLM-based instructional treatment is described.""}, ""c3"": {""status"": ""unclear"", ""evidence"": ""The focus is broadly on writing skills and writing development, but the study appears to center on perceptions and experiences rather than a defined pedagogical writing intervention. It is not explicit that there is a structured writing-instruction context being experimentally manipulated.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study uses questionnaires and semi-structured interviews to explore attitudes, perceived benefits, and drawbacks. There is no indication of quantifiable writing outcome measures (e.g., pre/post writing scores, rubric-based writing performance) to assess effectiveness of LLM-mediated writing intervention.""}}"
Evaluating the Quality of Ai Feedback: a Comparative Study of Ai and Human Essay Grading,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The abstract specifies that the essays were written by English as a foreign language (EFL) students, which fits the target population of L2 English learners.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""AI tools are used only as evaluators of essays and their feedback quality is compared with human grading. There is no indication of an experimental or quasi-experimental pedagogical intervention integrating LLMs into writing instruction or writing processes.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on the quality and scoring behavior of AI essay grading/feedback compared to human raters, not on improving learners\u2019 writing competence through an instructional intervention. This aligns with automated essay scoring evaluation rather than a writing intervention.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports on AI vs. human scoring and feedback characteristics, but does not report quantifiable changes in students\u2019 writing outcomes resulting from an LLM-mediated intervention. No pre/post or comparative writing performance measures are described.""}}"
The Integration of Chatgpt in English for Foreign Language Course: Elevating Ai Writing Assistant Acceptance,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants were 95 English as a Foreign Language (EFL) students in Indonesia, clearly indicating L2 English learners in an EFL context.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study integrates ChatGPT, an LLM, into students\u2019 writing process over a four-week assignment, examining its use as a writing aid in an English language learning course.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is writing-related: students used ChatGPT during the writing process as a writing aid in an ICT for ELT course, and the discussion focuses on academic writing and use of ChatGPT in writing tasks.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study applies the Technology Acceptance Model and reports outcomes such as perceived ease of use, perceived usefulness, attitude toward change, and behavioral intentions. There is no mention of quantifiable writing performance or writing quality outcomes; the focus is on acceptance and attitudes, not writing competence.""}}"
Investigating Students’ Uptake of Teacher- and Chatgpt-generated Feedback in Efl Writing: a Comparison Study,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are explicitly described as EFL students: twenty Chinese undergraduate students writing argumentative essays in English. This fits L2 English learners in an EFL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""ChatGPT is used to generate feedback on students\u2019 writing, but the design is a comparison of teacher- vs ChatGPT-generated feedback uptake and perceptions. There is no indication of an experimental or quasi-experimental LLM-based instructional intervention aimed at improving writing performance (e.g., no pre/post or controlled treatment condition using ChatGPT as a pedagogical tool).""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is clearly EFL writing: students compose argumentative essays, receive feedback, and revise. The focus is on writing quality and feedback use, not on automated scoring or system evaluation.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Outcomes reported concern engagement with feedback, appropriateness of revisions, and questionnaire-based perceptions and preferences. The abstract does not report quantifiable writing outcome metrics (e.g., holistic/analytic writing scores, measurable gains in writing competence) to assess the effectiveness of the LLM-mediated intervention.""}}"
"Beyond Policing: Ai Writing Detection Tools, Trust, Academic Integrity, and Their Implications for College Writing",2025,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract mentions bias against non-native English speakers and focuses on college writing, but it does not specify that the participants are L2 English learners in ESL/EFL/ELL contexts, nor that data are specifically about L2 English learners.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The article discusses AI writing detection tools, trust, and academic integrity, and argues for collaborative AI integration in writing. However, it does not describe an experimental or quasi-experimental study using a specific LLM (e.g., ChatGPT, GPT-4) as an instructional intervention; it appears to be conceptual/perspective-based.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""While the context is college writing and AI, the focus is on detection tools, institutional guidelines, and trust-based frameworks rather than a concrete pedagogical intervention targeting writing competence or writing-related variables using LLMs.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The abstract does not report any quantitative writing outcome measures or experimental results. It discusses implications and proposed shifts in approach but no measured effects on writing performance.""}}"
Unpacking the Rejection of L2 Students Toward Chatgpt-generated Feedback: an Explanatory Research,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are L2 learners: 45 university students in a Computer Science program producing an argumentative writing report, described as L2 learners dealing with corrective feedback in an AI-assisted environment.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""ChatGPT, a generative AI LLM, is used to provide corrective feedback on students\u2019 argumentative writing. Students were encouraged to seek corrective feedback from ChatGPT, indicating an LLM-based writing-related intervention.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on learners\u2019 challenges and rejection of ChatGPT-generated feedback, framed through UTAUT (technology acceptance). The abstract does not indicate that the study\u2019s main aim is to improve or measure writing competence; rather, it examines acceptance and reasons for rejecting feedback.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports proportions of accepted vs. rejected AI feedback and qualitative reasons for rejection. It does not report quantifiable writing outcome metrics (e.g., changes in writing quality, accuracy, complexity) to assess the effectiveness of the LLM-mediated intervention.""}}"
Collaborative Writing Based on Generative Ai Models: Revision and Deliberation Processes in German as a Foreign Language,2025,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""Participants are learners of German as a foreign language, not L2 English learners. The title and abstract explicitly state the context is German as a Foreign Language, so the target language is not English.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study integrates generative AI models (e.g., ChatGPT) into a classroom-based collaborative writing intervention, where students compare their own writing with GenAI models. This reflects an LLM-based writing intervention.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The focus is on L2 writing and revision processes in collaborative writing, including how GenAI influences revision and deliberation, clearly centering on writing competence and writing-related variables.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""The study reports changes in text quality and evaluates texts for functional adequacy, indicating quantifiable writing outcome metrics linked to the GenAI-mediated intervention.""}}"
Exploring Chatgpt as a Tool for Thesis Writing: Perspectives of Efl Supervisors in Jordanian Universities,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The population of interest is clearly in an EFL context: the study focuses on Jordanian EFL supervisors and their views on students\u2019 thesis writing in English. Thus, the context involves L2 English learners in an EFL setting, even though the direct participants are supervisors.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""Although ChatGPT (an LLM) is central, the study is not an experimental or quasi-experimental intervention integrating ChatGPT into writing instruction. It is a perception study using a questionnaire to gather supervisors\u2019 views; no structured LLM-mediated instructional treatment is implemented or tested.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The focus is on supervisors\u2019 perspectives about ChatGPT\u2019s usefulness and effects on thesis writing, not on an implemented pedagogical intervention or measured changes in writing competence. It is attitudinal rather than an intervention study targeting writing outcomes.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study reports Likert-scale ratings of supervisors\u2019 opinions (e.g., on usefulness, correctness, dependability) but does not report any quantifiable writing performance outcomes for students. There are no experimental measures of changes in writing quality or related variables following LLM use.""}}"
More Human Than Human? Differences in Lexis and Collocation within Academic Essays Produced by Chatgpt-3.5 and Human L2 Writers,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""The study explicitly involves human L2 writers producing academic essays, indicating a population of second language learners writing in English.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""Although ChatGPT-3.5 is used to generate essays, there is no experimental or quasi-experimental pedagogical intervention integrating the LLM into L2 writing instruction or learners\u2019 writing processes. The design is a comparative corpus analysis of AI vs. human texts, not an instructional intervention.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on linguistic differences (lexis and collocation) between AI-generated and human L2 essays and implications for academic integrity and AI detection, not on improving writing competence through an instructional context.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The study does not report quantifiable writing outcome measures for an LLM-mediated intervention. It analyzes textual features but does not assess changes in learners\u2019 writing performance resulting from using an LLM in instruction.""}}"
Chatgpt and L2 Chinese Writing: Evaluating the Impact of Model Version and Prompt Language on Automated Corrective Feedback,2025,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""The focus is on L2 Chinese writing and the need for Chinese grammar checking. Although prompt language includes English, the target language being learned and corrected is Chinese, not English, so it does not match the review\u2019s requirement for L2 English learners and English writing outcomes.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study evaluates ChatGPT-3.5 and 4.0 (LLMs) as automated corrective feedback tools for L2 Chinese writing, comparing model versions and prompt languages in an experimental setup using erroneous sentences.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is writing-focused: the study examines automated corrective feedback on L2 Chinese written sentences, with teacher ratings of grammaticality, fluency, and related writing-focused dimensions.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""Quantitative evaluations are reported: teacher ratings of corrections and feedback on dimensions such as grammaticality, fluency, minimal alterations, over-correction, correctness, understandability, and detail, providing measurable outcome metrics.""}}"
Using an Ai-powered Chatbot for Improving L2 Korean Grammar: a Comparison between Proficiency Levels and Task Types,2025,exclude,"{""c1"": {""status"": ""fail"", ""evidence"": ""Participants are L2 Korean learners, not L2 English learners. The abstract explicitly states the study is about incorporating an AI chatbot (Iruda) in L2 Korean teaching to improve Korean lexico-grammar, and it is framed as research on a less commonly taught language (LCTL), not English.""}, ""c2"": {""status"": ""unclear"", ""evidence"": ""The tool is described as an AI-powered chatbot (Iruda), but the abstract does not specify whether it is based on a large language model or transformer-based generative architecture. It could be rule-based or another non-LLM system; this cannot be determined from the abstract alone.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on improving Korean lexico-grammar and assessing grammar performance via selected-response and constructed-response tasks. The study is about grammar instruction and grammatical accuracy, not writing competence in English or writing-related variables in an L2 English context.""}, ""c4"": {""status"": ""unclear"", ""evidence"": ""The study reports quantifiable outcomes (item means, t-tests, ANOVAs) on grammar tasks, including constructed-response sentence completion and composition. However, these outcomes pertain to Korean grammar, not L2 English writing, and it is unclear whether the constructed responses are treated as broader writing outcomes or narrowly as grammar test items.""}}"
Interacting with Chatgpt in Essay Writing: a Study of L2 Learners’ Task Motivation,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are L2 learners writing English argumentative essays at a public university in a non-English-speaking country, which fits ESL/EFL/ELL contexts with English as the target language.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study uses ChatGPT 4.0 (an LLM) as a writing support tool and tutor within an experimental, mixed-methods design to inform AI-enhanced interventions for L2 essay writing.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""The primary focus is on learners\u2019 task motivation when interacting with ChatGPT in English essay writing, not on writing competence or writing-related performance variables. Writing skill is mentioned only as a perceived improvement, not as a measured outcome.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The abstract reports quantitative outcomes on motivation, not on writing performance. There is no indication of quantifiable writing outcome metrics (e.g., scores, quality ratings, accuracy) used to assess the effectiveness of the LLM-mediated intervention on writing.""}}"
Empowering Dialogic Feedback in Flw with Llm,2025,unsure,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract refers broadly to foreign/second language (L2) writing and L2 learners but does not specify that the target language is English or that participants are ESL/EFL/ELL learners. The specific language context cannot be confirmed from the abstract alone.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study explicitly proposes leveraging large-language models (LLMs) to create an AI-writing tool and to facilitate dialogic feedback in L2 writing. It mentions testing the tool\u2019s effectiveness through experimental sessions, indicating an experimental or quasi-experimental LLM-based intervention in writing instruction.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The focus is on L2 writing instruction, dialogic feedback, and iterative editing/rewriting to enhance linguistic and cognitive development. The context is clearly writing competence and feedback practices, not automated essay scoring or non-pedagogical evaluation.""}, ""c4"": {""status"": ""unclear"", ""evidence"": ""The abstract states that the study will test the effectiveness of the AI-writing tool and understand the impact on L2 learners\u2019 writing progress, perceptions, and interaction patterns. However, it does not explicitly state that quantifiable writing outcome metrics (e.g., scores, rubric-based gains) will be reported, leaving the nature of outcome measures uncertain.""}}"
Exploring Efl Students’ Prompt Engineering in Human–ai Story Writing: an Activity Theory Perspective,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are 67 Hong Kong secondary school students described as English as a foreign language (EFL) students, clearly indicating L2 English learners in an EFL context.""}, ""c2"": {""status"": ""unclear"", ""evidence"": ""Students used and even created their own generative-AI tools based on open-source language models for story writing, which implies LLM use. However, the abstract does not specify an experimental or quasi-experimental intervention design (e.g., treatment vs. control, pre\u2013post comparison) integrating LLMs into instruction; it appears more exploratory/observational.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The context is short story writing in English, and the study explicitly examines how students prompt generative AI tools during story writing. The primary focus is on writing processes and related variables (purposes for prompting, activity systems) rather than on automated scoring or non-pedagogical evaluation.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""Although the study mentions the \u2018quality of their stories\u2019 as a characteristic of students\u2019 activity systems, the abstract does not indicate any structured, quantifiable writing outcome metrics used to assess the effectiveness of an LLM-mediated intervention. The focus is on thematic analysis of purposes and activity systems, not on measured writing gains or experimental outcomes.""}}"
Ai-assisted Feedback in Clil Courses as a Self-regulated Language Learning Mechanism: Students’ Perceptions and Experiences; Retroalimentación Asistida Por Ia En Cursos Clil Como Mecanismo De Aprendizaje Autorregulado De Idiomas: Percepciones Y Experiencias De Estudiantes,2025,unsure,"{""c1"": {""status"": ""unclear"", ""evidence"": ""Participants are university students in a business CLIL course, but the abstract does not explicitly state that they are L2 English learners or that English is the target language. CLIL often implies L2 use, yet the specific language (English vs. another L2) is not identified.""}, ""c2"": {""status"": ""pass"", ""evidence"": ""The study integrates ChatGPT (an LLM) into a 15-week Data Description writing course. Students used ChatGPT weekly to receive criteria-based feedback on compositions and revised drafts accordingly, which constitutes an LLM-mediated writing intervention.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The primary focus is on writing: a Data Description writing course, weekly compositions, and AI-assisted feedback on content, grammar, and vocabulary. The study examines improvements in writing and linguistic enhancement, not automated scoring or non-pedagogical evaluation.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""Beyond survey data on perceptions, the study reports that a sample of 336 compositions was coded and analyzed to evaluate linguistic enhancement, and it notes significant improvement in content accuracy and linguistic proficiency, indicating quantitative writing outcome measures.""}}"
Empowering Efl Learners: Assessing the Ai Brainstorming Tools' Impact on Essay Writing Proficiency,2025,exclude,"{""c1"": {""status"": ""pass"", ""evidence"": ""Participants are thirty Saudi EFL learners in English Departments, clearly indicating L2 English learners in an EFL context.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The intervention uses an AI brainstorming tool (AYOA). The abstract does not indicate that AYOA is a large language model or transformer-based generative model (e.g., ChatGPT, GPT-4). It appears to be a general AI brainstorming/mind-mapping tool rather than an LLM integrated into writing instruction.""}, ""c3"": {""status"": ""pass"", ""evidence"": ""The focus is on essay writing proficiency: learners brainstorm ideas (with or without AI) and then write essays based on those ideas. The primary outcome is writing performance, fitting the writing competence context.""}, ""c4"": {""status"": ""pass"", ""evidence"": ""The study uses an experimental design with control and experimental groups and reports that the experimental group showed better performance in essay writing, indicating quantifiable writing outcome metrics were collected.""}}"
Efl Teachers and Feedback Fatigue: Ai to the Rescue?,2025,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract discusses EFL teachers and L2 writing in general but does not specify any participant group of L2 English learners or empirical data collection with such learners. It appears to be a conceptual or discussion paper rather than a participant-based study.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The paper \""looks at the contribution of Automated Writing Evaluation (AWE) programmes and Generative Artificial Intelligence (GenAI) to feedback\"" but there is no indication of an experimental or quasi-experimental design integrating a specific LLM (e.g., ChatGPT, GPT-4) into instruction. It reads as a discussion of tools and roles rather than an intervention study.""}, ""c3"": {""status"": ""unclear"", ""evidence"": ""While the focus is on feedback for L2 writing and teacher workload, the abstract does not describe a concrete pedagogical intervention or study context; it appears more like a conceptual or reflective piece on AI-supported feedback and teacher roles.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""No quantifiable writing outcome metrics or experimental measures are mentioned. The abstract raises questions about whether AI can improve writers and not just texts, but does not report any empirical results or structured intervention outcomes.""}}"
Cognitive Load Scale for Ai-assisted L2 Writing: Scale Development and Validation,2025,exclude,"{""c1"": {""status"": ""unclear"", ""evidence"": ""The abstract refers broadly to 'L2 writing' and 'second language (L2) composition' but does not specify that participants are L2 English learners (ESL/EFL/ELL) or that the target language is English. The specific language context is not stated.""}, ""c2"": {""status"": ""fail"", ""evidence"": ""The study focuses on developing and validating a Cognitive Load Scale for AI-assisted L2 writing. It does not describe an experimental or quasi-experimental pedagogical intervention integrating a specific LLM (e.g., ChatGPT, GPT-4) into instruction; rather, it is a measurement instrument development study.""}, ""c3"": {""status"": ""fail"", ""evidence"": ""Although the context is AI-assisted L2 writing, the primary focus is on cognitive load measurement and scale validation, not on improving writing competence or writing-related performance through an instructional intervention. It aims to provide a tool for future pedagogy, not to test a writing intervention itself.""}, ""c4"": {""status"": ""fail"", ""evidence"": ""The outcomes reported are psychometric properties (factor structure, internal consistency, criterion-related validity via correlations with anxiety, self-efficacy, and mental effort). No quantifiable writing performance outcomes (e.g., writing quality scores, accuracy, complexity) are reported to assess effectiveness of an LLM-mediated writing intervention.""}}"
