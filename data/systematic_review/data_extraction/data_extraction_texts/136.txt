{
	"Basic Identification": {
		"author (作者)": "Long Quoc Nguyen, Ha Van Le, Phuc Thinh Nguyen",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Vietnam (private university, FPT University)",
		"journal_name (期刊名称)": "Education and Information Technologies",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal research article; within-subjects mixed-methods experimental study with two writing conditions (ChatGPT vs non-ChatGPT)",
		"doi_or_identifier (DOI或唯一标识)": "10.1007/s10639-024-13231-8",
		"research_aims (研究目的与问题)": "To examine how EFL learners utilize ChatGPT in the pre-writing stage, to determine the extent of their affective engagement with ChatGPT during this stage, to assess the impact of using ChatGPT in the pre-writing phase on their subsequent writing performance, and to explore the correlation between affective engagement with ChatGPT and overall writing performance.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses the scarcity of empirical research on ChatGPT use specifically in the pre-writing stage of L2 writing, the limited understanding of learners’ affective engagement with ChatGPT compared to non-ChatGPT conditions, and the lack of studies linking affective engagement with ChatGPT in planning to subsequent text quality; extends prior work by combining quantitative and qualitative data and using within-subject comparisons."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "First-year university students",
		"language_proficiency (语言熟练度水平)": "B1–B2 level on the CEFR based on the 60-question Oxford Quick Placement Test (OQPT score range 25–48)",
		"mother_tongue (母语)": "Vietnamese",
		"sex (性别)": "42 males and 14 females",
		"age (年龄)": "18–21 years",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in Vietnam",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "English Preparation course at a private university aimed at enhancing academic English writing and reading skills equivalent to B2; specific academic majors not reported",
		"prior_experience_llm (既有LLM使用经验)": "All participants had prior experience using ChatGPT to assist their learning before the study."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Within-subjects design with two counterbalanced writing conditions: a ChatGPT-supported pre-writing condition and a non-ChatGPT pre-writing condition; each participant completed one argumentative writing task with and one without ChatGPT support.",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods: quantitative analyses of writing performance, affective engagement questionnaires, and correlations, complemented by qualitative analysis of ChatGPT logs, note-taking sheets, and semi-structured interview data.",
		"sampling_method (抽样方法)": "Non-random convenience sampling of two intact first-year EFL classes (Class A and Class B) from an English Preparation course at a private Vietnamese university; voluntary participation with written informed consent.",
		"sample_size_and_effect (样本量及效应量)": "Total N = 56 first-year Vietnamese EFL students (42 males, 14 females). A priori power analysis using GPower (α = .05, power = .95, medium effect size f = .25, within-groups ANOVA with two conditions and two writing tasks) indicated a required sample of 54; the actual sample exceeded this. Writing performance: median total scores were higher in the ChatGPT condition (Mdn = 11.00, IQR = 2.00) than in the non-ChatGPT condition (Mdn = 10.50, IQR = 1.50). Linear Mixed-Effects Model with task condition as fixed effect (Score ~ TaskCondition + (1|Participant)) showed significantly higher scores in the ChatGPT condition (non-ChatGPT vs ChatGPT: β = -0.89, SE = 0.24, t = -3.72, p < .001), with Conditional R² = .21. Wilcoxon signed-rank tests indicated significant improvements in content (Z = -4.18, p < .001, r ≈ .56) and vocabulary (Z = -4.28, p < .001, r ≈ .57) but not grammar (Z = -0.36, p = .720) or organization (Z = -0.01, p = .994). Affective engagement: general affective engagement scores were higher in the ChatGPT condition (M = 4.37, SD = 0.51) than in the non-ChatGPT condition (M = 3.82, SD = 0.92), with paired-samples t-test t = 2.15, p = .036 and Cohen’s d = 8.90 as reported (interpreted as a large effect). Correlation: in the ChatGPT condition, affective engagement correlated moderately and positively with overall writing performance (Spearman rs = .301, p = .021); in the non-ChatGPT condition, the correlation was non-significant (rs = .160, p = .238).",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in Sociocultural Theory (Vygotsky, 1978) and its constructs of activity, mediating tools, and the Zone of Proximal Development, framing ChatGPT as a mediating tool that scaffolds learners’ pre-writing within their ZPD; informed by research on collaborative writing and languaging (Swain) in L2 development. The importance of the pre-writing stage is situated in the Writing Process Model (Hayes & Flower, 1980), and affective engagement is conceptualized following Fredricks et al.’s multidimensional engagement framework and Pekrun’s control-value theory of achievement emotions. Discussion also references the Trade-Off Hypothesis (Skehan, 1998) and the Noticing Hypothesis (Schmidt, 2001) to interpret learners’ focus on meaning over form and the relationship between noticing and gains in content and vocabulary.",
		"data_collection_instrument (数据收集工具)": "Two timed argumentative paragraph writing tasks (Task 1 on promoting public transport and cycling, Task 2 on restricting individual car and motorbike ownership, both on the topic of pollution; minimum 120 words, 25 minutes per task), scored with a writing rubric adopted from Nguyen et al. (2024) assessing Content, Vocabulary, Grammar, and Organization (0–5 points per criterion, total 0–20). An 8-item affective engagement questionnaire (items 1–3 measuring emotional responses adapted from Fan & Xu, 2020; items 4–8 measuring perceived value adapted from Kang, 2023) rated on a 5-point Likert scale (1 = totally disagree to 5 = totally agree), administered after each writing task. ChatGPT logs (saved conversations capturing all prompts and AI responses) and note-taking sheets (blank papers used to jot notes during pre-writing) for content analysis of utilization patterns. Semi-structured individual interviews with 10 randomly selected participants (5 from each class) using three main questions about feelings when using ChatGPT, perceived helpfulness for paragraph writing (including grammar, vocabulary, ideas, organization), and preference for writing with or without ChatGPT. Oxford Quick Placement Test (OQPT) with 60 questions used prior to the intervention to assess English proficiency.",
		"data_collection_validity_reliability (工具信度与效度)": "Affective engagement questionnaire: Cronbach’s alpha was .879 for the ChatGPT condition and .932 for the non-ChatGPT condition, with all item–total correlations > .30, indicating good internal consistency. Content validity of the questionnaire and interview protocol was established through review by two experienced researchers with PhDs in Applied Linguistics. Writing rubric was adopted from prior work (Nguyen et al., 2024); while specific reliability coefficients for rubric scoring are not reported, inter-rater coding agreement for a subset of written texts reached 88%. For qualitative data, inter-rater agreement between the first and second researchers on independently coded subsets of data was 92% for ChatGPT logs, 90% for note-taking sheets, 88% for written texts, and 92% for interview transcripts, indicating strong consensus. OQPT is a standardized placement test; additional reliability indices for OQPT within this sample are not reported.",
		"data_analysis_method (数据分析方法)": "For utilization patterns (RQ1), ChatGPT logs and note-taking sheets were analyzed inductively using a theme-based approach: reading, highlighting segments related to learners’ behavioral focus, coding, grouping similar codes into categories, iteratively refining categories, and counting frequencies and numbers of participants per category. For affective engagement (RQ2), questionnaire data were entered into SPSS 27.0, reliability checked via Cronbach’s alpha, descriptive statistics (means and standard deviations) were reported, and paired-samples t-tests on summed scores compared ChatGPT and non-ChatGPT conditions, with Cohen’s d effect sizes. For writing performance (RQ3), rubric scores for four criteria and total scores were computed; non-normality was confirmed via Shapiro–Wilk tests (all p < .001), so medians and interquartile ranges were reported. A Linear Mixed-Effects Model (LMM) in R (lmerTest package) with task condition as fixed effect and participant as random effect (Score ~ TaskCondition + (1|Participant)) tested the impact of ChatGPT use, with conditional R² reported via the MuMIn package; multicollinearity was checked using Variance Inflation Factor values (VIF < 2.0). Wilcoxon signed-rank tests with Bonferroni-adjusted alpha (.05/4) compared criterion scores between conditions, with effect sizes r calculated as Z/√N. For RQ4, Spearman correlation tests examined relationships between total writing scores and total affective engagement scores in each condition, with rs interpreted according to conventional thresholds. Semi-structured interview transcripts were analyzed using a theme-based approach with coding, category formation, frequency counts, and illustrative translated excerpts; member checking was conducted by returning transcripts to participants for accuracy confirmation.",
		"unit_of_analysis (分析单位)": "Individual student texts, individual questionnaire responses, individual ChatGPT logs and note-taking sheets, and individual interview transcripts.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Participants belonged to two intact classes (Class A and Class B) instructed by the same teacher. Task condition order was counterbalanced at the class level: in week two, Class A completed the ChatGPT condition and Class B the non-ChatGPT condition; in week three, the order was reversed so all students experienced both conditions. Assignment to classes was not randomized but based on existing class membership.",
		"power_analysis (功效分析与样本量论证)": "An a priori power analysis using GPower 3.1 (F-test family, within-groups ANOVA, two conditions and two writing tasks, α = .05, power = .95, medium effect size f = .25) indicated a required sample size of 54 learners; the actual sample size of 56 participants met and slightly exceeded this requirement.",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame comprised first-year Vietnamese EFL students enrolled in an English Preparation course at a private university, with English proficiency between B1 and B2 and approximately ten years of prior English study, and no overseas training experience. While the sample is adequate for the planned analyses, it reflects a single institutional context and may not be representative of all EFL learners or other educational settings.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Written texts were graded using the four-criterion rubric (Content, Vocabulary, Grammar, Organization) by two researchers (the first and second authors). The third researcher randomly selected students’ texts from the data pool for coding, and the coders were blinded to the task conditions (ChatGPT vs non-ChatGPT) to enhance objectivity. Before full-scale scoring, the researchers collaborated to discuss the coding plan and jointly analyze a small set of ChatGPT logs, note-taking sheets, written texts, and one interview to align coding practices, achieving inter-rater agreement rates of 92% for ChatGPT logs, 90% for note-taking sheets, 88% for written texts, and 92% for interviews; remaining data were coded by the first researcher. Specific numerical inter-rater reliability coefficients for rubric scores (e.g., Cohen’s kappa or correlation) are not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Four-week study: week 1 for information, consent, and OQPT; week 2 for the first writing task with pre-writing (15 minutes) and writing (25 minutes) plus affective engagement questionnaire (Class A in ChatGPT condition, Class B in non-ChatGPT condition); week 3 for the second writing task with reversed conditions and the same timing and questionnaire; week 4 for semi-structured interviews with 10 randomly selected students.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT 4.0 (OpenAI GPT-series large language model)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT 4.0 was used to provide students with access to the latest features of the tool; learners interacted with ChatGPT via its standard interface on their laptops during a 15-minute pre-writing period. The article does not report specific API usage, temperature or other parameter settings, or any custom system prompts.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students directly interacted with ChatGPT 4.0 during the pre-writing stage of one writing task, using their own prompts to request ideas, vocabulary, translations, summaries, organization suggestions, and other support; ChatGPT was not embedded in an LMS and teachers did not mediate the actual prompting during the tasks. During the drafting stage, ChatGPT was not accessible and writing was completed on paper.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "No explicit prompt engineering training or standardized prompt templates were provided; students, already familiar with ChatGPT, were allowed to use their own prompts independently. Content analysis of logs showed typical prompt types: asking ChatGPT to write the whole paragraph for the given topic, requesting content ideas and arguments, asking for translations into Vietnamese, asking for meanings of unfamiliar words, requesting shorter text (e.g., about 150 words), requesting appropriate organization (paragraph rather than essay), asking for simple grammatical structures, and asking other task-related questions (e.g., word counts). Participants often requested ChatGPT to write the composition more than once and repeatedly shorten or translate outputs.",
		"training_support_llm_literacy (LLM素养与提示培训)": "All participants had prior experience using ChatGPT and therefore no additional formal training session on ChatGPT usage was provided before the tasks. They were informed of the research purpose and procedures and then allowed to interact with ChatGPT independently during the pre-writing phase in the ChatGPT condition. While the article emphasizes that students should receive proper training in effective ChatGPT use and that the tool can produce inaccurate information, such explicit training was recommended as a pedagogical implication rather than implemented within the current study.",
		"intervention_implementation (干预实施流程_步骤与任务)": "After consent and proficiency testing in week 1, students completed two timed argumentative writing tasks under two different pre-writing conditions. In the ChatGPT condition, learners had 15 minutes to interact with ChatGPT 4.0 online to prepare for the writing task: they could solicit whole paragraphs, ideas, vocabulary, translations, explanations, shorter versions, and organizational guidance, while taking notes as desired. They then wrote a 120+ word argumentative paragraph on paper within 25 minutes without further access to ChatGPT or other materials and subsequently completed the affective engagement questionnaire. In the non-ChatGPT condition, students also had 15 minutes for pre-writing, but instead of ChatGPT they used blank paper to brainstorm and note ideas, vocabulary, and organizational plans based solely on their own knowledge; they then wrote the paragraph in 25 minutes and completed the same questionnaire. Conditions were counterbalanced across the two classes over two consecutive weeks so that each participant experienced both pre-writing modes. During all writing, no external materials or peer discussions were allowed to prevent contamination.",
		"experimental_group_intervention (实验组干预内容)": "In the ChatGPT pre-writing condition, all participants (when in that condition) used ChatGPT 4.0 for 15 minutes to prepare for their paragraph writing, engaging in various utilization patterns such as requesting whole paragraphs, content ideas, vocabulary, translations, shortened texts, and organization suggestions, then using this input to inform their subsequent handwritten argumentative paragraphs.",
		"control_group_intervention (对照组干预内容)": "In the non-ChatGPT pre-writing condition, participants prepared for the paragraph writing without any AI support by brainstorming on blank note-taking sheets for 15 minutes, focusing primarily on vocabulary and content, with limited attention to organization and grammar, and then composed their paragraphs by hand within 25 minutes.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT was integrated exclusively into the pre-writing (planning) stage, where learners gathered ideas, vocabulary, and organizational input; drafting of paragraphs and any revising during the 25-minute writing period were completed without ChatGPT or other external aids.",
		"writing_genre (写作体裁)": "Timed argumentative paragraphs related to urban air pollution and transport policies.",
		"writing_task_type (写作任务类型)": "Exam-like timed argumentative writing tasks requiring at least 120 words in 25 minutes on IELTS-style prompts about promoting public transport and cycling (Task 1) and restricting individual car and motorbike ownership (Task 2).",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT 4.0 functioned as a generative AI assistant and mediational tool in the pre-writing phase: generating whole sample paragraphs, providing ideas and arguments, supplying topic-related vocabulary, translating outputs into learners’ L1 (Vietnamese), explaining word meanings, shortening texts, and giving suggestions about paragraph organization and sentence complexity. ChatGPT was not used for automated scoring, rubric-based feedback, or direct grading of student texts.",
		"role_instructor (教师角色与介入方式)": "The same teacher instructed both classes in the English Preparation course, administered the tasks, controlled the conditions (ensuring that ChatGPT was used only in the designated pre-writing phase and that no materials or discussion were allowed during writing), distributed and collected questionnaires and note-taking sheets, and collected ChatGPT logs by having students save and email their ChatGPT pages. The research team (three authors) designed the study, data collection instruments, and interview protocol; researchers conducted and audio-recorded the interviews in Vietnamese and later managed transcription, translation, and analysis.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Private Vietnamese university, English Preparation course for first-year EFL students; classroom-based, face-to-face setting. Pre-writing with ChatGPT was conducted using students’ laptops with internet access, while actual paragraph writing was done on paper under timed, test-like conditions with no external resources.",
		"ethical_consideration (伦理审查与知情同意)": "The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of FPT University, Vietnam. Participants were informed about the study’s aims, procedures, potential risks, and data confidentiality, and all provided written informed consent to participate and consented to publication of anonymized data and quotes. Interviews were audio-recorded with explicit permission, and pseudonyms were used in reporting. Data and materials were stated to be available upon request.",
		"llm_access_policy (LLM使用规范_允许与限制)": "ChatGPT 4.0 use was allowed only during the 15-minute pre-writing phase in the designated ChatGPT condition; students had to rely solely on their own brainstorming in the non-ChatGPT condition and were not allowed to use ChatGPT or other materials during the 25-minute writing period in either condition. Tasks were completed individually with no peer discussion to avoid external influence. Although the article recommends providing training on responsible ChatGPT use due to potential inaccuracies and plagiarism risks, no detailed institutional AI usage policy beyond these task-specific rules is reported.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "NR",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Learners employed a broader and more diverse range of strategies during the pre-writing stage in the ChatGPT condition (eight categories of prompts, with high frequencies of requests for whole texts, ideas, translations, vocabulary explanations, shortening, and organization) compared to the non-ChatGPT condition, where their behaviors were limited mainly to noting vocabulary and content with minimal focus on organization and grammar. Affective engagement was positive in both conditions but significantly higher when ChatGPT was available in pre-writing, with higher mean scores for willingness, interest, excitement, and perceived usefulness for improving overall writing, vocabulary, grammar, organization, and ideas; all ten interviewed students preferred the ChatGPT-supported tasks, citing rich input (ideas and vocabulary) and convenience (time-saving, quick and detailed responses) as primary reasons. Writing performance improved with ChatGPT-supported pre-writing: overall writing scores, as well as content and vocabulary scores, were significantly higher in the ChatGPT condition than in the non-ChatGPT condition, with large effect sizes for content and vocabulary and no significant differences in grammar or organization. A moderate positive correlation was observed between affective engagement with ChatGPT and overall writing performance in the ChatGPT condition, but not in the non-ChatGPT condition, suggesting that learners who felt more positive and valued the ChatGPT-supported activity tended to produce higher-quality writing. The authors conclude that integrating ChatGPT into the pre-writing stage can enhance both emotional positivity and text quality, particularly for content and vocabulary, though the observed benefits are short-term and require careful, trained integration to avoid issues such as plagiarism or overreliance."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of ChatGPT integration in pre-writing was evaluated through multiple measures: (1) utilization patterns and strategy diversity derived from content analysis of ChatGPT logs and note-taking sheets; (2) affective engagement with and without ChatGPT measured by an 8-item questionnaire on emotional responses and perceived value, administered after each writing task; (3) writing performance scores on two timed argumentative paragraphs assessed using a four-criterion rubric (Content, Vocabulary, Grammar, Organization); and (4) Spearman correlations between affective engagement scores and overall writing scores, along with semi-structured interview data on learners’ experiences and preferences.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Analytic writing rubric adapted from Nguyen et al. (2024) assessing Content, Vocabulary, Grammar, and Organization, each scored from 0 to 5 with detailed descriptors, yielding a total score ranging from 0 to 20 for each paragraph.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Focus on text quality in terms of the richness and relevance of content, appropriateness and diversity of vocabulary, grammatical accuracy, and logical organization of argumentative paragraphs.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "An 8-item affective engagement questionnaire administered after each writing task, with items 1–3 measuring emotional responses (willingness, interest, excitement) adapted from Fan & Xu (2020) and items 4–8 measuring perceived value of the activity for improving overall writing, vocabulary, grammar, organization, and ideas based on Kang (2023); responses on a 5-point Likert scale from 1 (totally disagree) to 5 (totally agree).",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Learners’ emotional responses and perceived value related to the writing tasks with and without ChatGPT, including willingness to participate, interest, excitement, and perceived helpfulness of the activity for improving overall writing ability, vocabulary, grammar, organization, and idea generation.",
		"cognitive_aspect_measure (认知因素测量工具)": "Cognitive aspects were primarily inferred from the writing performance rubric scores on content, vocabulary, grammar, and organization, and from qualitative analysis of ChatGPT logs and note-taking sheets documenting how learners planned, generated, and organized ideas during pre-writing; no separate standardized cognitive or metacognitive strategy questionnaires were used.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Learners’ planning and idea-generation strategies in the pre-writing stage (e.g., focusing on meaning vs form, requesting arguments and examples, seeking vocabulary and translations, attention to organization), and the resulting changes in text content and lexical richness; the discussion links these behaviors to focusing on meaning, noticing task-relevant features, and potential trade-offs between content/vocabulary and grammar/organization.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Behavioral engagement and utilization patterns were measured via ChatGPT interaction logs (saved chat pages) and note-taking sheets, which were coded and categorized into prompt and focus types, along with frequency counts and number of participants per category.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Patterns of ChatGPT use such as frequency and type of prompts (writing whole texts, requesting ideas, translations, vocabulary explanations, shortening, organization, grammar, and other queries), diversity of pre-writing strategies employed, and in the non-ChatGPT condition, extent of focus on vocabulary, content, organization, and grammar on note-taking sheets; results showed more diverse behaviors with ChatGPT and a predominant focus on vocabulary and content in both conditions, with very limited attention to grammar.",
		"other_outcomes_measure (其他结果测量工具)": "Semi-structured interviews with 10 participants, audio-recorded, transcribed in Vietnamese, member-checked, and analyzed thematically; descriptive statistics for questionnaire reliability (Cronbach’s alpha) also provided information about instrument performance.",
		"other_outcomes_focus (其他结果维度说明)": "Qualitative findings indicated that learners overwhelmingly preferred using ChatGPT during pre-writing because it provided essential input such as ideas and vocabulary that they could not think of on their own and did so quickly and conveniently, saving time and effort. Interviewees reported reusing ideas and lexical items from ChatGPT in their paragraphs and noted that ChatGPT’s detailed answers reduced the need to consult multiple sources. Some learners also highlighted that ChatGPT responses could be lengthy or difficult to understand, necessitating repeated shortening or translation into Vietnamese.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Proficiency assessed with the OQPT in week 1; affective engagement questionnaires administered immediately after each of the two writing tasks (one in the ChatGPT condition and one in the non-ChatGPT condition) in weeks 2 and 3; writing performance assessed for both tasks; semi-structured interviews conducted in week 4 after both conditions had been experienced. No delayed or follow-up testing was conducted beyond the four-week period.",
		"primary_outcome_variables (主要结果变量_因变量)": "Primary outcome variables were overall writing performance scores (total rubric scores) and criterion scores (content, vocabulary, grammar, organization) for each paragraph in ChatGPT vs non-ChatGPT conditions; total affective engagement scores (summed questionnaire scores) for each condition; frequencies and categories of pre-writing strategies and prompt types; and Spearman correlation coefficients between affective engagement and writing performance in each condition.",
		"independent_variables_and_factors (自变量与实验因素)": "Main independent variable was task condition (ChatGPT-supported pre-writing vs non-ChatGPT pre-writing). In the Linear Mixed-Effects Model, task condition was treated as a fixed effect and participant as a random effect. No additional experimental factors (e.g., proficiency group, class) were formally modeled.",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Paired-samples t-test showed significantly higher general affective engagement scores in the ChatGPT condition (M = 4.37, SD = 0.51) than in the non-ChatGPT condition (M = 3.82, SD = 0.92), t = 2.15, p = .036, with Cohen’s d = 8.90 reported as a large effect. Linear Mixed-Effects Model indicated that learners achieved significantly higher overall writing scores in the ChatGPT condition than in the non-ChatGPT condition (non-ChatGPT vs ChatGPT: β = -0.89, SE = 0.24, t = -3.72, p < .001). Wilcoxon signed-rank tests showed significant differences favoring the ChatGPT condition for content (Z = -4.18, p < .001) and vocabulary (Z = -4.28, p < .001), but not for grammar (Z = -0.36, p = .720) or organization (Z = -0.01, p = .994). Spearman correlation in the ChatGPT condition revealed a moderate positive association between affective engagement and overall writing performance (rs = .301, p = .021), whereas the correlation in the non-ChatGPT condition was non-significant (rs = .160, p = .238).",
		"effect_size_summary (效应量摘要)": "Effect sizes reported include Conditional R² = .21 for the Linear Mixed-Effects Model examining the impact of task condition on writing scores (interpreted as a small effect under the study’s criteria), Wilcoxon signed-rank effect sizes of approximately r = .56 for content and r = .57 for vocabulary indicating large effects, and a Spearman correlation rs = .301 between affective engagement and writing performance in the ChatGPT condition indicating a moderate relationship. Cohen’s d = 8.90 for the difference in general affective engagement between conditions was reported and interpreted as indicating a large effect.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study acknowledges general concerns that ChatGPT can lead to dishonesty or plagiarism in L2 writing and that inappropriate use may result in superficial learning; the design of using ChatGPT only in the pre-writing stage (with writing completed on paper without AI) is presented as a way to address these concerns. Analysis of logs shows that many learners repeatedly requested ChatGPT to write full compositions and then asked for shortening and translation, raising the risk that some may rely heavily on AI-generated text, especially at lower proficiency levels. The authors note that ChatGPT’s lengthy, natural-language responses can be challenging for learners to understand, prompting heavy dependence on translation into L1, which may encourage overreliance on L1 and AI rather than independent text construction. They also emphasize that ChatGPT can produce inaccurate information, so students need training to avoid accepting output uncritically. No direct instances of plagiarism or major negative academic consequences are reported within the study, but the potential for misuse, superficial learning, and overreliance is highlighted in the discussion and pedagogical implications.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No formal subgroup or equity analyses (e.g., by gender, proficiency sub-level, or class) were conducted. The authors suggest that lower-proficiency learners may find ChatGPT’s natural-language outputs more difficult to understand and thus may rely more on translation, but this is a qualitative observation rather than a statistical subgroup comparison; no detailed subgroup statistics or equity-focused outcomes are reported.",
		"limitation (研究局限)": "The study examined ChatGPT use only in the pre-writing stage and did not investigate its effects on drafting, revising, or other stages of the writing process; thus, the broader impact on the full writing cycle remains unclear. It focused mainly on affective engagement and did not systematically measure other engagement dimensions such as cognitive, behavioral, or social engagement. The study primarily addressed short-term use of ChatGPT within two tasks over a brief period, so the long-term effects of ChatGPT on learners’ planning behaviors, engagement, and writing performance are unknown. The sample was limited to 56 first-year Vietnamese EFL students from a single private university and one English Preparation course, which constrains generalizability. Additionally, while within-subject counterbalancing helped control for order effects, the design did not include more complex modeling of potential moderators (e.g., proficiency variations), and the discussion notes the malleability of engagement over time, which was not captured in this short-term design.",
		"challenge (实施挑战与风险)": "Challenges and risks highlighted include learners’ tendency to ask ChatGPT to produce full compositions and then depend heavily on AI-generated content, which may encourage overreliance and raise plagiarism concerns if used beyond the pre-writing stage; difficulty for some learners, particularly at lower proficiency levels, in understanding ChatGPT’s lengthy and complex outputs, leading to frequent requests for translation into L1 and simplified versions; the potential for ChatGPT to generate inaccurate or misleading information; and the need for teachers to monitor students’ AI use through logs and observation and to provide guidance so that ChatGPT functions as a supportive tool rather than a crutch.",
		"future_work (未来研究方向)": "Future research is recommended to explore ChatGPT’s role across all stages of the writing process (planning, drafting, and revising) rather than only pre-writing; to investigate additional dimensions of learner engagement beyond affective (including cognitive, behavioral, and social engagement); and to conduct longitudinal studies to examine how learners’ engagement with ChatGPT and its influence on writing performance evolve over time, given the malleability of engagement. Broader samples and contexts (e.g., multiple institutions or countries) and more nuanced analyses of learner characteristics (e.g., proficiency level, engagement profiles) are also suggested to provide a more comprehensive understanding of ChatGPT use in L2 writing.",
		"implication (理论与教学实践启示)": "The study suggests that integrating ChatGPT into the pre-writing stage of EFL writing, even without intensive training, can broaden learners’ planning strategies, enhance affective engagement, and improve text quality, particularly in content and vocabulary. From a theoretical standpoint, it supports the view that LLMs can act as mediational tools within Sociocultural Theory, functioning as more competent partners that scaffold learners within their ZPD and promote noticing and languaging. Pedagogically, institutional leaders and teachers are encouraged to embrace ChatGPT as a supportive tool in L2 writing classrooms rather than banning it, extending its use beyond post-writing feedback to the planning phase, while simultaneously providing proper training on how to formulate effective prompts, evaluate AI-generated information critically, and avoid plagiarism. Teachers are advised to monitor students’ interactions with ChatGPT (e.g., via logs) to understand their emotional and strategic engagement and to offer timely feedback and adjustments, ensuring that ChatGPT supports rather than replaces learners’ own idea generation and writing efforts."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "The authors reported that they received no financial support for the research, authorship, and/or publication of the article.",
		"conflict_of_interest (利益冲突声明)": "The authors declared no potential conflicts of interest with respect to the research, authorship, and/or publication of the article.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Random assignment to classes or conditions was not used; participants were members of two existing classes and experienced both conditions in a counterbalanced order. The within-subjects design reduces between-group variability, but the lack of randomization to different treatment groups means selection bias at the class level cannot be fully ruled out.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Participants and instructors were aware of when ChatGPT was or was not available in pre-writing, so there was no blinding at the participant or teacher level. However, for writing performance grading, the third researcher randomly selected texts for scoring and the two raters (first and second researchers) were blinded to the task conditions when assigning rubric scores, which reduces detection bias in outcome assessment.",
		"attrition_and_missing_data (流失与缺失数据处理)": "Attrition and missing data handling procedures are not explicitly described; the analyses report N = 56 for both ChatGPT and non-ChatGPT conditions and for questionnaires and writing scores, suggesting that complete data were available for all participants, but this is not directly stated.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of participants, proficiency assessment, instructional context, task prompts, timing, conditions, counterbalancing scheme, instruments (including item origins and sample items), data collection procedures, and statistical methods, including parameters for the power analysis, model specifications, and effect size interpretations. Qualitative coding procedures, inter-rater agreement rates, and ethics approvals are also reported. However, full questionnaires and rubrics are summarized rather than reproduced in full, and raw data are not included, although the authors state that data and materials are available on request.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that ChatGPT 4.0 was used to provide students with access to the latest features, which enhances reproducibility relative to unspecified versions; however, no additional technical details such as exact deployment dates, default parameter settings, or prompt templates are provided, and given the evolving nature of ChatGPT, exact replication may still be constrained.",
		"baseline_equivalence (基线等同性检验)": "Baseline English proficiency was measured with the OQPT, confirming that learners’ levels ranged from B1 to B2, but no statistical tests comparing baseline characteristics between the two classes (Class A vs Class B) or other subgroups are reported. The within-subjects design and counterbalancing of task conditions mitigate but do not eliminate concerns about baseline equivalence across classes.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "For writing scores, Shapiro–Wilk tests indicated non-normal distributions (all p < .001), leading the authors to use non-parametric tests (Wilcoxon signed-rank) and Linear Mixed-Effects Models, which are more robust to non-normality. For the LMM, Variance Inflation Factor values were checked and found to be less than 2.0, suggesting no serious multicollinearity among predictors. No additional diagnostics (e.g., residual plots) are reported.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Questionnaire responses were entered into SPSS, reliability checked via Cronbach’s alpha, and summed scores computed for affective engagement; no transformations are reported. Writing scores were based on rubric ratings and analyzed directly as ordinal/interval data using non-parametric statistics and LMM. Qualitative data (ChatGPT logs, notes, interview transcripts) were transcribed, translated where necessary, and coded using a theme-based approach with iterative refinement and member checking; no additional data transformations are described."
	}
}