{
	"Basic Identification": {
		"author (作者)": "Juan Escalante; Austin Pack; Alex Barrett",
		"publication_year (发表年份)": "2023",
		"study_region (研究地区)": "Asia–Pacific region (small liberal arts university)",
		"journal_name (期刊名称)": "International Journal of Educational Technology in Higher Education",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article; two-study longitudinal quasi-experimental and survey design",
		"doi_or_identifier (DOI或唯一标识)": "10.1186/s41239-023-00425-2",
		"research_aims (研究目的与问题)": "To investigate (1) whether AI-generated feedback on academic writing using ChatGPT (GPT-4) leads to superior linguistic progress for university English as a new language (ENL) students compared with feedback from a human tutor, and (2) whether ENL students prefer AI-generated feedback over human tutor feedback, and why.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Prior work has examined traditional AWE tools and the technical feasibility or scoring reliability of ChatGPT-like systems, but there is limited empirical evidence on learning outcomes when GPT-based feedback replaces human tutor feedback, and little on ENL learners’ preferences and perceptions when AI feedback is used as an automatic essay evaluator; this paper addresses both efficacy and learner preference for GPT-4-generated feedback."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "University ENL students enrolled in an academic reading and writing language course",
		"language_proficiency (语言熟练度水平)": "At least CEFR B1 English proficiency based on the institution’s English Language Admission Test",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "Study 1: 21 males, 27 females (N=48); Study 2: 13 males, 30 females (N=43)",
		"age (年龄)": "Study 1: 20–30 years; Study 2: 19–36 years",
		"learning_context (学习语境_ESL_EFL_ELL等)": "English as a new language (ENL) in higher education, academic reading and writing course",
		"target_language (目标语言)": "English (ENL/L2)",
		"discipline (学科背景)": "University students from various majors; all taking an academic ENL reading and writing course; specific disciplines NR",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Study 1: six-week longitudinal mixed repeated measures quasi-experimental pretest–posttest design with two feedback conditions (AI-generated versus human tutor feedback); Study 2: six-week repeated survey design on feedback preference with the same weekly writing tasks but both AI and human feedback provided.",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods: quantitative analyses (repeated-measures ANOVA, independent-samples t-tests, descriptive statistics) plus qualitative thematic analysis of open-ended survey responses.",
		"sampling_method (抽样方法)": "Non-probability self-selection sampling of ENL students enrolled in an academic reading and writing course who voluntarily consented to participate; participants then allocated to Study 1 (N=48) or Study 2 (N=43); group assignment in Study 1 described as quasi-experimental with no randomization reported.",
		"sample_size_and_effect (样本量及效应量)": "Total participants across both studies N=91 ENL students. Study 1: N=48 (EG AI-feedback n=23, CG human-tutor feedback n=25). Pretest means: EG M=27.522, SD=1.301; CG M=26.820, SD=1.391; t(46)=1.801, p=0.078, Cohen’s d≈0.52 (no significant baseline difference). Posttest means: EG M=33.370, SD=1.908; CG M=33.680, SD=2.066; t(46)=-0.539, p=0.591, d≈-0.16 (no significant group difference). RM-ANOVA showed a strong main effect of time on writing scores, F=487.661, p<0.001, partial η²=0.914, indicating substantial improvement for both groups; no significant interaction between time and group, F=3.094, p=0.085, partial η²=0.063; no significant between-groups effect, F=0.241, p=0.626, partial η²=0.005. Study 2: N=43; descriptive statistics for weekly preference items (n per week 32–41) showed means generally above 4.0 for satisfaction, clarity, helpfulness, and preference for both AI and human feedback, with no statistically significant differences between subgroups preferring AI vs human feedback in week 6 (all t-tests non-significant).",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in second language writing and automated writing evaluation literature, the emerging role of generative AI and large language models in education, technology acceptance theory (perceived usefulness and ease of use), and principles of second language acquisition such as comprehensible input and the role of a more knowledgeable other; framed by calls for thoughtful, goal-aligned integration of AI-enabled tools in ENL writing rather than prohibition.",
		"data_collection_instrument (数据收集工具)": "Study 1: diagnostic writing test (pretest) and final writing exam (posttest), each requiring a 300-word paragraph on an academic topic related to course readings, integrating sources; recurring weekly 300-word writing assignments on class-related academic topics with source integration; analytic writing rubric assessing content, coherence, language use, and sources and evidence, each on a 4-point scale with performance descriptors, total maximum score reported as 40; GPT-4-generated written feedback for the experimental group; 30-minute one-on-one human tutoring sessions for control group. Study 2: weekly online preference questionnaire via Qualtrics with eight 5-point Likert items (satisfaction, clarity, helpfulness, and preference comparing human versus AI feedback) plus a forced-choice item on preferred feedback type for next week and an open-ended question requesting explanations for their choice.",
		"data_collection_validity_reliability (工具信度与效度)": "Pretest and posttest essays in Study 1 were independently rated by two experienced academic English instructors using the analytic rubric; intraclass correlation coefficients indicated excellent inter-rater reliability for pretests (CG ICC=0.932, p<0.001; EG ICC=0.919, p<0.001) and good reliability for posttests (CG ICC=0.877, p<0.001; EG ICC=0.816, p<0.001). For RM-ANOVA, Shapiro–Wilk tests suggested normality for all conditions except the control-group posttest; Levene’s tests indicated homogeneity of variance for both pretest (p=0.888) and posttest (p=0.938); Greenhouse–Geisser correction was applied. In Study 2, three researchers independently conducted thematic analysis of open-ended responses and then reconciled salient themes; no formal reliability coefficients were reported for the qualitative coding.",
		"data_analysis_method (数据分析方法)": "Study 1: descriptive statistics for pretest and posttest scores; Shapiro–Wilk and Levene’s tests for normality and homogeneity; mixed 2×2 repeated-measures ANOVA with time (pre vs post) as within-subject factor and group (AI feedback vs human tutor feedback) as between-subject factor; Greenhouse–Geisser corrections applied where needed; independent-samples t-tests to compare groups at pretest and posttest; effect sizes reported as partial eta-squared in RM-ANOVA and Cohen’s d for t-tests. Study 2: descriptive statistics of weekly Likert-scale items across six weeks; independent-samples t-tests comparing means for students who ultimately preferred human versus AI feedback in week 6; inductive thematic analysis of open-ended responses to identify reasons for preferences and perceived advantages of each feedback type.",
		"unit_of_analysis (分析单位)": "Study 1: individual student writing test scores (pretest and posttest) and weekly writing assignments; Study 2: individual weekly questionnaire responses and associated qualitative comments.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Quasi-experimental non-randomized grouping into experimental group (AI-generated feedback via GPT-4) and control group (human tutor feedback); assignment procedure not described as random.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame was ENL students enrolled in an academic reading and writing course at a small liberal arts university in the Asia–Pacific region during a shortened Spring 2023 semester; participants had at least B1 proficiency; convenience self-selected sample from a single institution limits representativeness.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Pretest and posttest essays were scored independently by two experienced academic English language instructors using a shared analytic rubric; inter-rater reliability was quantified via ICC and disagreements resolved implicitly via averaging (explicit reconciliation not detailed). Human tutors in the control group were trained English language tutors from the university’s ENL Tutor Program and were certified by the College Reading & Learning Association, providing weekly 30-minute one-on-one feedback sessions. GPT-4 feedback in the experimental group was generated by a teaching assistant using a standardized prompt developed by two experienced language educators."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Study 1: approximately eight weeks in a shortened semester, with a pretest in the first week, six consecutive weeks of weekly 300-word writing assignments with feedback and revision, and a posttest in the final week.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (GPT-4, OpenAI)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "OpenAI’s GPT-4 accessed via the ChatGPT interface; a standardized multi-part prompt was iteratively developed by two experienced language educators following a prompt-engineering framework, specifying ChatGPT’s role as a professional language teacher, including the weekly writing prompt, and instructing it to provide simple-language feedback on specific aspects of student writing and to output a structured grammar feedback table; a teaching assistant used this prompt with GPT-4 to generate individual feedback for each student and emailed the feedback within two working days; no API or parameter settings beyond default are reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Teacher-assistant-mediated integration: students did not directly interact with ChatGPT; instead, their de-identified drafts were submitted by a teaching assistant to GPT-4 using a standardized prompt, and the resulting AI-generated feedback was sent back to students via email as their primary written feedback for revision.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A carefully engineered single prompt assigned GPT-4 the role of a professional language teacher expert in feedback on ENL writing, supplied the weekly writing assignment prompt, and asked for feedback in six areas: (1) quality and prompt-alignment of the topic sentence with improvement suggestions, (2) development of ideas and supporting details, (3) language that lowers academic quality with more academic alternatives, (4) appropriateness and improvement of transitional phrases, (5) use of sources and evidence, and (6) grammatical accuracy; for grammar it required a four-column table listing the sentence with the error, error type, description of the error type, and suggestions to address it, all in simple, learner-accessible language; this standardized prompt was used repeatedly without providing scoring samples (zero-shot style).",
		"training_support_llm_literacy (LLM素养与提示培训)": "Students were not trained to prompt or interact with ChatGPT themselves; AI use was restricted to receiving standardized GPT-4-generated feedback prepared by the teaching assistant, so there was no explicit LLM or prompt literacy training for learners within this intervention.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Study 1: In week 1, all participants completed a diagnostic writing pretest (300-word paragraph on an academic topic integrating sources). Over the following six weeks, all students completed one 300-word academic paragraph per week on class-related topics with source integration. The control group met weekly with a human ENL tutor for a 30-minute one-on-one session to receive and discuss feedback, revise, and submit a final draft. The experimental group submitted drafts electronically; identifying information was removed, and a teaching assistant used GPT-4 with the standardized prompt to generate written feedback, including comments on topic sentence, development, academic language, transitions, sources and evidence, and a detailed grammar error table; feedback was emailed to students within two working days, after which they revised and submitted a final draft. In the final week, both groups took a posttest final writing exam of similar format (300-word academic paragraph with sources), scored with the analytic rubric.",
		"experimental_group_intervention (实验组干预内容)": "Experimental group (EG, n=23) received AI-generated feedback via ChatGPT (GPT-4): their weekly 300-word academic paragraphs were de-identified and submitted to GPT-4 with a standardized feedback prompt, and the resulting AI feedback (analytic comments and grammar table) was emailed to them for use in revising and resubmitting their assignments; they did not receive human tutor feedback on these weekly tasks during the intervention.",
		"control_group_intervention (对照组干预内容)": "Control group (CG, n=25) received traditional human tutor feedback: each student had a weekly 30-minute one-on-one session with a trained ENL tutor who provided feedback and guidance on their 300-word paragraphs, which they then revised and resubmitted; no AI-generated feedback was used for this group.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT was used at the feedback and revision stage: after students produced initial drafts of their weekly academic paragraphs, GPT-4 analyzed the drafts and generated feedback that students used for revising and producing final versions.",
		"writing_genre (写作体裁)": "Academic paragraph writing (approximately 300 words) integrating sources from course readings on diverse academic topics.",
		"writing_task_type (写作任务类型)": "Process-oriented academic L2 writing tasks: diagnostic and final exam paragraphs plus six weekly course-based writing assignments requiring summarizing and integrating reading sources into a coherent academic paragraph.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "GPT-4 functioned as an automated writing evaluator and feedback generator, providing detailed formative feedback on topic sentence quality, idea development, academic style and vocabulary, transitional cohesion, use of sources and evidence, and grammatical accuracy in a structured error table; it did not assign rubric scores in this study.",
		"role_instructor (教师角色与介入方式)": "Course instructors designed the academic reading and writing curriculum and writing tasks, and two experienced language educators developed the GPT-4 feedback prompt; trained ENL tutors provided human feedback to the control group via weekly sessions; a teaching assistant mediated GPT-4 use for the experimental and Study 2 groups by running the standardized prompt and emailing AI feedback to students; experienced instructors independently scored pretest and posttest essays using the analytic rubric.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Small liberal arts university in the Asia–Pacific region; academic English as a new language (ENL) reading and writing course during a shortened spring semester; modality of instruction (face-to-face, hybrid, or online) not reported.",
		"ethical_consideration (伦理审查与知情同意)": "Standard ethical procedures were followed; participation was voluntary with informed consent, and students were explicitly told that participation would not yield additional academic credit; identifying information was removed from student writing before submitting texts to GPT-4.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students in the experimental and Study 2 groups did not directly log into ChatGPT for the study; instead, a teaching assistant controlled GPT-4 access, submitted de-identified drafts, and relayed feedback via email; AI use was restricted to formative feedback on course assignments, and human-generated rubric scoring was retained for pretest and posttest; the design intentionally limited direct AI access to reduce the risk of misuse such as having ChatGPT write assignments for students.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "The study acknowledged general risks of GenAI such as hallucinations, bias, and privacy concerns and emphasized that ChatGPT does not function as a fact-retrieval system; within the intervention, safety practices included stripping identifying information from student essays before submission to GPT-4 and limiting students to indirect receipt of AI feedback rather than direct unrestricted interaction; no additional technical content filters or monitoring routines were reported.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Study 1 found that both the AI-feedback group and the human-tutor group showed substantial and statistically significant improvement in writing scores from pretest to posttest, but there was no significant difference between groups and no significant interaction between time and feedback type, indicating that GPT-4-generated feedback did not produce superior or inferior writing gains compared with human tutor feedback. Study 2 showed that ENL students’ preferences for feedback type were roughly evenly split between AI-generated and human tutor feedback, with Likert ratings for satisfaction, clarity, helpfulness, and preference generally above 4.0 for both sources; qualitative data indicated that students valued human feedback for interactive, face-to-face engagement and opportunities to ask questions and practice speaking, while they valued AI feedback for its clarity, specificity, consistency, level of detail, and availability at any time. Many students saw a blended approach using both AI and human feedback as ideal."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness was evaluated by comparing pretest and posttest analytic writing scores (content, coherence, language use, and sources/evidence) in Study 1 and by tracking student satisfaction and preference ratings over six weeks in Study 2. Writing scores improved significantly over time for both AI and human feedback groups with a large main effect of time and no significant group or interaction effects, implying that incorporating GPT-4-generated feedback as an alternative to human tutor feedback did not negatively or positively alter learning outcomes. Student survey responses indicated comparably high perceived value for both AI and human feedback.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Analytic writing rubric for pretest and posttest academic paragraphs, with four criteria (content, coherence, language use, sources and evidence) each rated on a 4-point scale, with descriptors from initial to highly developed and a reported maximum total score of 40; weekly 300-word paragraphs on course-related topics integrating sources were used as practice but not directly analyzed in the main statistical comparisons.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Overall academic L2 writing proficiency in paragraph tasks, focusing on quality of content development, coherence and organization, accuracy and appropriateness of language use, and effective integration of sources and evidence in English academic writing.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Weekly online preference questionnaire (Study 2) with eight five-point Likert items comparing human tutor versus AI feedback on satisfaction, clarity, helpfulness, and preference, plus a forced-choice preference item and an open-ended question asking students to explain their choice.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Student satisfaction with feedback, perceived clarity and comprehensibility, perceived helpfulness for improving writing, and stated preference for receiving human tutor feedback versus AI-generated feedback, including attitudes toward engagement, comfort, and perceived usefulness.",
		"cognitive_aspect_measure (认知因素测量工具)": "NA",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NA",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "Open-ended qualitative responses explaining feedback preferences and perceived strengths and weaknesses of AI versus human feedback, analyzed thematically by three researchers.",
		"other_outcomes_focus (其他结果维度说明)": "Learner perceptions of AI and human feedback, including perceived time efficiency, opportunity for interaction, risk of losing personal voice in writing, perceived accuracy and specificity of AI error detection and vocabulary suggestions, perceived flexibility and availability of AI, and perceived complementarity of combining both feedback sources.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Study 1: pretest diagnostic writing on the first day of class; six weeks of weekly writing tasks with feedback and revision; posttest final writing exam in the final week of the shortened semester. Study 2: weekly preference surveys administered after completion of each weekly writing assignment and receipt of both AI and human tutor feedback for six consecutive weeks.",
		"primary_outcome_variables (主要结果变量_因变量)": "Study 1: total analytic writing rubric scores on pretest and posttest academic paragraphs; Study 2: Likert-scale ratings of satisfaction, clarity, helpfulness, and preference for human versus AI feedback, plus final forced-choice preference for feedback type.",
		"independent_variables_and_factors (自变量与实验因素)": "Study 1: within-subject factor time (pretest vs posttest); between-subject factor feedback type (experimental group with AI-generated feedback vs control group with human tutor feedback). Study 2: feedback source (human tutor vs AI program) as a within-subject comparison factor for preference and perception ratings over time.",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Study 1: RM-ANOVA showed a highly significant main effect of time on writing scores, F=487.661, p<0.001, indicating substantial improvement for both groups across six weeks. There was no significant group-by-time interaction, F=3.094, p=0.085, and no significant between-group effect, F=0.241, p=0.626, indicating that AI-generated feedback did not lead to significantly different gains compared to human tutor feedback. Independent-samples t-tests showed no significant differences between experimental and control groups at pretest (t(46)=1.801, p=0.078) or at posttest (t(46)=-0.539, p=0.591). Study 2: all t-tests comparing mean preference ratings between students who ultimately preferred human versus AI feedback in week 6 were non-significant, supporting the descriptive finding of a near-even split in overall preference.",
		"effect_size_summary (效应量摘要)": "Study 1: main effect of time had a very large partial eta-squared (η²≈0.914), indicating that about 91.4% of the variance in scores was associated with improvement over time; the group-by-time interaction had partial η²≈0.063 (small-to-moderate, non-significant), and the between-group effect had partial η²≈0.005 (very small, non-significant). Cohen’s d for pretest group difference was around 0.52 (moderate, non-significant), and for posttest around -0.16 (small, non-significant). Study 2: effect sizes were not separately reported, but similar mean ratings across weeks and groups suggest small or negligible differences in perceived value between AI and human feedback.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The article highlights several potential negative aspects of generative AI use in writing: ChatGPT and similar LLMs can produce untruthful or misleading text because they generate statistically plausible language rather than retrieving factual information; there are concerns about students misusing GenAI to have essays written for them or to correct entire texts wholesale without engaging in the writing process; students may accept AI-generated feedback or content uncritically without verifying accuracy; and broader concerns include bias in AI outputs and privacy issues related to user data. In this study, direct misuse was mitigated by restricting student access to ChatGPT and using it only as a teacher-mediated feedback generator, but the authors warn that learners will need to be taught to approach GenAI outputs critically.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "NR",
		"limitation (研究局限)": "Key limitations include the quasi-experimental design without random assignment and with only two groups from a single institution; a relatively short six-week intervention period for writing improvement; a modest sample size (N=48) for Study 1 and N=43 for Study 2; use of GPT-4 as a general-purpose LLM rather than a model optimized for AWE or writing annotation; lack of opportunity for students in Study 1 to ask follow-up questions to the AI or interact directly with ChatGPT; reliance on rubric total scores without detailed analysis of each analytic dimension; and lack of analysis by learner proficiency level, so it remains unclear how different proficiency groups might differentially benefit from AI feedback.",
		"challenge (实施挑战与风险)": "Implementation challenges identified or implied include how to integrate AI feedback ethically into ENL curricula without encouraging overreliance on AI-generated text; ensuring students understand the limitations and potential inaccuracies of AI feedback; balancing teacher workload relief from AI with the pedagogical value of human interaction; designing prompts and workflows that make AI feedback clear, accurate, and aligned with course objectives; and adapting existing writing assessment practices as GenAI becomes increasingly integrated into educational platforms.",
		"future_work (未来研究方向)": "Suggested future research includes examining how learners’ proficiency levels affect their ability to interpret and learn from AI-generated feedback; investigating the reliability and validity of GenAI systems for scoring and assessing writing rather than just providing feedback; exploring the efficacy of AI-generated feedback for native-English-speaking students; and studying classroom models that allow students to interact more directly with GenAI (for example, asking follow-up questions) while still safeguarding academic integrity and promoting active engagement in the writing process.",
		"implication (理论与教学实践启示)": "The findings suggest that GPT-4-generated feedback can be incorporated into ENL academic writing courses as an alternative to human tutor feedback without detriment to writing gains, and that many students value AI feedback for its clarity, specificity, and availability. At the same time, a substantial proportion of learners prefer human feedback for its interactive, affective, and speaking practice benefits, supporting a blended model where GenAI handles detailed, surface-level and structural feedback while human instructors focus on dialogic support, higher-order concerns, and speaking interaction. The study highlights the need for clear institutional guidelines and pedagogical frameworks on when and how to use GenAI as an automatic essay evaluator and emphasizes that language educators should develop competencies in working alongside AI while continuing to provide irreplaceable human pedagogical functions."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "The authors report that the research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.",
		"conflict_of_interest (利益冲突声明)": "The authors state that they have no competing interests.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "There was no random assignment; grouping into AI-feedback and human-tutor feedback conditions in Study 1 was quasi-experimental, creating potential for selection and confounding biases despite similar pretest scores.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Participants and staff were aware of whether AI or human tutors were providing feedback in Study 1; there was no blinding of students to condition. In Study 2, human tutors were unaware that students also received AI-generated feedback, but students knew they were receiving both kinds; rubric raters for pre- and posttests were described as independent but blinding to group and time was not explicitly reported.",
		"attrition_and_missing_data (流失与缺失数据处理)": "In Study 1, one student who completed only the posttest was excluded from analysis; otherwise, pretest and posttest data were available for the remaining 48 students. In Study 2, weekly survey response counts varied from 32 to 41 across six weeks, indicating some missing data from week to week, but no specific attrition analysis or imputation procedures were reported.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of participants, instruments, analytic rubric categories, intervention procedures, GPT-4 prompt structure (including an example prompt and AI-generated feedback), statistical methods, and key parameters such as ICCs, F-values, p-values, and effect sizes. Appendices include sample questionnaire items, the exact feedback prompt, and a sample of AI feedback. The authors state that datasets are available from the corresponding author on reasonable request; no open data repository or pre-registration is mentioned.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study explicitly identifies GPT-4 as the LLM used via ChatGPT but does not specify the exact model version or date of access; as GPT-4 continues to evolve, future replications may not obtain identical outputs even with the same prompt, limiting strict reproducibility.",
		"baseline_equivalence (基线等同性检验)": "Baseline writing proficiency in Study 1 was examined via pretest scores, with no significant difference between the experimental and control groups (t(46)=1.801, p=0.078), suggesting approximate baseline equivalence in the main outcome measure.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Shapiro–Wilk tests assessed normality for each group and time point, showing approximate normality except for the control-group posttest; Levene’s tests indicated homogeneity of error variances for both pretest and posttest; RM-ANOVA used Greenhouse–Geisser correction; missing data, outliers, and potential assumption violations were examined before analysis, though details of outlier diagnostics are not extensively discussed.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "One case where a participant completed only the posttest was excluded from the repeated-measures analysis; no further details on outlier detection, trimming, or sensitivity analyses were reported.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Student essays were de-identified before being submitted to GPT-4 for feedback to protect privacy; writing scores were analyzed in raw form without reported transformations; survey data were exported from Qualtrics for descriptive and inferential statistics; no additional data transformations (such as normalization or log transformation) were described."
	}
}