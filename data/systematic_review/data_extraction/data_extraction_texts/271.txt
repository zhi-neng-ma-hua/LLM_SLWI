{
	"Basic Identification": {
		"author (作者)": "Areen Alnemrat, Hesham Aldamen, Mohamad Almashour, Mutasim Al-Deaibes, Rami AlSharefeen",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Jordan (data collected at a large public university in Jordan)",
		"journal_name (期刊名称)": "Frontiers in Education",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; original quantitative research article using quasi-experimental pretest-posttest 2×2 factorial design",
		"doi_or_identifier (DOI或唯一标识)": "10.3389/feduc.2025.1614673",
		"research_aims (研究目的与问题)": "To investigate the comparative effectiveness of AI-generated feedback versus teacher-generated feedback on EFL undergraduate students’ argumentative writing performance at different proficiency levels (Intermediate-Low vs Advanced-Low) in Jordan; specifically, to examine (1) the extent to which feedback type (AI vs teacher) influences improvement in argumentative writing, (2) the extent to which language proficiency level affects writing performance after revision, and (3) whether there is an interaction between feedback type and proficiency level in determining post-revision writing outcomes.",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses the limited empirical research on the use of large language models (LLMs) such as ChatGPT for scaling feedback in EFL argumentative writing classes, especially in higher education settings in the Arab world, by directly comparing structured AI-generated feedback with teacher-generated feedback across proficiency levels using a controlled 2×2 factorial design and a rubric-aligned AI mentor prompt."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate students in a writing-focused EFL course at a large public university",
		"language_proficiency (语言熟练度水平)": "EFL learners classified at two ACTFL proficiency levels: Intermediate-Low and Advanced-Low, based on institutional Oral Proficiency Interviews aligned with ACTFL guidelines",
		"mother_tongue (母语)": "Arabic (all participants were native speakers of Arabic)",
		"sex (性别)": "Total 83 females and 37 males (distribution across subgroups not further specified)",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context (English as a Foreign Language) in higher education in Jordan",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Undergraduate English-as-a-Foreign-Language writing-focused course (English language and literature/second language writing)",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental, pretest-posttest, between-subjects 2×2 factorial design with Feedback Type (AI-generated vs teacher-generated) and Proficiency Level (Intermediate-Low vs Advanced-Low) as independent variables; within each subgroup, pre- and post-revision writing scores were compared.",
		"research_method (研究方法_定量_定性_混合)": "Quantitative research using experimental-comparative methods and inferential statistics (paired-samples t-tests, independent-samples t-test, two-way ANOVA, effect sizes) on rubric scores.",
		"sampling_method (抽样方法)": "Stratified sampling by proficiency level with assignment to feedback conditions based on intact classroom sections to maintain ecological validity; equal distribution across the four feedback × proficiency subgroups.",
		"sample_size_and_effect (样本量及效应量)": "The abstract initially mentions 60 students, but the Participants and Results sections report N=120 undergraduate EFL students (83 females, 37 males) distributed equally across four subgroups: AI/Intermediate-Low (n=30), AI/Advanced-Low (n=30), Teacher/Intermediate-Low (n=30), Teacher/Advanced-Low (n=30). All groups showed statistically significant pre–post improvements with large effect sizes: AI Advanced-Low t=8.32, p=0.0, Cohen’s d=1.518; AI Intermediate-Low t=14.10, p=0.0, d=2.575; Teacher Advanced-Low t=7.60, p=0.0, d=1.388; Teacher Intermediate-Low t=10.92, p=0.0, d=1.993. Between-group comparison of gain scores (AI vs teacher) yielded t(118)=0.55, p=0.586, Cohen’s d=0.10, indicating a very small effect size. Two-way ANOVA showed a significant main effect of proficiency level (F=74.36, p≈3.89e−14) but no significant main effect of feedback type and no significant interaction.",
		"theoretical_foundation (理论基础_理论框架)": "The study is grounded in second language writing and feedback theory, including Hattie and Timperley’s model of feedback (feed up, feed back, feed forward), the Feedback Engagement Model by Winstone et al., and work on feedback uptake and dialogic feedback (Carless and Boud); it also draws on literature on large language models (LLMs) and AI-supported scaffolding in writing, emphasizing prompt engineering and feedback engagement in EFL argumentative writing.",
		"data_collection_instrument (数据收集工具)": "A single timed argumentative writing task (250–300 words in 45 minutes) responding to the prompt “Should university education be free for all students?”; Draft 1 and Draft 2 essays; an analytic writing rubric with five equally weighted dimensions (Claim Clarity and Relevance, Support and Evidence, Counterarguments and Rebuttal, Organization and Coherence, Language Use) scored on a 5-point scale (1–5; total score 25); structured AI “mentor” prompt for ChatGPT (GPT-4) designed to provide rhetorical-level feedback; teacher feedback checklist aligned with the rubric dimensions.",
		"data_collection_validity_reliability (工具信度与效度)": "The analytic rubric was adapted from established frameworks used in prior EFL writing research and reviewed by two L2 writing experts for content validity; inter-rater reliability was assessed on a stratified random subsample of 36 essays (30% of the corpus): Pearson’s r for pre-test scores was 0.653 with mean absolute difference 2.11 and for post-test scores r=0.316 with mean absolute difference 2.25; ICC(2,1) for post-test scores was 0.61, indicating moderate agreement; rater discrepancies were discussed post hoc to calibrate interpretations and ensure consistent rubric application.",
		"data_analysis_method (数据分析方法)": "Descriptive statistics (means, standard deviations, ranges) by feedback group and proficiency level; paired-samples t-tests within each subgroup to test pre–post improvement; independent-samples t-test on gain scores (post − pre) to compare AI vs teacher feedback; 2×2 two-way ANOVA to examine main effects of Feedback Type and Proficiency Level and their interaction on post-test scores; effect sizes calculated as Cohen’s d for t-tests and partial eta squared (η²) for ANOVA (η² numerical values not reported); assumptions of normality and homogeneity of variance were checked using Shapiro–Wilk tests and Levene’s tests; analyses conducted using Python with libraries such as pandas, scipy, and statsmodels.",
		"unit_of_analysis (分析单位)": "Individual student argumentative essay scores (total rubric score per draft per student); gain scores (post-test minus pre-test) at the student level; group-level summary statistics by feedback condition and proficiency level.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Non-random assignment based on intact classroom sections, with stratification by proficiency level to ensure equal representation in each feedback condition (AI vs teacher).",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Undergraduate EFL students enrolled in a writing-focused course at a large public university in Jordan; all participants were native Arabic speakers with at least 4 years of formal English instruction at the university level and were classified as Intermediate-Low or Advanced-Low on ACTFL-based institutional placement assessments; the balanced 2×2 design (n=30 per cell) supports internal comparisons, but the authors do not claim broad representativeness beyond similar Jordanian higher-education EFL contexts.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "A primary rater scored all Draft 1 and Draft 2 essays using the validated analytic rubric; a second trained evaluator independently scored a stratified random sample of 36 essays (30% of the corpus), blind to group assignment, using the same rubric; inter-rater reliability metrics were computed and discrepancies discussed to calibrate rubric interpretation; specific details about rater training sessions and duration are not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Single argumentative writing task with one revision cycle: students produced Draft 1 during a 45-minute in-class session, received feedback (AI or teacher) and then had 1 week to revise and submit Draft 2; no additional longitudinal writing cycles were reported.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (GPT-4, large language model)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "Students in the AI group used ChatGPT (GPT-4) via an interface that allowed “chat history off” (privacy mode); interactions were driven by a structured, piloted AI mentor prompt designed to provide rhetorical-level, genre-specific feedback on argumentative writing based on an analytic rubric; the prompt was iteratively developed drawing on recent AI pedagogy literature; no further information on API vs web UI access, date of access, or parameter settings (e.g., temperature) was reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students in the AI feedback group directly interacted with ChatGPT using the standardized AI mentor prompt to obtain feedback on their Draft 1 essays and to guide revision of Draft 2; AI feedback was used as a stand-alone feedback source for this group, not embedded within an LMS or blended with teacher comments for that condition.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A structured, piloted AI mentor prompt was used to elicit genre-specific, rhetorical-level feedback from ChatGPT; the prompt instructed the AI to focus on argument structure, clarity, support and evidence, counterarguments and rebuttal, and organization and coherence, and explicitly to avoid rewriting the student’s text; it simulated an interactive, step-by-step mentoring session where the AI asked one question at a time to gather information about the student’s goals and concerns, highlighted strengths, then provided scaffolded constructive feedback, guided revision, and allowed for follow-up feedback; the prompt was aligned with the analytic rubric and intended to foster reflective and iterative revision.",
		"training_support_llm_literacy (LLM素养与提示培训)": "Students received an orientation session explaining the study, feedback mechanisms, and ethical protections; participants in the AI group were trained to input their essays into ChatGPT with chat history disabled, instructed to apply AI feedback independently, and reminded to critically evaluate and not accept AI-generated suggestions uncritically; teacher support was available for clarification; the article emphasizes the importance of training learners to critically engage with AI feedback but does not report a separate, extended LLM literacy curriculum.",
		"intervention_implementation (干预实施流程_步骤与任务)": "All students attended an initial orientation about study aims, feedback conditions, and ethical considerations. They then completed Draft 1 of an argumentative essay on the prompt “Should university education be free for all students?” (250–300 words, 45 minutes, standardized classroom conditions). Draft 1 essays were then processed according to feedback group: students in the AI group submitted their text to ChatGPT (GPT-4) using the standardized AI mentor prompt and received structured rhetorical feedback, while students in the teacher group received individualized handwritten formative feedback from the instructor guided by a rubric-aligned checklist. Feedback in both groups focused on argument clarity, support and evidence, counterarguments and rebuttal, organization and coherence, and language use. After receiving feedback, students had 1 week to revise their essays and submit Draft 2. All drafts were scored using the analytic rubric, with a subset double-rated for reliability, and scores were analyzed to compare pre–post gains across feedback and proficiency conditions.",
		"experimental_group_intervention (实验组干预内容)": "AI feedback group: Draft 1 essays were submitted to ChatGPT (GPT-4) using the structured AI mentor prompt; ChatGPT provided immediate, rubric-aligned, rhetorical-level feedback on claim clarity and relevance, support and evidence, counterarguments and rebuttal, organization and coherence, and language use without rewriting the text; students were instructed to critically evaluate and apply this feedback to revise their essays into Draft 2 over a 1-week period.",
		"control_group_intervention (对照组干预内容)": "Teacher feedback group: Draft 1 essays received individualized, handwritten formative feedback by the course instructor, guided by a feedback checklist aligned with the same analytic rubric dimensions (argument clarity, support and evidence, counterarguments and rebuttal, organization and coherence, language use); feedback was formative and non-evaluative and provided within 48 hours; students were encouraged to reflect on the comments and revise their essays into Draft 2 over a 1-week period.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Argumentative essay drafting (Draft 1), feedback reception (AI- or teacher-generated), revision and rewriting (Draft 2) based on the feedback; the focus was on feedback and revision stages rather than initial idea generation only.",
		"writing_genre (写作体裁)": "Argumentative essay in academic EFL context.",
		"writing_task_type (写作任务类型)": "Timed in-class argumentative writing task (250–300 words, 45 minutes) on a single prompt: “Should university education be free for all students?”, followed by a revised version after feedback.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT acted as an AI feedback provider and mentor, generating immediate, detailed, and personalized feedback on students’ argumentative writing, focusing on rhetorical and content-level features (structure, argument clarity, evidence, counterarguments, coherence) rather than simply correcting grammar or generating full replacement texts; it did not perform automated scoring for this study.",
		"role_instructor (教师角色与介入方式)": "The instructor provided rubric-aligned handwritten feedback to the teacher feedback group, explained the feedback process, encouraged students to engage with and reflect on feedback in both conditions, and was available to support students in interpreting AI feedback; the instructor also participated in rubric development and scoring, and maintained consistent teaching conditions across groups aside from the feedback modality.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Large public university in Jordan; undergraduate writing-focused EFL course; classroom-based instruction with in-class writing tasks and out-of-class revision, with AI use occurring via ChatGPT for the AI group.",
		"ethical_consideration (伦理审查与知情同意)": "The study received ethical approval from the University of Jordan Board of Ethics; it complied with institutional and international ethical guidelines; written informed consent was obtained from all participants; no personally identifiable information was collected or shared with the AI tool; writing samples and scores were anonymized and stored securely; AI interactions were conducted with chat history disabled to prevent data retention.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students in the AI group were required to use ChatGPT with chat history turned off to protect privacy and data security; they were explicitly instructed to critically evaluate AI feedback, avoid overreliance on AI, and revise their own work; teacher support and oversight were available; the study acknowledges ethical concerns such as data privacy, plagiarism, and overreliance and frames AI use within institutional guidelines.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "Safety measures included disabling chat history for AI interactions to avoid data retention, standardizing and extensively testing prompts before deployment to reduce risks of hallucinations and generic responses, instructing students to critically evaluate AI feedback, and providing teacher mediation for clarification; no additional technical content filters or guardrail parameters were detailed.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Both AI-generated feedback (via ChatGPT GPT-4) and teacher-generated feedback led to statistically significant improvements in EFL students’ argumentative writing performance across all subgroups, with large pre–post effect sizes. There was no statistically significant difference in gain scores between the AI and teacher feedback groups (t(118)=0.55, p=0.586, Cohen’s d=0.10), indicating comparable effectiveness of AI and teacher feedback under the study conditions. Two-way ANOVA showed a strong main effect of proficiency level on post-test scores, with Advanced-Low learners achieving higher final scores than Intermediate-Low learners, but no significant interaction between feedback type and proficiency level, suggesting that AI feedback did not disadvantage lower-proficiency learners. Intermediate-Low learners in both conditions showed the greatest within-group gains, indicating that structured feedback—whether AI- or teacher-based—was particularly impactful for lower-proficiency students. The findings support the potential of LLMs, when guided by carefully designed prompts and embedded in ethical, scaffolded practices, to serve as scalable complements to teacher feedback in large, mixed-proficiency EFL writing classrooms."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Using a validated analytic rubric to score pre- and post-revision argumentative essays, the study found that both AI-generated and teacher-generated feedback produced significant improvements in students’ writing performance, with large within-group effect sizes; AI feedback was statistically equivalent to teacher feedback in promoting revision gains, while proficiency level significantly influenced post-test performance, particularly benefiting Intermediate-Low learners’ improvement.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Analytic writing rubric with five equally weighted dimensions (Claim Clarity and Relevance; Support and Evidence; Counterarguments and Rebuttal; Organization and Coherence; Language Use) scored on a 5-point scale (1–5) for a maximum total score of 25; rubric developed from existing EFL writing frameworks and reviewed by L2 writing experts.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Argumentative writing quality with focus on clarity and relevance of the thesis/claim, strength and appropriateness of supporting evidence, presence and handling of counterarguments and rebuttals, organization and coherence of the essay, and grammatical accuracy and vocabulary use.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "NA",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "NA",
		"cognitive_aspect_measure (认知因素测量工具)": "NA",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NA",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "Inter-rater reliability metrics (Pearson’s r, mean absolute difference, ICC) for a subsample of essays; no additional instruments beyond the analytic rubric and quantitative score analyses were reported.",
		"other_outcomes_focus (其他结果维度说明)": "Reliability of scoring procedures, examined via Pearson correlation, mean absolute difference, and ICC for a stratified subsample of essays, supporting the consistency of rubric-based assessment; no separate measures of feedback uptake, attitudes, or perceptions were included.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pre-test (Draft 1 written before feedback) and immediate post-test (Draft 2 written after feedback and 1 week of revision); no delayed follow-up assessment beyond this single revision cycle.",
		"primary_outcome_variables (主要结果变量_因变量)": "Total analytic rubric score for the argumentative essay at pre-test and post-test; gain score calculated as post-test minus pre-test total score.",
		"independent_variables_and_factors (自变量与实验因素)": "Feedback Type (AI-generated feedback vs teacher-generated feedback); Proficiency Level (Intermediate-Low vs Advanced-Low); the interaction between feedback type and proficiency level in a 2×2 factorial design.",
		"followup_length_and_type (随访时长与类型)": "No longitudinal follow-up; only immediate post-revision outcomes were measured following a 1-week revision period.",
		"statistical_significance (统计显著性结果摘要)": "Paired-samples t-tests showed statistically significant pre–post improvements in all four subgroups: AI Advanced-Low t=8.32, p=0.0; AI Intermediate-Low t=14.10, p=0.0; Teacher Advanced-Low t=7.60, p=0.0; Teacher Intermediate-Low t=10.92, p=0.0. Independent-samples t-test on gain scores between AI and teacher feedback groups yielded t(118)=0.55, p=0.586, indicating no statistically significant difference in revision gains between feedback modalities. Two-way ANOVA revealed a significant main effect of proficiency level on post-test scores (F=74.36, p≈3.89e−14), but no significant main effect of feedback type (F≈1.06, p≈0.305) and no significant interaction effect between feedback type and proficiency level (F≈1.30, p≈0.257).",
		"effect_size_summary (效应量摘要)": "Large to very large within-group effect sizes (Cohen’s d) for pre–post improvement: AI Advanced-Low d=1.518; AI Intermediate-Low d=2.575; Teacher Advanced-Low d=1.388; Teacher Intermediate-Low d=1.993; between-group comparison of gain scores (AI vs teacher) showed a very small effect size (Cohen’s d=0.10) indicating practical equivalence; partial eta squared (η²) values for ANOVA are not numerically reported.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study acknowledges potential risks of AI feedback identified in the broader literature, including hallucinated or generic critiques, missing contextually nuanced issues, overreliance on AI, passive uptake, and plagiarism; these concerns were mitigated in this study by using standardized, tested prompts, disabling chat history, providing ethical and critical-use instructions, and maintaining teacher support for clarification; no empirical evidence of systematic misuse or negative performance effects due to AI was reported in the outcomes.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The main subgroup analysis focused on proficiency level: Advanced-Low learners achieved higher post-test scores, but Intermediate-Low learners showed the largest within-group gains under both feedback conditions; the absence of a significant interaction between feedback type and proficiency level suggests that AI feedback did not differentially disadvantage or advantage learners by proficiency level; no analyses by gender, socioeconomic status, or other equity-related variables were reported.",
		"limitation (研究局限)": "Limitations include the use of a single argumentative writing prompt, limiting generalizability to other genres and tasks; essays were mostly scored by a single primary rater, with inter-rater reliability established only on a 30% subsample; post-test scores showed lower inter-rater Pearson correlation than pre-test scores, indicating some variability in rating of revised writing; only short-term effects from a single revision cycle were examined, with no longitudinal follow-up; the study did not directly measure feedback uptake processes or learners’ perceptions; and results are based on a single institutional context and may not generalize across different institutions or EFL populations.",
		"challenge (实施挑战与风险)": "Challenges and risks noted or implied include ensuring consistent and reliable rubric-based scoring across raters, particularly for post-revision essays; the need to manage potential overreliance on AI feedback and passive uptake of AI suggestions; addressing limitations of AI feedback such as hallucinations, generic comments, and failure to capture nuanced rhetorical or contextual issues; integrating AI feedback ethically and effectively in mixed-proficiency classrooms; and the limitation of using only one writing task and short-term outcome measures, which constrains understanding of long-term skill development and transfer.",
		"future_work (未来研究方向)": "The authors recommend future research that includes full dual scoring of all essays with stronger rater calibration and ICC reporting, longitudinal designs with multiple writing and revision cycles to assess sustained effects of AI-assisted feedback, use of multiple writing genres and disciplinary tasks to test genre effects, more differentiated AI prompt strategies targeting specific revision behaviors, mixed-methods approaches incorporating qualitative data (e.g., think-aloud protocols, stimulated recall interviews, reflective journals, digital revision logs) to investigate feedback uptake and cognitive processes, and comparative studies exploring student attitudes, confidence, and engagement with AI-generated versus teacher-provided feedback in diverse EFL contexts.",
		"implication (理论与教学实践启示)": "The study suggests that large language models like ChatGPT (GPT-4), when deployed with well-designed prompts and pedagogical scaffolding, can provide AI-generated feedback that is as effective as teacher feedback for improving EFL argumentative writing, supporting their use as scalable feedback partners in large, mixed-proficiency classrooms; it highlights the potential of standardized, rubric-aligned AI prompts to deliver genre-specific, rhetorical-level guidance, freeing instructors to focus on higher-order pedagogical tasks; it also underscores that AI should complement rather than replace teacher feedback, ideally functioning as a preliminary revision tool in tiered feedback models; theoretically, the findings support perspectives that emphasize feedback engagement and dialogic feedback and point to the need to train learners to critically evaluate AI feedback to avoid overreliance and to maintain the human element in language teaching."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "The authors state that no financial support was received for the research and/or publication of this article.",
		"conflict_of_interest (利益冲突声明)": "The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Group assignment was based on intact classroom sections with stratification by proficiency level; no randomization was used, which may introduce selection bias; however, equal group sizes and similar pre-test means across AI and teacher groups suggest some baseline comparability, though formal baseline equivalence tests were not reported.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "A second rater scored a subsample of essays blind to group assignment, but the primary rater scored all essays and was not explicitly reported as blinded; students and instructors were aware of feedback condition as part of the intervention; thus, performance and detection bias cannot be ruled out.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The study reports that 120 participants were included in the final analysis; attrition, dropouts, or missing data handling procedures are not discussed.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of the participants, design, writing task, feedback conditions, analytic rubric, inter-rater reliability procedures, and statistical analyses; tables present means, standard deviations, ranges, gain scores, t-values, p-values, and summarized ANOVA results; a data availability statement notes that original contributions are included in the article and supplementary material and that inquiries can be directed to the corresponding author; the authors also include a Generative AI statement indicating that no Generative AI was used in the creation of the manuscript.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The model used is identified as ChatGPT (GPT-4) and the AI mentor prompt, analytic rubric, and representative feedback samples are provided in supplementary appendices; students used ChatGPT with chat history disabled; while this information improves reproducibility of the AI component, detailed implementation settings (e.g., temperature, API vs UI) and full chat logs are not reported, so exact replication of AI behavior may be limited.",
		"baseline_equivalence (基线等同性检验)": "Pre-test mean scores were very similar across AI and teacher groups within each proficiency level (e.g., AI Advanced-Low pre M=17.23 vs Teacher Advanced-Low pre M=17.20; AI Intermediate-Low pre M=12.50 vs Teacher Intermediate-Low pre M=12.40), suggesting comparable baseline performance; however, no formal statistical tests of baseline equivalence are reported.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Before conducting parametric analyses, the authors report evaluating normality using Shapiro–Wilk tests and homogeneity of variance using Levene’s tests to ensure the appropriateness of t-tests and ANOVA; further diagnostics (e.g., residual plots) are not described.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR (no explicit procedures for identifying or handling outliers, and no sensitivity analyses are reported).",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Pre-test (Draft 1) and post-test (Draft 2) total rubric scores were computed for each student; gain scores were calculated as post-test minus pre-test; data were organized in a structured Excel database and then analyzed using Python (pandas, scipy, statsmodels); no additional transformations (e.g., standardization, log transformation) are reported."
	}
}