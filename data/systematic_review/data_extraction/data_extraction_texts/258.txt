{
	"Basic Identification": {
		"author (作者)": "Liqaa Habeb Al-Obaydi, Marcel Pikhart",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Iraq and Czech Republic",
		"journal_name (期刊名称)": "Language Testing in Asia",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; qualitative-dominant mixed-method exploratory pilot study comparing AI-based and human peer assessment in EFL writing",
		"doi_or_identifier (DOI或唯一标识)": "10.1186/s40468-025-00375-8",
		"research_aims (研究目的与问题)": "To compare AI-based peer assessment using ChatGPT with traditional human peer assessment for EFL college students’ writing skills; to examine the impact of AI-partner versus human-partner peer assessment on students’ writing abilities, on the perceived quality and usefulness of feedback, and on students’ engagement (behavioral, emotional, cognitive, and social).",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses limited research directly comparing ChatGPT-based peer assessment with human peer assessment for EFL college students’ writing in different socio-cultural contexts; uses detailed analytic rubrics, controlled writing tasks, and teacher evaluation sheets to explore how AI versus human feedback affects writing improvement, feedback quality, and multi-dimensional engagement in Iraq and Czech Republic."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Fourth-year undergraduate college students",
		"language_proficiency (语言熟练度水平)": "Fourth-year EFL college students whose writing skills are described as advanced enough to participate in the experiment; no standardized proficiency test scores reported.",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "Iraqi participants: 50% female and 50% male; Czech participants: 4 male and 4 female; overall equal numbers of male and female students.",
		"age (年龄)": "Participants aged 22–23 years.",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL higher education context in Iraq and in the Czech Republic.",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Iraqi students: English Department, College of Education, University of Diyala; Czech students: information and communication technology at the Faculty of Informatics and Management, University of Hradec Kralove.",
		"prior_experience_llm (既有LLM使用经验)": "Prior use of ChatGPT or other LLMs before the study is not reported; all participants were trained on computer and mobile applications and Group B students were trained to use ChatGPT prompts for peer assessment within the study."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Qualitative-dominant mixed-method exploratory pilot with two groups (human peer assessment vs AI-based peer assessment using ChatGPT-4) completing four repeated writing and assessment cycles over approximately one month.",
		"research_method (研究方法_定量_定性_混合)": "Qualitative-dominant mixed methods combining small-sample descriptive quantitative rubric scores with qualitative teacher observations and thematic analysis.",
		"sampling_method (抽样方法)": "Purposive sampling of fourth-year EFL students; volunteers were then chosen at random from larger groups at each university and assigned to Group A (human peer assessment) or Group B (AI-based peer assessment) with efforts to match demographic attributes.",
		"sample_size_and_effect (样本量及效应量)": "Total N = 16; Group A (human peer assessment) n = 8 (4 Iraqi, 4 Czech), Group B (ChatGPT-based peer assessment) n = 8 (4 Iraqi, 4 Czech). For Group A in Iraq, final composition scores ranged from 7 to 14/15 and increased across C1–C4; in the Czech Republic Group A scores ranged from 8 to 14/15 with similar improvement. For Group B in Iraq, final scores ranged from 6 to 12/15 with progressive increases; in the Czech Republic Group B scores also ranged from 6 to 12/15 with improvement over compositions. Both groups showed steady score increases over four compositions, with Group A generally achieving slightly higher final scores; no inferential statistics or formal effect sizes were reported.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in literature on feedback in education, peer assessment, AI in higher education, and student engagement (behavioral, emotional, cognitive, social); the study cites work on feedback quality, peer feedback, and AI-generated feedback but does not adopt a single explicit named theoretical framework.",
		"data_collection_instrument (数据收集工具)": "Analytic writing rubric with five criteria (main idea/focus, organization and format, language use and style, originality, creativity) each scored 1–3 for a total of 15; four composition tasks on pre-defined topics; ChatGPT-4 prompt sequence for rubric-aligned AI feedback in Group B; teacher evaluation sheet/behavior checklist to rate effectiveness of feedback, feedback quality and perception, impact on learning outcomes, and impact on learning engagement.",
		"data_collection_validity_reliability (工具信度与效度)": "The writing rubric and evaluation sheet were exposed to a group of international specialists for face and content validity; small suggested modifications were incorporated and experts agreed the study plan was methodologically appropriate. Reliability coefficients for the rubric-based assessments were calculated at 0.879. The evaluation sheet’s reliability was not quantified. ChatGPT prompts were standardized and practised with students to align AI feedback with rubric criteria.",
		"data_analysis_method (数据分析方法)": "Quantitative analysis involved calculating rubric scores for each composition, summing scores across the five criteria to obtain a total out of 15, averaging across compositions per student, and descriptively comparing score trajectories for different students and groups. Qualitative analysis of teachers’ evaluation sheets followed thematic analysis steps (familiarization with data, generating initial codes, organizing codes into candidate themes, reviewing and refining themes, and presenting themes in a coherent narrative) using guidance from Creswell and Plano Clark and Braun and Clarke.",
		"unit_of_analysis (分析单位)": "Individual student writing performance (rubric scores per composition and per student) and group-level teacher evaluations of the effectiveness and impact of human versus AI peer assessment.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Participants were purposively selected fourth-year EFL students, then volunteers were randomly chosen from broader cohorts; they were organized into two groups with equal numbers from Iraq and the Czech Republic: Group A using human peer assessment and Group B using ChatGPT-4 for peer assessment; the allocation procedure to groups is described but not explicitly stated as fully random.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Small exploratory pilot sample of 16 fourth-year EFL college students from one Iraqi and one Czech university; contexts were selected to provide geographical and cultural diversity, but the authors acknowledge that these two institutions cannot represent their entire regions and that the small sample limits generalizability.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "For Group A, students used the rubric to assess peers’ compositions; they received training on giving constructive criticism, practised scoring and alignment in calibration sessions, and were given thorough explanations and examples of each rubric criterion. The rubric and evaluation procedure were reviewed by international specialists. For Group B, students asked ChatGPT-4 to revise compositions and score them based on the same rubric criteria using a fixed set of prompts, with teachers first demonstrating the process and supervising its use to ensure correct prompting and comparable feedback."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Four composition tasks (C1–C4) written and assessed once per week over about one month; Group A peer assessments took about 30 minutes per composition and Group B ChatGPT-based assessments about 5–10 minutes per composition.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (GPT-4)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "Students in Group B used their own ChatGPT-4 accounts via the standard interface; they input their full compositions and used a prescribed prompt sequence to request revision based on rubric criteria, a list of faults as points, and a final score out of 15; specific API parameters or system-level settings (e.g., temperature) were not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students in the AI group interacted directly with ChatGPT-4 as an AI partner for peer assessment on their own devices, entering compositions and questions and receiving feedback and scores; the tool was used under teacher supervision but not embedded into a separate learning management system.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Group B students followed a standardized prompting sequence: (1) ask ChatGPT to revise their composition based on main idea/focus, organization and format, language use and style, originality, and creativity; (2) paste the composition; (3) ask ChatGPT to list their faults as points; (4) ask ChatGPT to rate the work out of 15. Teachers demonstrated this procedure, and students used it repeatedly for the four compositions.",
		"training_support_llm_literacy (LLM素养与提示培训)": "All participants were ensured adequate training on computers and mobile applications. Group B students were additionally trained in how to use ChatGPT-4 with the specified prompts; the teacher entered the questions first with the students to show the process and avoid obstacles, and peer assessment using ChatGPT was conducted under teacher supervision during college time; broader AI literacy topics such as critical appraisal, bias awareness, or plagiarism prevention were not detailed.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Both groups were assigned the same set of four composition topics. Students wrote and printed their compositions, often from home to save time. In Group A (human peer assessment), during class each student read a peer’s composition and assessed it using the rubric for main idea/focus, organization and format, language use and style, originality, and creativity, assigning a total score out of 15; this was repeated across four compositions over a month. In Group B (ChatGPT-4), students input their compositions into ChatGPT using the agreed prompts to receive detailed revision comments, a list of faults, and a total score out of 15. Throughout the process, the two teacher-researchers observed classroom activities, held periodic discussions with students, and completed evaluation sheets rating feedback effectiveness, quality, learning outcomes, and engagement.",
		"experimental_group_intervention (实验组干预内容)": "AI-based peer assessment condition (Group B): students wrote four English compositions on fixed topics and then used ChatGPT-4 as an AI partner to revise their work according to rubric criteria, receive fault lists, and obtain a score out of 15, following a standard prompt sequence under teacher supervision.",
		"control_group_intervention (对照组干预内容)": "Traditional human peer assessment condition (Group A): students wrote the same four English compositions on identical topics and then assessed peers’ compositions in class using the paper-based rubric, providing scores out of 15 based on main idea/focus, organization and format, language use and style, originality, and creativity.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Post-draft feedback and evaluation stage of student compositions, repeated over four tasks; feedback informed subsequent compositions but no detailed within-task revision of the same composition was described.",
		"writing_genre (写作体裁)": "Short English compositions/essays on general expository or argumentative topics such as mobile phones being positive and negative, newspaper reading as a habit, expressing gratitude to teachers, and education and technology.",
		"writing_task_type (写作任务类型)": "Take-home or out-of-class composition writing on four pre-assigned topics with subsequent in-class peer assessment using either human peers with a rubric (Group A) or ChatGPT-4 feedback aligned to the same rubric (Group B).",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT-4 functioned as an AI peer assessor providing automated written feedback on compositions, identifying faults, and assigning rubric-aligned scores out of 15; it did not generate the compositions themselves or act as an instructor.",
		"role_instructor (教师角色与介入方式)": "The two authors acted as teachers and researchers: they designed the study and instruments, selected and trained participants, demonstrated peer assessment and ChatGPT-4 prompting procedures, supervised the assessment processes, observed classroom behaviour, discussed experiences with students, and completed evaluation sheets summarizing criteria such as effectiveness, feedback quality, and engagement for both groups.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Face-to-face classroom settings in higher education: English Department, College of Education, University of Diyala (Iraq) and Faculty of Informatics and Management, University of Hradec Kralove (Czech Republic); students wrote and printed compositions partly at home but performed peer assessment and AI-based assessment during scheduled class time.",
		"ethical_consideration (伦理审查与知情同意)": "Ethical approval was obtained from the Ethics Committee of the University of Hradec Kralove; the research was conducted in accordance with the Declaration of Helsinki; participants provided informed consent to take part in the study and consent for publication; privacy and confidentiality were assured.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Only Group B students used ChatGPT-4 via their own accounts; usage was limited to supervised college time for the peer assessment tasks, and students were instructed to use standardized prompts aligned with the rubric; Group A students did not use AI for assessment within the study.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "Use of ChatGPT-4 was supervised by teachers, and prompts were constrained to rubric-based revision and scoring of students’ own compositions; the authors discuss potential algorithmic bias, the impersonal nature of AI feedback, and the need for transparent AI evaluation procedures, continuous bias assessment, and human oversight, but no specific technical content-filter settings or automated safety mechanisms are detailed.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Both traditional human peer assessment (Group A) and AI-based peer assessment using ChatGPT-4 (Group B) led to progressive improvement in students’ writing scores across four compositions, with scores in both Iraq and Czech Republic increasing from the first to the fourth composition. Group A achieved slightly higher final scores and was judged by teachers to have better learning outcomes because of reciprocal learning through writing and evaluating peers’ work. Group B, while showing slightly lower scores, benefited from feedback that was described as more accurate, faster, neutral, encouraging, autonomous, and comprehensive. Teachers reported that Group A fostered greater social, emotional, and cognitive engagement through interaction and discussion about marks, whereas Group B showed higher behavioral engagement and enthusiasm linked to using AI and developing technological skills. Overall, both methods positively contributed to students’ writing development, revealing complementary strengths of human and AI feedback mechanisms."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of AI-based versus human peer assessment was evaluated through repeated rubric-based writing scores for four compositions (total score out of 15 on five criteria) and teacher evaluation sheets rating the impact of each feedback method; both conditions showed clear improvement in composition scores over time, with traditional peer assessment slightly outperforming AI-based assessment in final scores, while AI feedback was seen as efficient and high quality.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Researcher-developed analytic writing rubric with five criteria—main idea/focus, organization and format, language use and style, originality, and creativity—each scored from 1 (poor quality) to 3 (excellent quality), yielding a total writing score out of 15; ChatGPT-4’s feedback and scoring in Group B were aligned with these same criteria.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Focus on overall quality of compositions including clarity and relevance of the main idea or focus, organization and format of ideas, correctness and effectiveness of language use and style, originality of ideas, and creativity in expressing those ideas.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Teachers’ evaluation sheet (behavior checklist) based on repeated classroom observations and discussions, used to judge affective aspects such as emotional engagement and perceived motivation; no standardized student self-report affective questionnaires were used.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Emotional and motivational engagement, including students’ enthusiasm, emotional connection, and affective responses to feedback; human peer assessment was associated with stronger emotional engagement and reciprocity, whereas AI-based feedback was perceived as neutral and less emotionally engaging despite being encouraging in tone.",
		"cognitive_aspect_measure (认知因素测量工具)": "Rubric-based writing scores across four compositions as indicators of cognitive development in writing, combined with teacher observations recorded on the evaluation sheet regarding cognitive engagement in reading, evaluating, and revising compositions.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Cognitive engagement with writing and feedback processes, including critical evaluation of texts, application of rubric criteria, depth of thinking during peer assessment, and development of writing-related skills such as organizing ideas, refining language, and experimenting with originality and creativity; teachers noted that Group A required deeper reading and thinking about assessment criteria, whereas Group B mainly focused on interpreting AI-generated corrections.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Teacher observation checklists (evaluation sheets) rating behavioral engagement in class, such as participation in assessment activities, discussion and objection about marks (Group A), and observable motivation and enthusiasm in using ChatGPT-4 (Group B); no digital usage logs were reported.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Behavioral engagement, including participation, persistence, and enthusiasm in the peer assessment process; Group A showed active discussion and negotiation around peer feedback and marks, while Group B showed high motivation and enthusiasm related to interacting with ChatGPT-4 and exploring AI-based feedback.",
		"other_outcomes_measure (其他结果测量工具)": "Teachers’ evaluation sheets summarizing perceived effectiveness of feedback style, quality and perception of feedback, impact on learning outcomes, and impact on learning engagement; thematic analysis of these qualitative evaluations; descriptive comparison of Iraqi and Czech participants’ score patterns.",
		"other_outcomes_focus (其他结果维度说明)": "Perceived quality and style of feedback (accuracy, speed, neutrality, comprehensiveness, effort required), reciprocal nature of learning, development of peer-assessment technical skills versus AI-related technological skills, and teachers’ reflections on AI’s potential biases, neutrality, and the risk of reducing peer assessment to a one-way process.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Four assessment timepoints corresponding to four compositions (C1–C4), each written and assessed once per week over one month; no separate pretest beyond the first composition and no delayed post-test or long-term follow-up.",
		"primary_outcome_variables (主要结果变量_因变量)": "Rubric-based total writing scores (0–15) for each composition and per student in each group, and teachers’ qualitative ratings and narrative evaluations of feedback effectiveness, feedback quality and perception, impact on writing outcomes, and impact on learning engagement for human versus AI-based peer assessment.",
		"independent_variables_and_factors (自变量与实验因素)": "Type of peer assessment partner: AI partner using ChatGPT-4 (Group B) versus human partner (traditional peer assessment in Group A); geographical context (Iraq versus Czech Republic) was controlled and reported but not treated as a formal experimental factor in the analysis.",
		"followup_length_and_type (随访时长与类型)": "No long-term follow-up study; outcomes were tracked only over the four weekly composition and assessment cycles during approximately one month.",
		"statistical_significance (统计显著性结果摘要)": "No inferential statistics such as t-tests, ANOVA, or p-values were reported; analysis relied on descriptive comparisons of score ranges and patterns across compositions and groups, showing steady improvement in both groups and slightly higher final scores in the human peer assessment group.",
		"effect_size_summary (效应量摘要)": "NA",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study did not report instances of overt misuse of ChatGPT-4 by students but highlighted several concerns: AI feedback can be impersonal and may lower emotional engagement; algorithmic bias in AI systems could reinforce existing educational disparities depending on training data; AI has limitations in answering complex questions and lacks nuanced understanding and contextual sensitivity; there is a risk that relying solely on AI feedback may reduce the reciprocal nature and perceived credibility of peer assessment if students only write and accept AI corrections without engaging in critical discussion.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The sample intentionally included students from Iraq and the Czech Republic to introduce geographical and cultural diversity, and both groups used the same tasks and tools; results showed almost identical score patterns between the two contexts, so data were combined and no significant cross-context differences were reported. The authors note that socio-cultural and socio-educational environments might influence how EFL learners use peer assessment and AI, and they stress the importance of incorporating cultural considerations in analysis, but they did not present formal subgroup statistical analyses.",
		"limitation (研究局限)": "Major limitations include the very small sample size (N = 16) and exploratory pilot design, which limit generalizability; reliance on descriptive statistics without inferential testing; potential bias in teacher observations and any self-reported data; differences in implementation procedures between paper-based peer assessment and ChatGPT-based assessment that may introduce procedural bias; absence of long-term follow-up to see sustained writing gains; and inherent constraints and ethical issues associated with AI feedback, including algorithmic bias, technical limitations, and impersonal interaction.",
		"challenge (实施挑战与风险)": "Challenges and risks include aligning AI-generated feedback with pedagogical goals and rubric criteria, dealing with potential algorithmic bias and ethical issues, managing reduced emotional and social engagement in AI-based feedback, ensuring that AI complements rather than replaces human peer assessment, mitigating differences in task execution between paper-based and AI-mediated assessment, training students and teachers to use AI tools effectively, and developing institutional guidelines for transparent, supervised, and fair use of AI in peer assessment.",
		"future_work (未来研究方向)": "Future research should expand sample sizes and include more diverse participants to allow robust statistical analyses and generalizable findings; conduct longitudinal studies to examine long-term effects of AI versus human peer feedback on writing skills and academic performance; explore the integration of other AI tools and more advanced models into peer assessment; investigate and address algorithmic bias and ethical implications of AI use in educational assessment; develop explicit rubrics and baseline criteria to align human and AI assessors; provide comprehensive AI literacy training for educators; design and test hybrid models that gradually increase AI involvement while maintaining human supervision; and perform cross-cultural comparisons to understand how different educational and cultural contexts mediate the effectiveness of AI-supported peer assessment.",
		"implication (理论与教学实践启示)": "The study implies that AI-based feedback via ChatGPT-4 can make peer assessment more efficient, consistent, and scalable by delivering quick, rubric-aligned, and comprehensive feedback, while traditional human peer assessment remains vital for fostering deeper social, emotional, and cognitive engagement, reciprocal learning, and critical assessment skills in EFL writing. Educators are encouraged to adopt blended approaches that combine AI and human feedback, to maintain teacher oversight and support, to use AI as a complementary tool rather than a replacement, and to provide training for students and teachers on effective, ethical, and context-sensitive use of AI in L2 writing assessment and instruction."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "The authors report that no funding was received; the research is noted as part of the Excellence 2025 project at the Faculty of Informatics and Management, University of Hradec Kralove.",
		"conflict_of_interest (利益冲突声明)": "The authors declare no competing interests.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Participants were purposively selected fourth-year EFL students and volunteers were randomly chosen, but group assignment to human versus AI conditions was not clearly described as fully random and each group contained only eight students; teachers were also the researchers, so there is a risk of selection and allocation bias.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Participants and teachers were aware of group assignments because the intervention types (human peer assessment vs ChatGPT-4-based assessment) were obvious; no blinding of outcome assessors or participants was reported.",
		"attrition_and_missing_data (流失与缺失数据处理)": "No participant dropout or missing data were mentioned; it appears that all 16 students completed the four compositions and peer assessment cycles in their assigned groups.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of participants, contexts, procedures for both groups, rubric criteria, composition topics, and sample score tables, as well as the structure of the teachers’ evaluation sheet and the thematic analysis process; limitations and ethical issues are explicitly discussed. However, no raw datasets are shared and no inferential statistical analyses are reported.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies using ChatGPT-4 and documents the prompt sequence students employed, enabling conceptual replication with similar models; however, it does not report detailed configuration parameters or system prompts, so exact replication of AI behavior might vary with future versions.",
		"baseline_equivalence (基线等同性检验)": "The authors state that Group A and Group B had similar demographic attributes and that efforts were made to control for previous academic level and technology usage skills; equal numbers of Iraqi and Czech students were placed in each group, but no formal statistical tests of baseline equivalence in writing ability or other variables were reported.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "No formal statistical assumption checks or diagnostics (e.g., normality, homogeneity of variance) were conducted or reported, as the analysis relied on descriptive statistics and qualitative thematic analysis rather than inferential tests.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Rubric scores for each composition were compiled into tables and used to compute totals and descriptive patterns across compositions and groups; no data transformation or complex preprocessing was described. Teacher evaluation sheet responses were transcribed and coded for thematic analysis without reported transformation."
	}
}