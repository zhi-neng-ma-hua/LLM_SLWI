{
	"Basic Identification": {
		"author (作者)": "Nouf J. Aljohani",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Saudi Arabia (University of Jeddah English language institute context)",
		"journal_name (期刊名称)": "Forum for Linguistic Studies",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; longitudinal mixed-methods empirical study with one-group pretest–posttest design",
		"doi_or_identifier (DOI或唯一标识)": "https://doi.org/10.30564/fls.v7i7.9849",
		"research_aims (研究目的与问题)": "To evaluate the effectiveness of instructional assistance (IA) formative feedback provided by ChatGPT in enhancing EFL learners' writing skills; to examine how integrating ChatGPT formative feedback into the writing process impacts students' writing skills across structure, organisation, mechanics/style/diction, and argumentation/critical thinking; to investigate changes in students' perceived usefulness, perceived ease of use, attitudes toward usage, and behavioural intention to use ChatGPT for formative feedback before and after experience with the tool; and to explore how perceived ease of use of ChatGPT correlates with improvements in students' writing skills.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses the limited research on how ChatGPT-based formative feedback influences EFL learners' writing development, particularly at the level of specific writing attributes such as syntactic errors, logic, word choice, clarity, composition, and logical development of ideas; responds to calls for research on what writers actually do with AI feedback and whether such feedback leads to more and better revision; integrates Technology Acceptance Model constructs with objective writing performance and think-aloud data to provide a comprehensive view of ChatGPT formative feedback in EFL writing."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate students in English language courses at an English language institute",
		"language_proficiency (语言熟练度水平)": "Upper-intermediate Level 3 based on the institute entry test (described as upper-intermediate level)",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "Average age approximately 24 years",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in an English language institute in Saudi Arabia",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "English language learners at an English language institute; detailed majors beyond English language NR",
		"prior_experience_llm (既有LLM使用经验)": "Participants had general technology familiarity but specific prior experience with ChatGPT or structured AI writing instruction was not reported; NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Cohesive longitudinal model combining longitudinal and cross-sectional elements; one-group pretest–posttest design following the same cohort of EFL undergraduates over a 16-week ChatGPT formative feedback intervention, with repeated measures on writing performance and TAM constructs and think-aloud protocols during AI formative feedback sessions.",
		"research_method (研究方法_定量_定性_混合)": "Mixed-methods design combining quantitative analyses (MANOVA, paired-samples t-tests, correlation, multiple regression) with qualitative think-aloud protocols analysed via inductive thematic analysis.",
		"sampling_method (抽样方法)": "Targeted sampling of 50 undergraduates from an English language institute; participants were a particular group of upper-intermediate Level 3 students with similar educational backgrounds and motivations; they were sorted into groups at random for study purposes, but no separate control group was used.",
		"sample_size_and_effect (样本量及效应量)": "Student participants: N=50 undergraduates (average age 24, upper-intermediate English Level 3); essay rating instructors: 5 experienced English language teachers; one specialist reviewer used for expert judgment. Writing outcomes: significant pre–post gains on all four rubric criteria. Structure: before M=4.02 (SD=0.901), after M=6.10 (SD=0.823), t(48)≈−11.94, p<0.001. Organisation: before M=3.84 (SD=0.746), after M=5.78 (SD=0.823), t(48)≈−12.22, p<0.001. Mechanics and diction style: before M=3.53 (SD=0.680), after M=5.49 (SD=0.960), t(48)≈−11.66, p<0.001. Argumentation and critical thinking: before M=4.14 (SD=0.842), after M=6.43 (SD=0.707), t(48)≈−14.56, p<0.001. One-way repeated-measures MANOVA: Pillai’s Trace for ChatGPT=0.816, F(4,93)=103.237, p<0.001, indicating a strong multivariate effect on combined writing skills. Univariate one-way ANOVA after intervention: F(1,96) for structure≈142.60, organisation≈149.28, mechanics/style≈135.86, argumentation/critical thinking≈211.86, all p<0.001; explicit effect size indices such as Cohen’s d or partial eta squared were not reported.",
		"theoretical_foundation (理论基础_理论框架)": "Formative feedback theory emphasising confirmation and explanation as efficient feedback components; Technology Acceptance Model (TAM) including perceived usefulness, perceived ease of use, attitudes toward usage, and behavioural intention; Ajzen’s Theory of Planned Behavior as a supporting framework for intention and technology adoption; literature on AI-assisted writing, generative AI (ChatGPT) in language learning, and CALL; metacognition and cognitive dissonance theories framing emotional and cognitive responses to feedback; feedback loop and Continuous Assessment and Support System (CASS) concepts characterising iterative AI-supported feedback cycles.",
		"data_collection_instrument (数据收集工具)": "Writing rubric with four validated criteria (structure; organisation; mechanics, style, and diction; argumentation and critical thinking) used by five language instructors and a specialist reviewer to rate students' essays before and after the intervention. TAM-based survey adapted from Shroff et al. with 27 items on a 5-point Likert scale (1=strongly disagree to 5=strongly agree), including perceived usefulness (16 items), perceived ease of use (3 items), attitudes toward usage (5 items), and communicative or behavioural intention to use (3 items), administered after the initial use of ChatGPT and after the experiment. Think-aloud protocols collected during AI formative feedback sessions in which students verbalised their thoughts while using ChatGPT. Observation of sessions by the researcher. No separate standardised language test beyond the institute's entry test is used in the study.",
		"data_collection_validity_reliability (工具信度与效度)": "The writing rubric had been previously validated and used by instructors at the English language institute; five ESL raters and one specialist reviewer applied it, with the specialist using an expert-judgment model to adjust scores where necessary. The TAM instrument was adapted from previously validated TAM scales in educational and CALL contexts; however, specific reliability coefficients (e.g., Cronbach's alpha) and validity indices for this study's sample were not reported. Thematic analysis was grounded in established inductive procedures; triangulation through think-aloud, TAM surveys, and quantitative writing scores aimed to enhance credibility.",
		"data_analysis_method (数据分析方法)": "Expert judgment scoring model: five ESL raters scored first and last essays using the rubric; a specialist reviewer provided blind assessments and could adjust initial scores, assuming higher precision. Quantitative analyses: paired-samples t-tests for each writing rubric criterion to compare pre- and post-ChatGPT formative feedback scores; one-way repeated-measures MANOVA to assess the multivariate effect of ChatGPT formative feedback on combined writing skills; one-way ANOVA for each writing criterion; descriptive statistics (means, standard deviations); correlation analyses between writing criteria before and after ChatGPT use; paired-samples t-tests on TAM dimensions (perceived usefulness, perceived ease of use, attitudes, behavioural intention) pre and post intervention; multiple regression analyses predicting behavioural intention from perceived usefulness, perceived ease of use, and attitudes toward usage before and after the intervention. Qualitative analyses: inductive thematic analysis of think-aloud data to identify emergent themes (e.g., prediction and uncertainty, emotional responses to feedback, revisions as learning, role of peer/instructor feedback, strategic use of feedback).",
		"unit_of_analysis (分析单位)": "Individual student for TAM scores, behavioural intention, and think-aloud data; individual essays for writing rubric scores; individual participants for regression analyses and correlations.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Participants belonged to a targeted cohort of upper-intermediate students; although the study notes that participants were sorted into groups at random, the main analytic design is a single group with repeated measures pre and post intervention; there is no separate non-AI control group.",
		"power_analysis (功效分析与样本量论证)": "No formal a priori power analysis was reported; the study used a convenience cohort of 50 undergraduates and reported statistically significant results but did not justify sample size through power calculations.",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame was a particular group of upper-intermediate Level 3 undergraduates at an English language institute linked to the University of Jeddah; they had similar educational backgrounds and motivations to learn. The authors note the small sample and specific EFL context, and recognise that the findings may not generalise to all EFL groups, proficiency levels, or educational settings.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Five language course instructors from the English language institute rated students' essays before and after the intervention using the four-criterion rubric. A specialist reviewer acted as a third evaluator, providing blind assessments of each essay and using an expert judgment design in which specialist ratings could supersede or adjust initial ratings if necessary. The paper reports that a comprehensive training process for assessors included detailed instructions on using the rubric, exposure to essays representing various grades, collaborative exercises to reach consensus on scores, and regular meetings to review scoring discrepancies and refine rubric application, aiming to promote consistency and reproducibility."
	},
	"Intervention": {
		"duration (干预时长与频率)": "The experimental period lasted 16 weeks. Within this period, students received ChatGPT training workshops, wrote multiple essays over successive units (each essay type unit lasting approximately 4 weeks), underwent repeated AI formative feedback sessions and reflective presentations, and had pre- and post-intervention essays rated.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (OpenAI generative AI large language model; specific underlying model version NR)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT is described as an AI-supported chatbot designed by OpenAI and used to generate formative feedback on student essays; the article does not specify whether it was accessed via web interface or API, nor does it provide model version, temperature, or other parameter settings; configuration details are NR.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students directly interacted with ChatGPT as a formative feedback tool: after writing and submitting essays on Blackboard at the end of lectures (without AI assistance during drafting), they used ChatGPT to obtain comprehensive feedback on their essays. ChatGPT was not embedded into the LMS but used alongside it; teachers provided training, monitored sessions, and observed think-aloud protocols while students used the chatbot.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "The study involved tutorials and a structured program to teach students how to use ChatGPT to generate formative feedback, including workshops and practical exercises focused on searching through data, understanding the model’s capabilities, and utilising feedback to enhance writing; however, it does not describe specific prompt-engineering strategies such as chain-of-thought, role prompting, or rubric-based prompts. Students generally prompted ChatGPT to obtain detailed formative feedback on their essays and then used this feedback to revise.",
		"training_support_llm_literacy (LLM素养与提示培训)": "Before and during the 16-week intervention, the researcher conducted tutorials on how to use ChatGPT to generate formative feedback, using a structured program of workshops and practical exercises to help students correctly search, understand ChatGPT’s capabilities, and apply feedback to improve essays. Students were guided on integrating chatbot feedback into their writing process and participated in reflective classroom presentations about common mistakes, thereby developing metacognitive awareness and responsible AI use. Teacher-led training on the scoring rubric was given to instructors; student training focused on ChatGPT literacy rather than on academic integrity procedures beyond consent and process description.",
		"intervention_implementation (干预实施流程_步骤与任务)": "After obtaining ethical clearance and informed consent, the 16-week procedure began with ChatGPT training workshops for students. The TAM survey was administered after the initial use of ChatGPT and again after the full experiment. During the later weeks, language teachers introduced different types of essays, and each essay type unit lasted about 4 weeks. In the last two weeks of each unit, students wrote essays in class and submitted them immediately via Blackboard to prevent use of chatbots during writing. Instead of teacher-written feedback, students then used the ChatGPT chatbot to receive comprehensive formative feedback on their essays. During feedback sessions, students conducted small presentations reflecting on the most common mistakes identified in their essays. Throughout these sessions, the researcher observed and facilitated think-aloud protocols while students voiced their thoughts and reasoning as they reviewed ChatGPT feedback. This cycle of writing, AI feedback, and reflection was repeated multiple times across the 16 weeks. Finally, five instructors rated each student’s first and last essays (written without AI support), and TAM post-surveys were collected.",
		"experimental_group_intervention (实验组干预内容)": "All 50 students formed a single intervention cohort: they received training on ChatGPT use, wrote essays without AI assistance in class, and then received ChatGPT formative feedback on these essays. They engaged in repeated cycles of AI-supported formative feedback, think-aloud reflection, and classroom presentations about their errors and revisions, with the aim of improving writing skills and increasing autonomy and confidence. There was no separate control group; the pre-intervention essays and initial TAM responses served as baselines for comparison with post-intervention performance and perceptions.",
		"control_group_intervention (对照组干预内容)": "NA (no separate control or comparison group was included; the study used a one-group pretest–posttest design).",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT was used primarily at the feedback and revision stages: students drafted essays in class without AI help, submitted them, and then used ChatGPT to receive formative feedback, identify errors and weaknesses, and guide revision. The formative feedback and subsequent revisions targeted structural organisation, mechanics/style/diction, and argumentation and critical thinking.",
		"writing_genre (写作体裁)": "Different types of essays in English, introduced as distinct essay types across units (specific genres such as argumentative, narrative, or expository are not explicitly named; described generally as essays addressing structure, organisation, style, and argumentation).",
		"writing_task_type (写作任务类型)": "Classroom essay writing tasks in English, written and submitted in class via Blackboard under time constraints to prevent AI use during drafting; followed by AI-supported revision and reflection tasks using ChatGPT formative feedback.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT served as a formative feedback provider and instructional assistance tool: evaluating learner writing, recognising sections for enhancement, displaying mistakes, offering customised recommendations to improve structure, organisation, mechanics, diction, and argumentation, and engaging students in interactive dialogue about revisions. It did not function as an official grader of final performance; instead, teachers rated essays for research purposes.",
		"role_instructor (教师角色与介入方式)": "Language instructors taught essay types, set and explained writing tasks, refrained from providing direct feedback on the intervention essays, and instead allowed students to use ChatGPT for formative feedback. They rated pre- and post-intervention essays using the rubric for research analysis and participated in collaborative rater training to ensure scoring consistency. The researcher acted as observer, facilitated think-aloud protocols, administered surveys, and oversaw the ChatGPT training workshops while ensuring ethical procedures; instructors also had a role in debriefing and supporting students as they interpreted AI feedback.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "English language institute associated with the University of Jeddah, Saudi Arabia; undergraduate English language courses in an EFL context. Instruction and writing activities took place in classroom settings with technology support, using Blackboard for essay submission and ChatGPT as an online chatbot for formative feedback; think-aloud sessions and presentations occurred in class.",
		"ethical_consideration (伦理审查与知情同意)": "The study was conducted in accordance with the Declaration of Helsinki and approved by the Ethics Committee of Jeddah University (protocol code 4356-2, dated 24-6-2024). Informed consent was obtained from all participants, who were informed that participation would not affect their grades and that they could withdraw at any time. Data availability is described as not applicable due to privacy, and no sensitive personal data were collected.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students were allowed to use ChatGPT only for formative feedback, not during the initial in-class drafting of essays; they were required to submit essays immediately at the end of the lecture to prevent reliance on chatbots for composing. Institutional policies on AI beyond the study context are not detailed. The study emphasises responsible AI use, with teacher-led tutorials and guidance on how to use ChatGPT as a feedback tool rather than as a text generator for original assignment writing.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "NR (no explicit technical safety, content filtering configurations, or moderation settings for ChatGPT are described; the main safeguards relate to task design and teacher monitoring of AI use).",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Quantitative analyses showed that integrating ChatGPT-based formative feedback significantly improved students' writing skills across all four rubric criteria: structure, organisation, mechanics and diction style, and argumentation and critical thinking, with especially large gains in argumentation and critical thinking. One-way repeated-measures MANOVA indicated a strong multivariate effect of ChatGPT formative feedback on combined writing outcomes (Pillai’s Trace≈0.816, F(4,93)≈103.24, p<0.001). Paired-samples t-tests and ANOVA confirmed significant pre–post improvements for each criterion (all p<0.001). Students’ perceptions measured via TAM also improved significantly: perceived usefulness, perceived ease of use, attitudes toward usage, and behavioural intention all increased after the intervention, with paired t-tests showing statistically significant changes. Regression analyses before and after ChatGPT use indicated that perceived usefulness, perceived ease of use, and attitudes toward usage predicted behavioural intention, with attitude becoming a stronger predictor after students gained hands-on experience. Correlation analyses showed that relationships among writing skills changed after ChatGPT use, suggesting a more integrated approach to writing. Think-aloud data revealed themes including initial prediction and uncertainty, emotional responses to feedback, revision as learning, the importance of peer and instructor feedback, and strategic use of AI feedback, indicating that students gradually used ChatGPT’s formative feedback to become more independent, reflective writers."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "The study concludes that ChatGPT-based instructional assistance formative feedback was effective in enhancing students' writing performance and empowering them as more independent writers. Effectiveness was measured through pre–post comparisons of essays rated with a four-criterion rubric (structure, organisation, mechanics/style/diction, argumentation/critical thinking) by multiple instructors and a specialist reviewer; statistical analyses (paired-samples t-tests, one-way repeated-measures MANOVA, and ANOVA) showed significant improvements in all criteria. Additionally, TAM surveys administered before and after the intervention captured significant positive changes in perceived usefulness, perceived ease of use, attitudes, and behavioural intention toward ChatGPT, and think-aloud protocols revealed qualitative evidence of increased confidence, autonomy, and strategic engagement with feedback.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Writing rubric used by the English language institute, consisting of four criteria: structure; organisation; mechanics, style, and diction; and argumentation and critical thinking. Each criterion was rated on a scale with descriptors ranging from 'excellent' (3 points) to 'no evidence' (0 points), with a note that any indication of plagiarism would result in a total score of zero. The rubric had been validated by institute instructors and was applied by five ESL raters and a specialist reviewer.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Focus on structural and organisational quality (introduction, body paragraphs, conclusion, thesis clarity, logical progression), mechanics and stylistic accuracy (clarity of sentences, flow of ideas, appropriate vocabulary, correct spelling, grammar, punctuation, capitalisation, transitions), and higher-order features of argumentation and critical thinking (compelling and well-reasoned arguments, critical analysis of the topic, identification of biases, generation of novel ideas).",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Technology Acceptance Model (TAM) based survey adapted from Shroff et al., with 27 Likert-scale items measuring perceived usefulness (16 items), perceived ease of use (3 items), attitudes toward usage (5 items), and behavioural intention or communicative goal to use ChatGPT (3 items), administered after initial ChatGPT use and after the full intervention; qualitative think-aloud protocols also captured affective reactions such as frustration, discouragement, growing confidence, and motivation.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Perceived usefulness (belief that using ChatGPT formative feedback enhances writing performance), perceived ease of use (belief that using ChatGPT is free of effort), attitudes toward using ChatGPT (positive or negative feelings about using the tool), and behavioural intention to use ChatGPT (willingness to continue using it for formative feedback). Qualitative data addressed emotional responses to feedback such as initial frustration or embarrassment, shifting to increased confidence, sense of control, and autonomy in writing.",
		"cognitive_aspect_measure (认知因素测量工具)": "Argumentation and critical thinking dimension within the writing rubric, rated by instructors and specialist reviewer; TAM survey items that indirectly relate to cognitive appraisals of usefulness; inductive thematic analysis of think-aloud protocols capturing metacognitive processes, prediction, uncertainty management, self-awareness, and strategic evaluation of ChatGPT feedback.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Critical thinking in writing (analysis, evaluation, argument strength, identification of biases, generation of novel ideas), metacognitive processes such as planning, monitoring, and revising based on feedback, students' self-awareness of thesis focus and argument strength, and cognitive strategies for using ChatGPT feedback (e.g., creating checklists from AI suggestions, systematically addressing identified problems).",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Behavioural intention subscale in the TAM survey, capturing intended future use of ChatGPT; think-aloud protocols and classroom observations documenting actual use patterns of AI feedback (e.g., how students navigated and applied feedback); no automated usage logs are reported.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Students' willingness to continue using ChatGPT for formative feedback, patterns of interacting with AI (frequency of consultation, types of feedback they focused on, step-by-step revision using AI suggestions), participation in repeated revision cycles and classroom presentations, and strategic behaviour such as using AI-generated checklists or prioritising certain feedback categories.",
		"other_outcomes_measure (其他结果测量工具)": "Multiple regression analyses predicting behavioural intention from perceived usefulness, perceived ease of use, and attitudes; correlation analysis of relationships among writing criteria before and after ChatGPT use; multivariate tests (Pillai’s Trace, Wilks’ Lambda, Hotelling’s Trace, Roy’s Largest Root) for assessing overall intervention effects; thematic analysis of think-aloud transcripts using inductive coding to identify recurring themes related to feedback processing, emotion, and strategy.",
		"other_outcomes_focus (其他结果维度说明)": "Relationships between TAM constructs and behavioural intention before and after the intervention; changes in intercorrelations among writing skills (e.g., shifting relationships between structure, organisation, mechanics, and argumentation/critical thinking) after ChatGPT use; perceived cognitive and affective implications of AI feedback; students' strategic use of AI feedback in conjunction with peer and instructor input; and reflections on the role of ChatGPT within continuous assessment and support systems.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Essays were rated at two time points: a first essay before the ChatGPT-based formative feedback intervention and a final essay after the intervention (both written and submitted without AI support). The TAM survey was administered twice: after participants’ initial use of ChatGPT and upon completion of the 16-week experiment. Think-aloud protocols and observational data were collected during multiple AI formative feedback sessions throughout the intervention. No delayed follow-up beyond the 16-week period was reported.",
		"primary_outcome_variables (主要结果变量_因变量)": "Students' writing scores in four rubric criteria (structure; organisation; mechanics and diction style; argumentation and critical thinking) before and after ChatGPT formative feedback; changes in TAM dimensions (perceived usefulness, perceived ease of use, attitudes toward usage, behavioural intention); behavioural intention to use ChatGPT as predicted by TAM constructs.",
		"independent_variables_and_factors (自变量与实验因素)": "The main independent factor was time (pre-intervention vs post-intervention) within a single cohort. In regression analyses, perceived usefulness, perceived ease of use, and attitudes toward usage were predictors of behavioural intention before and after ChatGPT use. There was no separate control group or additional experimental factor.",
		"followup_length_and_type (随访时长与类型)": "Sixteen-week longitudinal intervention with pre-intervention and post-intervention assessments embedded within the same semester; no extended delayed follow-up beyond this time window.",
		"statistical_significance (统计显著性结果摘要)": "Paired-samples t-tests showed significant improvements in all four writing criteria after using ChatGPT for formative feedback: structure t(48)≈−11.94, p<0.001; organisation t(48)≈−12.22, p<0.001; mechanics and diction style t(48)≈−11.66, p<0.001; argumentation and critical thinking t(48)≈−14.56, p<0.001. Repeated-measures MANOVA revealed a significant multivariate effect of ChatGPT use on combined writing skills (Pillai’s Trace≈0.816, F(4,93)=103.237, p<0.001). Univariate one-way ANOVAs indicated significant between-time differences for each writing criterion: structure F(1,96)=142.602, p<0.001; organisation F(1,96)=149.276, p<0.001; mechanics and diction style F(1,96)=135.862, p<0.001; argumentation and critical thinking F(1,96)=211.862, p<0.001. For TAM, paired t-tests indicated significant changes in perceived usefulness (t≈2.559, p≈0.008), perceived ease of use (t≈2.244, p≈0.012), attitudes toward usage (t≈3.516, p≈0.001), and behavioural intention (t≈−2.121, p≈0.003), reflecting a significant shift in perceptions over time. Multiple regression analyses before and after the intervention showed that perceived usefulness, perceived ease of use, and attitudes significantly predicted behavioural intention, with overall models highly significant (F values around 20.771 and 27.441, both p<0.001).",
		"effect_size_summary (效应量摘要)": "The study reports large t and F statistics and a multivariate Pillai’s Trace value of approximately 0.816 for the effect of ChatGPT formative feedback on combined writing skills, indicating a strong multivariate effect; however, explicit effect sizes such as Cohen’s d, partial eta squared, or confidence intervals for mean differences are not reported. Regression analyses report R²≈0.698 before and R²≈0.778 after the intervention for models predicting behavioural intention, suggesting that TAM variables explained about 70–78 percent of the variance in behavioural intention; no standardised beta weights or additional effect size indices beyond R² are given.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The paper acknowledges potential risks of over-reliance on AI and the possibility that students may depend too heavily on ChatGPT, which could limit deep thinking and raise academic integrity concerns. To mitigate these risks, the design prevented AI use during initial essay drafting and emphasised that ChatGPT should be used for formative feedback rather than as an authoring tool. Emotional responses such as initial frustration, embarrassment, and discouragement were observed when ChatGPT feedback contradicted students' self-perceptions, but these reactions were framed as part of a productive learning process in which students eventually used feedback to improve and felt more confident. No instances of plagiarism or explicit misuse of ChatGPT are reported; instead, the authors stress the need for clear guidelines and teacher mediation to avoid overdependence and ensure responsible AI use.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No subgroup analyses by gender, proficiency bands, age, or other demographic variables were reported, and equity-related considerations such as differential effects across subgroups are not discussed; NR.",
		"limitation (研究局限)": "The author notes that the sample size was relatively small and drawn from a single EFL group at one English language institute, limiting generalisability to other EFL populations, proficiency levels, educational settings, or cultural contexts. The lack of diversity in educational background and the specific technological familiarity of this cohort may influence how students interact with AI tools. There was no control group, making it difficult to fully separate the effects of ChatGPT from other instructional factors over the 16 weeks. The study also notes that findings may not extend universally to all contexts, and that further work is needed across different settings and with larger, more diverse samples.",
		"challenge (实施挑战与风险)": "Challenges include the potential for students to become overly reliant on AI, shaping their thinking around ChatGPT feedback rather than developing independent critical judgment; concerns about academic integrity and ensuring that essays remain students' own work in the presence of powerful generative tools; the need for ongoing teacher mediation to help students interpret and apply AI feedback effectively; the workload associated with training instructors as reliable essay raters and providing comprehensive assessor training; and the need to guide students in managing emotional responses to critical feedback so that they move from initial frustration to constructive engagement.",
		"future_work (未来研究方向)": "Future research directions include conducting longitudinal studies to assess long-term effects of ChatGPT use on writing outcomes and revision habits; integrating ChatGPT into broader classroom settings to examine its impact on student engagement and collaborative learning; conducting comparative studies that contrast ChatGPT-based formative feedback with traditional writing support methods; exploring differences across diverse EFL contexts, proficiency levels, and cultural backgrounds; and using qualitative interviews to further investigate students' perceptions, motivation, and confidence when using ChatGPT, as well as additional mediating factors such as self-regulation and epistemic beliefs.",
		"implication (理论与教学实践启示)": "Theoretically, the study supports the applicability of the Technology Acceptance Model and related behavioural intention frameworks in understanding students' adoption of AI formative feedback tools, showing that perceived usefulness, perceived ease of use, and attitudes are central to behavioural intention. It also contributes to feedback and metacognition literature by illustrating how AI-based formative feedback can stimulate self-reflection, metacognitive awareness, and iterative revision in L2 writing. For practice, the findings suggest that ChatGPT can effectively support EFL writing instruction by providing timely, detailed formative feedback that improves both lower-order (mechanics, grammar) and higher-order (organisation, argumentation, critical thinking) writing skills, while fostering student independence. Educators are encouraged to integrate ChatGPT into drafting and revision cycles with clear guidelines, teacher mediation, and reflective activities, such as presentations and think-aloud exercises, to maximise learning benefits, maintain academic integrity, and ensure that AI augments rather than replaces human feedback and instructional judgement."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "This work received no external funding.",
		"conflict_of_interest (利益冲突声明)": "The author declares no conflict of interest.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "The study used a one-group pretest–posttest design with a targeted cohort of upper-intermediate students at an English language institute; while participants were sorted into groups at random for some classroom purposes, there was no random assignment to intervention versus control and no comparison group, so selection bias and maturation effects cannot be ruled out.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "The specialist reviewer conducted blind assessment of essays within an expert-judgment framework; it is implied that raters focused on essay quality rather than knowing pre or post status, but the paper does not explicitly state whether all raters were blinded to timepoint. Students and instructors were aware of ChatGPT use, so participant and teacher blinding was not possible. Blinding of data analysts is not reported.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The study initially reports 50 student participants; some quantitative tables report N=49 for writing analyses but do not explicitly discuss attrition, missing data, or how missing values were handled. No description of participant dropout or imputation strategies is provided; NR.",
		"reporting_transparency (报告透明度与可重复性)": "The article clearly describes the research questions, theoretical framework, participants, setting, rubrics, TAM instrument structure, think-aloud procedures, intervention timeline, and main statistical methods (t-tests, MANOVA, ANOVA, correlation, regression). Results are reported with means, standard deviations, t and F values, p-values, and R² for regression, and tables present detailed pre–post scores. Ethical approval, informed consent, and conflicts of interest are disclosed. However, exact ChatGPT configuration details, raw datasets, and analysis code are not shared, and psychometric coefficients for the TAM scales in this sample are not reported.",
		"preregistration_or_protocol (预注册或研究方案)": "NR (no preregistration or pre-specified study protocol is mentioned).",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study states that ChatGPT, developed by OpenAI, was used as the AI formative feedback tool, but it does not specify the model version or configuration (e.g., GPT-3.5, GPT-4, web UI vs API), so exact replication with the same LLM instance and settings is not possible; only the general instructional design and use of ChatGPT as a feedback provider are reproducible.",
		"baseline_equivalence (基线等同性检验)": "NA (only a single cohort with pre–post measures was used; there were no separate groups for which baseline equivalence could be tested).",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The authors describe the use of MANOVA, regression, correlation, and t-tests but do not report specific checks for assumptions such as normality, homogeneity of variance, or multicollinearity, nor do they discuss diagnostic plots; NR.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "No procedures for identifying or handling outliers, and no sensitivity analyses or robustness checks, are described; NR.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Essays were collected via Blackboard and scored with a validated rubric by multiple raters and a specialist reviewer; TAM survey responses were collected using Likert scales and used directly in t-tests, correlations, and regression; think-aloud data were transcribed and analysed inductively. The paper does not mention any additional data transformations (e.g., standardisation, normalisation) or cleaning steps beyond scoring, averaging, and coding; NR."
	}
}