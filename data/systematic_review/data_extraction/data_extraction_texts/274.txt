{
	"Basic Identification": {
		"author (作者)": "Ronja Schiller, Johanna Fleckenstein, Lars Hoft, Andrea Horbach, Jennifer Meyer",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Germany (federal state of Schleswig-Holstein)",
		"journal_name (期刊名称)": "Computers & Education",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; classroom experiment with LLM-generated automated feedback on EFL writing",
		"doi_or_identifier (DOI或唯一标识)": "10.1016/j.compedu.2025.105386",
		"research_aims (研究目的与问题)": "To investigate the effectiveness of LLM-generated automated feedback on EFL learners writing from a process-oriented perspective; to examine whether behavioral and pausing-based indicators of learning engagement during text revision mediate the effects of automated feedback on revision and transfer writing performance; to replicate previous findings using offline log-based engagement measures and extend them with fine-grained keystroke logging measures.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses the lack of process-oriented evidence on how automated feedback works by focusing on learners task-level engagement during revision; extends prior work based on traditional AWE by using feedback generated by a large language model (GPT 3.5 Turbo) with upper secondary EFL learners; introduces online keystroke logging measures (keystrokes, typing time, pausing behavior) as additional indicators of behavioral and cognitive learning engagement; compares offline versus online engagement measures as mediators of automated feedback effects on revision and transfer performance."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "10th grade upper secondary school students",
		"language_proficiency (语言熟练度水平)": "English-as-a-foreign-language learners in Grade 10; performance evaluated via TOEFL iBT-like writing tasks; no standardized proficiency level labels reported.",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "57% female; remaining 43% male (overall sample).",
		"age (年龄)": "Mean age 16.11 years (SD 1.26).",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL classroom context in German upper secondary schools.",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "General upper secondary education; English as a foreign language classes.",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Computer-based classroom experiment with a between-group design (automated feedback vs no feedback) and within-session repeated writing and revision tasks; process data used for mediation analyses.",
		"research_method (研究方法_定量_定性_混合)": "Quantitative, process-oriented study using log data and keystroke logging for mediation models; no qualitative data collection reported.",
		"sampling_method (抽样方法)": "Schools in Schleswig-Holstein were contacted and volunteered; within participating schools, learners with parental consent took part during regular class time; participants were randomly assigned within classes to feedback or control condition.",
		"sample_size_and_effect (样本量及效应量)": "Initial N = 552 learners; 93 excluded due to technical difficulties and 6 excluded for not completing the first writing assignment, resulting in final N = 453 Grade 10 EFL learners (feedback n = 199, control n = 254). Automated feedback significantly increased behavioral engagement indicators during revision (e.g. more keystrokes, longer typing time, higher edit distance, longer total time on task) with small to medium effects (e.g. d approximately 0.30–0.55 for engagement variables); revision performance was slightly higher in the feedback group (d approximately 0.19); there was no significant total effect on transfer performance (d approximately 0.13). Mediation models showed significant indirect effects of automated feedback on revision and transfer performance through behavioral engagement indicators; model R2 for revision performance ranged from 0.46 to 0.49 depending on the engagement indicator included.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in feedback theories emphasizing the learner’s active role and task-level engagement (e.g. feedback as information about current and target states and how to close the gap; learning engagement as affective, cognitive, behavioral dimensions); builds on process-oriented feedback research and learning analytics using process data, and on writing research that interprets pauses as potential indicators of planning and reflection.",
		"data_collection_instrument (数据收集工具)": "Two TOEFL iBT-like argumentative writing prompts; LimeSurvey 3-based online environment; tablets and external keyboards; LLM-generated feedback messages (GPT 3.5 Turbo) covering content, structure, and language; automated scoring algorithm for holistic text quality scores (0–5) based on linguistic features; keystroke logging script embedded in the survey tool to capture time-stamped key events and positions; log files from the survey tool to derive total time on revision page.",
		"data_collection_validity_reliability (工具信度与效度)": "The automated scoring model was trained and cross-validated on a large, expert-rated essay corpus using a specific scoring rubric, achieving a quadratically weighted kappa of 0.76 on a held-out test set; the TOEFL-based tasks are standard argumentative prompts familiar in the regional curriculum; LLM feedback prompts were designed according to principles of effective feedback (e.g. Hattie and Timperley) and tailored to upper secondary EFL learners; keystroke logging script was implemented directly in the survey tool to ensure complete capture of keystrokes; no separate reliability indices for process measures were reported.",
		"data_analysis_method (数据分析方法)": "Descriptive statistics and group comparisons (Wilcoxon rank-sum tests and independent-samples t-tests, with Box-Cox transformations applied to skewed variables) for keystroke-based and log-based engagement indicators and performance; bivariate correlations overall and by condition; multiple mediation analyses using structural equation modeling (lavaan in R) to test whether offline and online engagement measures mediate the effect of automated feedback on revision and transfer performance, controlling for initial writing performance and sociodemographic covariates; indirect effects and portions of mediation estimated via bootstrapping with 1000 samples; examination of variance explained (R2) in revision performance by models including different engagement indicators.",
		"unit_of_analysis (分析单位)": "Individual learner (per-student revision behavior and performance scores).",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Random assignment of learners within classes to feedback (LLM-generated automated feedback during revision) or control (no feedback) conditions via the survey tool.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Participants were Grade 10 EFL learners from a set of upper secondary schools in one German federal state; schools volunteered and classes participated during regular school hours; authors note that findings may not generalize beyond this population, and that age groups can differ in how much they benefit from automated feedback.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Text quality for the initial draft, revision, and transfer essay was scored automatically using a pre-existing algorithm trained on texts holistically rated by expert human raters with a standardized rubric; no new human raters were involved in this study; training and calibration of the original raters are described in prior work cited but not detailed here."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Single classroom session up to 90 minutes: 20 minutes for initial writing, unlimited time for revision, 20 minutes for the transfer task, plus time for questionnaires and summative evaluation.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "GPT 3.5 Turbo (large language model)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "GPT 3.5 Turbo integrated into the LimeSurvey 3-based environment; prompted to provide feedback on content, structure, and language in a tabular format with hints and examples appropriate for upper secondary EFL learners; specific API parameters or temperature settings not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "LLM feedback was embedded server-side in the online writing platform; students did not chat interactively but received a one-shot feedback message displayed above the revision text box while revising their draft.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A single, pre-defined prompt for each essay instructed GPT 3.5 Turbo to generate feedback on three aspects (content, structure, language) using a table with separate columns for hints and examples of improvement; the prompt was designed following principles of effective feedback (informing about aspects needing improvement and how to address them) and tuned to the level of upper secondary EFL learners.",
		"training_support_llm_literacy (LLM素养与提示培训)": "NR",
		"intervention_implementation (干预实施流程_步骤与任务)": "Learners first wrote an argumentative essay (initial draft) in 20 minutes using tablets and keyboards within LimeSurvey. After a brief fixed waiting period, they moved to a revision page where they saw the original prompt and their draft. In the feedback condition, learners also saw an LLM-generated feedback message on content, structure, and language displayed above the text field; in the control condition, no feedback was shown. All learners were instructed to revise their text with no time limit, and all keystrokes during revision were logged. After submitting the revision, all learners wrote a second argumentative essay (transfer task) on a comparable TOEFL-based prompt in 20 minutes. At the end of the session, all learners received a summative holistic score (0–5) for their texts from an AWE scoring model, regardless of condition.",
		"experimental_group_intervention (实验组干预内容)": "Automated LLM feedback condition: learners revised their initial essay while viewing GPT 3.5 Turbo-generated feedback on content, structure, and language, presented in a structured table with hints and example improvements; feedback remained visible throughout revision.",
		"control_group_intervention (对照组干预内容)": "No-feedback condition: learners revised their initial essay under the same instructions and interface but without receiving any feedback; only the original task prompt and their draft were visible during revision.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Revision of an initial draft with or without automated feedback, followed by a new writing (transfer) task without feedback.",
		"writing_genre (写作体裁)": "Argumentative essays on TOEFL iBT-style prompts.",
		"writing_task_type (写作任务类型)": "Timed, computer-based argumentative writing tasks adapted from TOEFL iBT; first essay (initial draft and revision) plus a subsequent transfer essay.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "LLM served as an automated feedback generator providing task-specific formative feedback on learners essays (content, structure, language); it did not generate the essay text itself and was not used for scoring or grading.",
		"role_instructor (教师角色与介入方式)": "Teachers and researchers facilitated the session logistics, obtained consent, provided equipment and instructions, and ensured implementation during regular school hours; they did not provide writing feedback or scores and did not mediate the LLM feedback content.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Face-to-face classroom sessions in upper secondary schools in Schleswig-Holstein, Germany; tasks delivered through an online survey tool (LimeSurvey 3) on tablets with keyboards during regular EFL class hours.",
		"ethical_consideration (伦理审查与知情同意)": "Approved by the Ministry of Education in Schleswig-Holstein and by an independent ethics committee at the Leibniz Institute for Science and Mathematics Education; parents or guardians provided consent; study conducted during regular school hours without incentives; data from learners with technical problems or incomplete initial tasks were excluded.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Only learners randomly assigned to the feedback condition received LLM-generated feedback during revision; control learners did not; all learners later received AWE-based summative evaluation; external or unsupervised LLM use was not part of the protocol.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "No specific technical safety filters or guardrails are described; authors acknowledge that LLM feedback quality was not independently checked and note possible hallucinations or inaccuracies as limitations.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Compared to revising without feedback, receiving GPT 3.5 Turbo feedback led learners to invest more behavioral effort in revision, reflected in higher normalized edit distance, more keystrokes, longer typing time, and longer total time on the revision page. Revision performance was slightly higher in the feedback condition, and mediation models showed that the positive effect of automated feedback on revision performance was fully mediated by behavioral engagement indicators. Behavioral engagement during revision also positively predicted performance on a subsequent transfer essay, and automated feedback had significant indirect effects on transfer performance via engagement despite a non-significant total effect. Keystroke-based pausing measures revealed that learners with feedback made proportionally more but shorter pauses than controls, yet pausing variables did not significantly mediate feedback effects on revision or transfer performance. Overall, the study supports a process-oriented view in which LLM-generated feedback primarily enhances writing outcomes by increasing learners behavioral engagement during text revision rather than through the measured pausing-based cognitive engagement."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Automated LLM feedback was effective in increasing learners behavioral engagement during text revision and, via this engagement, improving revision performance and supporting transfer performance. Effectiveness was evaluated using automated holistic writing scores (0–5) for initial drafts, revisions, and transfer texts, and a range of process measures (edit distance, total time on task, total keystrokes, typing time, and pausing measures) derived from log files and keystroke logging. Automated feedback had a small positive total effect on revision performance, fully mediated by behavioral engagement, and no significant total effect on transfer performance, though indirect effects via engagement were significant.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Automated scoring model assigning holistic scores from 0 (low quality) to 5 (high quality) to each text (initial draft, revision, transfer), trained on a large corpus of expert-rated essays using a rubric covering organization, ideas, and language accuracy and complexity; the model uses linguistic features such as lexical, structural, and complexity attributes and achieved a quadratically weighted kappa of 0.76 on a held-out test set.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Overall argumentative essay quality including organization, provision and development of ideas, and accurate, appropriate use of English language features (lexical, structural, and complexity-based attributes).",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "NR",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "NA",
		"cognitive_aspect_measure (认知因素测量工具)": "Keystroke-based pausing variables during revision (normalized total pausing time, mean pause length, normalized number of pauses) were used as potential indicators of cognitive processes such as planning and reflection during text revision.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Potential cognitive engagement during revision operationalized via writing pauses, interpreted as possible phases of planning, reflecting on content and structure, or investing mental effort in text improvement, while acknowledging that pauses may also reflect off-task behavior and that cognitive processes can occur during active typing.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Offline behavioral engagement indicators derived from log data: normalized edit distance between initial and revised drafts, total time spent on the revision page; online behavioral engagement indicators from keystroke logging: total number of keystrokes and typing time between first and last keystroke during revision.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Task-level behavioral engagement during text revision, including the extent of active changes made to the text, persistence and effort as reflected in the number of keystrokes and typing time, and total time invested in the revision activity.",
		"other_outcomes_measure (其他结果测量工具)": "Sociodemographic covariates (age, gender, last English grade, parental education, number of books at home) used as predictors and controls; no additional outcome instruments beyond automated scores and process measures.",
		"other_outcomes_focus (其他结果维度说明)": "Associations between sociodemographic background and performance, and how these covariates relate to engagement and outcomes; comparison of offline and online engagement indicators regarding how much variance they explain in revision performance.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Initial writing task (baseline performance), revision of the initial task (post-feedback performance on same task), and subsequent writing task in the same session serving as a transfer task (performance on a new but similar prompt); no delayed post-test or longitudinal follow-up.",
		"primary_outcome_variables (主要结果变量_因变量)": "Holistic automated scores (0–5) for revision performance and transfer performance; initial text quality served as baseline covariate.",
		"independent_variables_and_factors (自变量与实验因素)": "Primary experimental factor: condition (LLM-generated automated feedback vs no feedback) during text revision; mediators: offline and online behavioral engagement indicators (edit distance, total time on task, total keystrokes, typing time) and pausing variables; covariates: initial writing score, age, gender, last English grade, parental education, number of books at home, and task order.",
		"followup_length_and_type (随访时长与类型)": "No follow-up beyond the in-session transfer writing task; no longitudinal follow-up across weeks or months.",
		"statistical_significance (统计显著性结果摘要)": "Learners in the feedback condition showed significantly higher behavioral engagement during revision than controls: more total keystrokes, longer typing time, greater normalized edit distance, and longer total time on the revision page (all p values less than .01, with small to medium effect sizes). Mean pause length was significantly shorter and normalized number of pauses significantly higher in the feedback group, while normalized total pausing time did not differ significantly between conditions. Revision performance was significantly higher in the feedback condition with a small effect, whereas transfer performance did not differ significantly between conditions. Mediation analyses showed that for revision performance, the positive effect of feedback was fully mediated by each behavioral engagement indicator (significant indirect effects, non-significant direct effects when mediators included). For transfer performance, total effects of feedback were not significant, but indirect effects via behavioral engagement measures were significant; no significant mediation effects were found for pausing variables in either revision or transfer models.",
		"effect_size_summary (效应量摘要)": "Group differences in engagement variables showed small to medium standardized mean differences (e.g. Cohen’s d approximately 0.51 for total keystrokes, 0.41 for typing time, 0.30 for normalized edit distance, 0.55 for total time on revision page, 0.37 for mean pause length, 0.32 for normalized number of pauses). The difference in revision performance between feedback and control conditions was small (d approximately 0.19), and the difference in transfer performance was smaller and non-significant (d approximately 0.13). Mediation models explained about 46–49 percent of the variance in revision performance, depending on whether edit distance, total time on task, total keystrokes, or typing time was used as the behavioral engagement mediator; indirect effects via behavioral engagement were statistically significant, with estimated portions of mediation ranging from about 0.42 to 0.88 for revision outcomes.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study did not report direct misuse of the LLM by students but identified several potential negative aspects and limitations: the quality and reliability of GPT 3.5 Turbo feedback were not independently evaluated, and the authors note that LLMs can hallucinate and provide incorrect information; the automated scoring and feedback tools may contain measurement errors; writing pauses and time-based indicators may partly reflect off-task behavior; and LLM-generated feedback has been found in other work to be inferior to expert feedback, raising concerns about overreliance on imperfect automated feedback.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "Analyses controlled for sociodemographic variables (age, gender, last English grade, parental education, books at home), and correlations between these covariates, engagement, and performance were reported, but no focused subgroup or fairness analyses were conducted. Learners in feedback and control conditions did not differ significantly in initial performance or sociodemographic characteristics, supporting baseline equivalence, but equity-related issues such as differential benefits across subgroups were not investigated in depth.",
		"limitation (研究局限)": "Limitations include potential measurement error and limited granularity in automatically scored text quality (single holistic score; possible scoring inaccuracies); the LLM feedback quality was not directly evaluated, so hallucinations and mismatches between feedback and text may have occurred; cognitive engagement operationalized via writing pauses may not capture the full complexity of cognitive processes during revision, and pause thresholds were derived from the initial writing task rather than a neutral copy task; pausing measures and time-based indicators are difficult to interpret, as they may reflect off-task behavior; behavioral and cognitive engagement are conceptually hard to separate based solely on keystroke data; the study used one age group (Grade 10) and one context, limiting generalizability; and engagement was measured only quantitatively, without multimodal or qualitative triangulation.",
		"challenge (实施挑战与风险)": "Challenges included ensuring accurate and interpretable process measures from keystroke logging and log data, conceptual difficulties in distinguishing behavioral and cognitive engagement, uncertainty about the quality and potential hallucinations in GPT 3.5 Turbo feedback, technical issues that led to the exclusion of some learners, and the inherent ambiguity of time-based measures and pauses which can conflate on-task thinking with off-task behavior; the study also highlights the need for multimodal data and more nuanced methods to interpret process measures in LLM-supported writing contexts.",
		"future_work (未来研究方向)": "Future studies should combine keystroke logging with other data sources (e.g. eye tracking, think-aloud protocols, self-report engagement measures) to better interpret behavioral and cognitive engagement; include copy tasks to establish baseline typing skills for pause thresholds; differentiate feedback aspects and examine how engagement with specific feedback components relates to performance; evaluate the quality, fit, and reliability of LLM-generated feedback in detail; investigate long-term transfer of feedback effects using longitudinal designs; explore different populations, writing genres, and age groups; analyze writing patterns and individual differences in engagement more deeply; and develop automated feedback systems that adapt to learners needs and maximize productive engagement.",
		"implication (理论与教学实践启示)": "The study underscores that LLM-generated automated feedback can enhance writing outcomes primarily by increasing learners behavioral engagement during revision rather than through measurable pausing-based cognitive engagement. For theory, it supports a process-oriented feedback framework in which task-level engagement mediates feedback effectiveness and highlights the need for well-theorized operationalizations of engagement dimensions. For practice, it suggests that designing automated feedback (including LLM-based feedback) to invite active revision effort and sustained engagement is critical, and that educators should monitor how students interact with feedback rather than only looking at final scores; integrating keystroke logging and other process analytics can help teachers and researchers understand and support effective engagement in L2 writing tasks."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Funded by the German Federal Ministry of Education and Research (grant number 01JG2104); Jennifer Meyer additionally supported by a Jacobs Foundation Research Fellowship (2024–2026).",
		"conflict_of_interest (利益冲突声明)": "Authors report no conflicts of interest.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Learners were randomly assigned within classes to feedback or control conditions, reducing selection bias; large sample size and classroom-based implementation further support internal validity; however, some participants were excluded due to technical problems, which may introduce some bias if exclusions were not random with respect to engagement or performance.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Participants could not be blinded to condition because only the feedback group saw the LLM feedback; however, text scoring was fully automated by an algorithm that did not depend on condition, effectively blinding outcome assessment to group membership.",
		"attrition_and_missing_data (流失与缺失数据处理)": "Of 552 initial participants, 93 were excluded due to technical difficulties and 6 for not completing the first writing assignment; the final analytic sample was 453 learners; no further attrition during the session was reported; missing data handling beyond these exclusions was not discussed.",
		"reporting_transparency (报告透明度与可重复性)": "The article reports detailed descriptions of tasks, procedures, measures, sample characteristics, and statistical methods; full model specifications, transformation procedures, and covariates are described; supplementary materials with additional tables are referenced; data and materials are made available on OSF for independent validation.",
		"preregistration_or_protocol (预注册或研究方案)": "The study was not preregistered.",
		"llm_version_reproducibility (LLM版本与可复现性)": "The LLM is specified as GPT 3.5 Turbo and the feedback prompt, including tabular structure and content, is described conceptually with exact prompt text available in a related publication; however, LLM behavior may change over time with updates, and implementation details such as API settings are not fully documented, which may limit exact reproducibility of feedback outputs.",
		"baseline_equivalence (基线等同性检验)": "Learners in feedback and control conditions did not differ significantly in initial writing performance or in sociodemographic covariates such as age, gender, English grade, parental education, or books at home, as reported in supplementary analyses, supporting baseline equivalence between groups.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Distributions of engagement variables were inspected and found to be skewed; Box-Cox transformations were applied using optimal lambda values to approximate normality and stabilize variance; mediation analyses used bootstrapping to estimate indirect effects; no additional assumption checks (e.g. residual diagnostics, multicollinearity) are described.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "Beyond excluding learners with technical problems or incomplete initial tasks and using Box-Cox transformations for skewed variables, no specific outlier detection procedures or sensitivity analyses are reported.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Normalized edit distance was computed by dividing raw edit distance by text length; total pausing time and number of pauses were normalized by typing time; individual pause thresholds were based on each learner’s mean inter-keystroke interval plus one standard deviation from the initial writing task; several engagement variables (edit distance, total time on task, keystrokes, typing time, pausing measures) were transformed using Box-Cox transformations prior to parametric analyses."
	}
}