{
	"Basic Identification": {
		"author (作者)": "Zahra Shahsavar; Reza Kafipour; Laleh Khojasteh; Farhad Pakdel",
		"publication_year (发表年份)": "2024",
		"study_region (研究地区)": "Shiraz, Iran",
		"journal_name (期刊名称)": "Frontiers in Education",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article (original research)",
		"doi_or_identifier (DOI或唯一标识)": "10.3389/feduc.2024.1457744",
		"research_aims (研究目的与问题)": "To investigate whether using ChatGPT as a writing assistant enhances medical students’ English academic writing skills compared to conventional writing training, and to examine how ChatGPT affects different components of academic writing (content, organization, vocabulary, language use, mechanics).",
		"research_gap_or_novelty (研究创新性与知识空白)": "There is a lack of longitudinal intervention research measuring the actual impact of ChatGPT on L2 academic writing performance rather than perceptions, especially in medical education and humanities/social science contexts; this study applies an experimental pretest–posttest design to quantify ChatGPT’s effects on overall and component-level writing scores in an EFL medical university setting."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate junior medical students in a compulsory 3‑unit academic writing course",
		"language_proficiency (语言熟练度水平)": "Intermediate level English academic writing proficiency as indicated by pre-test scores on Jacobs et al. (1981) rubric; no standardized language test reported",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context at an Iranian medical university (English academic writing as a foreign language)",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Medical students at Shiraz University of Medical Sciences taking a compulsory academic writing course",
		"prior_experience_llm (既有LLM使用经验)": "Most students had prior experience using ChatGPT before the study, based on the first author’s observations."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Experimental longitudinal pretest–posttest design with an experimental group using ChatGPT and a control group receiving conventional writing instruction over a 17‑week course.",
		"research_method (研究方法_定量_定性_混合)": "Quantitative",
		"sampling_method (抽样方法)": "Convenience sampling of medical students enrolled in the compulsory 3‑unit academic writing course, followed by random separation of 83 consenting students into experimental (n=42, two writing classes) and control (n=41, two writing classes) groups; analyses of writing scores were based on 60 students who completed all assignments.",
		"sample_size_and_effect (样本量及效应量)": "Initial N=83 (experimental group n=42, control group n=41); 9 students withdrew and 14 failed to submit all assignments, so 60 students’ writing (control n=30, experimental n=30) were analyzed. Independent samples t-test on post-test overall writing scores showed that the experimental group (M=89.20, SD=5.82) significantly outperformed the control group (M=79.50, SD=5.50), t(58)=6.63, p=0.001. Paired t-tests indicated significant pre–post improvement in both the control group (M=68.33→79.50, t(29)=4.73, p=0.001) and the experimental group (M=82.07→89.20, t(29)=4.90, p=0.001). ANCOVA on writing components showed that the experimental group scored significantly higher than the control group on content, organization, vocabulary, and mechanics in the post-test, with effect sizes interpreted as large for content and medium for organization, vocabulary, and mechanics.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in Elhossiny et al. (2022) theoretical framework integrating psychology, thinking style, and technology in AI-assisted writing, combined with AI-assisted learning literature and a process-oriented approach to teaching writing.",
		"data_collection_instrument (数据收集工具)": "Pre-test and post-test timed academic writing tasks on health-related topics (around 200 words in 30 minutes under exam conditions); Jacobs et al. (1981) analytic writing rubric with five components (content, organization, language use, vocabulary, mechanics) to score each essay; course assignments in the same genres; no separate attitude questionnaire was reported.",
		"data_collection_validity_reliability (工具信度与效度)": "Jacobs et al. (1981) rubric was selected as a comprehensive and widely used analytic rubric with demonstrated validity and reliability in evaluating students’ writing across educational settings; two researchers (ZSH and FP) calibrated their scoring on a set of essays, discussed discrepancies to agree on standards, then scored independently, yielding inter-rater reliability of 87%, which was interpreted as highly reliable.",
		"data_analysis_method (数据分析方法)": "Kolmogorov–Smirnov tests to check normality of scores (non-significant), followed by parametric analyses: independent samples t-test to compare post-test overall writing scores between groups; paired samples t-tests to compare pre- and post-test writing scores within each group; ANCOVA to compare post-test component scores between groups controlling for pre-test; effect sizes calculated and interpreted using eta squared, partial eta squared, Cohen’s d, and r based on Cohen’s (1992) guidelines.",
		"unit_of_analysis (分析单位)": "Individual student essay scores (overall and component scores) at pre-test and post-test.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "After informed consent, 83 students were randomly separated into the experimental group (two writing classes, n=42) and the control group (two writing classes, n=41); no details on the randomization procedure or allocation concealment are reported.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame was all medical students enrolled in the compulsory 3‑unit academic writing course at Shiraz University of Medical Sciences; participants were selected for ease of access (convenience sampling), so the sample is not claimed to be statistically representative of medical students more broadly.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Two researchers (ZSH and FP) independently scored essays using Jacobs et al. rubric; they first scored a subset of essays together, compared and discussed score differences to standardize their interpretations, then proceeded to evaluate the remaining essays independently; inter-rater reliability of 87% was calculated, indicating high consistency between raters."
	},
	"Intervention": {
		"duration (干预时长与频率)": "A 17‑week English academic writing course with two sessions per week; students in both groups wrote one draft essay per week, with pre-test at the beginning and post-test immediately after the 17‑week intervention.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (OpenAI; specific model/version not reported)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT was accessed through its standard dialog box interface where students typed their essays and an instruction prompt; the platform (web vs app), API use, parameter settings, and exact version beyond being ChatGPT are not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "In the experimental group, students directly interacted with ChatGPT outside class by pasting their self-written essays into the ChatGPT dialog box and receiving proofread-and-edit feedback; ChatGPT was used alongside a process-oriented writing syllabus and not embedded into a learning management system.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Students in the experimental group used a standardized instruction prompt when interacting with ChatGPT: Proofread and edit the text by providing reasons for every single edition. They submitted their entire essay text with this instruction and then reviewed ChatGPT’s edits and explanations; no additional prompt engineering techniques (e.g., few-shot prompting, role prompting, chain-of-thought) were described.",
		"training_support_llm_literacy (LLM素养与提示培训)": "Students in the experimental group were trained on how to use ChatGPT and how to formulate instructions to obtain the best results; they were required to submit before-and-after snapshots of their essays (original and ChatGPT-edited versions) and to provide either Persian or English explanations for each ChatGPT suggestion, enabling instructors to check understanding and encourage active, critical engagement with AI feedback; broader topics such as plagiarism, ethics, and AI limitations were discussed in the literature review, but no formal multi-session LLM literacy curriculum was described.",
		"intervention_implementation (干预实施流程_步骤与任务)": "At pre-test, all students wrote a health-related academic essay of about 200 words in 30 minutes under exam conditions. During the 17‑week course, both groups received process-oriented writing instruction with emphasis on grammar, vocabulary, and model texts. Control group students wrote one draft per week and received traditional written teacher feedback focusing on lexical and grammatical aspects; they then revised their essays based on this feedback. Experimental group students wrote their weekly drafts in class under instructor observation, then typed their essays at home into ChatGPT with the prompt Proofread and edit the text by providing reasons for every single edition, received AI feedback and edits, and produced a revised version; they also wrote explanations for each ChatGPT suggestion in Persian or English, which instructors reviewed to ensure comprehension and appropriate application of the feedback. Biweekly pre-class training sessions were held with the two experimental-group instructors to ensure consistent procedures. At post-test, all students again wrote a 200-word health-related essay in 30 minutes under exam conditions, without ChatGPT.",
		"experimental_group_intervention (实验组干预内容)": "Experimental group (two writing classes) followed the regular process-oriented writing syllabus plus structured ChatGPT use: drafting essays in class, then submitting texts to ChatGPT with a standardized proofreading-and-editing prompt, receiving reasoned edits, explaining each AI suggestion, and revising their writing accordingly for 17 weeks.",
		"control_group_intervention (对照组干预内容)": "Control group (two writing classes) received conventional writing instruction using the same process-oriented approach, with emphasis on grammar accuracy, vocabulary breadth, and model texts; students wrote weekly drafts and received teacher-written feedback on lexical and grammatical aspects, then revised their essays based on this feedback; no ChatGPT or other AI tools were used as part of the control-group instruction.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "The intervention targeted mainly the revision and editing stages of writing after an initial draft, providing feedback and supporting rewriting; the repeated cycle across 17 weeks also influenced planning and drafting over time.",
		"writing_genre (写作体裁)": "Academic essays on health-related topics in genres including argumentative, cause-and-effect, problem-solution, and analytical essays.",
		"writing_task_type (写作任务类型)": "Timed in-class and exam essays of around 200 words on health-related topics, plus weekly course assignments in the same essay genres.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT functioned as a writing assistant and automated feedback provider: it proofread and edited students’ essays, suggested revisions, and provided reasons for each change; it was not instructed to generate complete essays from scratch or to assign numerical scores.",
		"role_instructor (教师角色与介入方式)": "Writing instructors (with PhDs in English language education and over 15 years of experience) taught both groups using a process-oriented approach; in the experimental group they additionally trained students to use ChatGPT, monitored in-class drafting, required and reviewed students’ explanations of ChatGPT suggestions, and ensured appropriate use of AI feedback; in the control group the instructor provided traditional written feedback and followed the usual syllabus without AI support.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Shiraz University of Medical Sciences, Iran; compulsory 3‑unit English academic writing course for junior medical students; face-to-face instruction over a 17‑week semester.",
		"ethical_consideration (伦理审查与知情同意)": "The study was approved by the Ethics Committee of Shiraz University of Medical Sciences (ethics code IR.SUMS.REC.1402.616); written informed consent was obtained from all participating students.",
		"llm_access_policy (LLM使用规范_允许与限制)": "The university did not officially endorse the incorporation of ChatGPT, and some instructors expressed concerns about plagiarism and originality of students’ essays; ChatGPT use was restricted to the experimental classes as part of the research, pre- and post-test essays were written under exam conditions without AI assistance, and students in the experimental group were required to submit both original and ChatGPT-edited versions plus written explanations of AI suggestions.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "Safety and integrity measures included instructor oversight of ChatGPT use, mandatory before-and-after essay snapshots, requiring students to explain each ChatGPT suggestion to discourage blind copying, and emphasizing ChatGPT as a supplementary tool rather than a replacement for students’ own writing; no technical content filters or automated misuse detection mechanisms were described.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Using ChatGPT as a writing assistant led to significantly larger gains in overall English academic writing scores for medical students compared to conventional instruction alone, with the experimental group improving from M=82.07 to 89.20 versus the control group from M=68.33 to 79.50 and significantly higher post-test performance in the ChatGPT group. ANCOVA showed that ChatGPT use significantly enhanced content, organization, vocabulary, and mechanics components relative to the control group, with a particularly large effect on content and medium effects on organization, vocabulary, and mechanics, while the improvement in language use was smaller and not statistically significant. Within the experimental group, paired t-tests showed significant pre–post gains in content, organization, and mechanics but not in vocabulary and language use, indicating that ChatGPT was especially effective for idea development, structure, and surface mechanics but less so for deeper language-use features. The authors conclude that AI tools like ChatGPT can be valuable in supporting specific aspects of academic writing but should not be treated as a one-size-fits-all solution and need to be complemented by human guidance."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of ChatGPT as a writing assistant was evaluated using pre- and post-test essays scored with Jacobs et al. (1981) rubric for overall and component writing quality; results indicated that the ChatGPT-assisted experimental group showed significantly greater improvement in overall writing performance and in several components (content, organization, mechanics) than the control group, with large and medium effect sizes, while both groups improved significantly over time.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Jacobs et al. (1981) analytic writing rubric with five weighted components: content (25 points), organization (25 points), language use (25 points), vocabulary (15 points), and mechanics (10 points), total score 100.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Overall English academic writing quality and five specific dimensions: content (development and relevance), organization (coherence and logical sequencing), vocabulary (range and appropriateness), language use (grammatical accuracy and complexity), and mechanics (spelling, punctuation, capitalization, paragraphing).",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "NA",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "NA",
		"cognitive_aspect_measure (认知因素测量工具)": "NA",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NA",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "NA",
		"other_outcomes_focus (其他结果维度说明)": "NA",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pre-test at the beginning of the 17‑week course and post-test immediately after the 17‑week ChatGPT-supported or conventional writing instruction; no delayed follow-up test reported.",
		"primary_outcome_variables (主要结果变量_因变量)": "Overall writing score (0–100) and component scores for content, organization, vocabulary, language use, and mechanics on Jacobs et al. (1981) rubric.",
		"independent_variables_and_factors (自变量与实验因素)": "Instructional condition (ChatGPT-assisted experimental group vs conventional control group) and time (pre-test vs post-test), with writing components (content, organization, vocabulary, language use, mechanics) analyzed as separate dependent variables in ANCOVA and paired t-tests.",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Independent samples t-test showed a significant difference in post-test overall writing scores between groups, with the experimental group (M=89.20, SD=5.82) outperforming the control group (M=79.50, SD=5.50), t(58)=6.63, p=0.001. Within-group paired t-tests revealed significant pre–post improvements in overall writing scores for both the control group (M=68.33→79.50, t(29)=4.73, p=0.001) and the experimental group (M=82.07→89.20, t(29)=4.90, p=0.001). ANCOVA comparing post-test component scores, controlling for pre-test, indicated that the experimental group scored significantly higher than the control group on content, organization, vocabulary, and mechanics (p=0.00 for these components) but not significantly higher on language use (p=0.09). Within the experimental group, paired t-tests on components showed significant gains in content (t(29)=5.99, p=0.01), organization (t(29)=4.22, p=0.01), and mechanics (t(29)=2.53, p=0.01), but non-significant changes in vocabulary (t(29)=1.59, p=0.12) and language use (t(29)=1.73, p=0.09).",
		"effect_size_summary (效应量摘要)": "Based on ANCOVA eta squared values, the content component showed a large effect size (η²≈0.85) favoring the ChatGPT group; organization, vocabulary, and mechanics showed medium effect sizes (η²≈0.78, 0.71, and 0.69, respectively), while language use exhibited a small effect size (η²≈0.28) and did not reach statistical significance. The overall pre–post improvement in the experimental group is described as having a large effect, but specific Cohen’s d values for t-tests are not numerically reported.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study’s literature review and discussion acknowledge potential risks of ChatGPT and similar AI tools, including fairness issues, copyright and academic integrity concerns, difficulties in identifying AI-generated text, limited contextual understanding and topic comprehension, and the possibility that over-reliance on AI may hinder development of higher-order writing skills such as critical thinking, argumentation, coherence, and human-like intuition; within the reported intervention, no direct instances of plagiarism or misuse are documented, but the authors emphasize that ChatGPT’s impact on language use was limited and that AI tools should be used as supplementary resources with human oversight rather than as complete replacements for traditional instruction.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "NR",
		"limitation (研究局限)": "Limitations include reliance on convenience sampling at a single medical university, which restricts generalizability; use of a relatively small analytic sample (N=60) after attrition and incomplete data; limited number of essays practiced within the course; focus on specific academic essay genres (argumentative, cause-and-effect, problem-solution, analytical) rather than a broader range of writing types; potential bias introduced by the dual role of instructor-researcher; lack of significant improvement in language use and vocabulary, suggesting that ChatGPT may be less effective for certain higher-order or nuanced aspects of writing; and the fact that AI tools may not fully address critical thinking and argumentation skills, highlighting the need for continued human instruction.",
		"challenge (实施挑战与风险)": "Implementation challenges and risks discussed include the dual role of instructors as researchers potentially affecting objectivity, concerns among faculty about plagiarism and originality when students use ChatGPT, the difficulty of ensuring that students truly understand and critically evaluate AI feedback rather than copying blindly, ChatGPT’s limited capacity to improve higher-order language use and argumentation, potential unreliability when ChatGPT’s topic comprehension is paired with students’ limited knowledge, and the need to balance AI support with human guidance to avoid over-reliance on AI tools.",
		"future_work (未来研究方向)": "The authors recommend future studies using random sampling to increase generalizability; increasing the number and variety of essays practiced; examining ChatGPT’s role in additional types of writing such as journalistic writing, article writing, thesis writing, and creative writing; addressing the challenges of AI’s limited contextual understanding, domain-specific knowledge, and human-like intuition; exploring ways to combine or integrate different AI tools with traditional teaching methods to optimize language learning and writing improvement; and investigating why language use did not significantly improve and how AI and human feedback can be better aligned to enhance this component.",
		"implication (理论与教学实践启示)": "The findings suggest that ChatGPT can play a valuable supportive role in medical students’ academic writing by significantly enhancing content development, organization, vocabulary, and mechanics when integrated into a process-oriented writing course, reinforcing Elhossiny et al.’s framework of combining psychology, thinking style, and technology; however, its limited effect on language use underscores that AI should complement rather than replace human instruction, and that the most effective approach may be a hybrid model in which AI provides personalized, immediate feedback while instructors guide higher-order aspects of writing, critical thinking, and academic integrity."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "This work received financial support from the Shiraz University of Medical Sciences Research Council, grant number 29649.",
		"conflict_of_interest (利益冲突声明)": "The authors state that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Students were initially selected through convenience sampling from a single medical university and then randomly separated into experimental and control groups, but details about the randomization procedure, allocation concealment, or stratification are not provided; attrition reduced the sample from 83 to 60 analyzed cases, which may introduce additional bias.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Blinding of participants and writing instructors was not possible due to the nature of the intervention, and the study does not report whether essay raters were blinded to group assignment or test time, leaving potential risk of performance and detection bias.",
		"attrition_and_missing_data (流失与缺失数据处理)": "Of 83 students who initially consented, 9 withdrew from the course and 14 failed to submit all assignments, resulting in 60 students included in the analysis (30 per group); incomplete cases were excluded, and no detailed analysis of differential attrition or missing data mechanisms is reported.",
		"reporting_transparency (报告透明度与可重复性)": "The article clearly reports research questions, theoretical framework, context, participant numbers, group assignment, course duration, teaching procedures, ChatGPT use, writing tasks, scoring rubric, rater reliability, and statistical methods and provides tables with means, standard deviations, t-values, F-values, p-values, and eta squared for key outcomes; raw data are not included but the authors state that raw data will be made available on request, supporting partial reproducibility.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study identifies the tool as ChatGPT but does not specify the underlying model version (e.g., GPT‑3.5 or GPT‑4), configuration parameters, or time window of use; given that ChatGPT is regularly updated, reproducing the exact AI behavior used in this study may not be fully possible.",
		"baseline_equivalence (基线等同性检验)": "Homogeneity tests using ANCOVA on pre-test writing scores showed no statistically significant differences between experimental and control groups for overall writing and each component, and pre-test scores indicated both groups were at an intermediate writing proficiency level, supporting baseline equivalence.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Kolmogorov–Smirnov tests indicated that score distributions did not significantly deviate from normality, justifying the use of parametric tests; additional ANCOVA assumptions such as homogeneity of variance and homogeneity of regression slopes are not explicitly reported.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Writing scores were derived directly from Jacobs et al. rubric ratings and averaged across two raters after calibration and reliability checks; no data transformations or additional preprocessing steps are described before conducting t-tests and ANCOVA."
	}
}