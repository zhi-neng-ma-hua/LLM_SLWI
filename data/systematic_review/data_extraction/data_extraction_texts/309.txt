{
	"Basic Identification": {
		"author (作者)": "Fawaz Al Mahmud",
		"publication_year (发表年份)": "2026",
		"study_region (研究地区)": "Saudi Arabia (University of Jeddah)",
		"journal_name (期刊名称)": "World Journal of English Language",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; explanatory sequential mixed-methods quasi-experimental study",
		"doi_or_identifier (DOI或唯一标识)": "10.5430/wjel.v16n2p258",
		"research_aims (研究目的与问题)": "To compare the affordances of ChatGPT written corrective feedback and human-derived corrective feedback in fostering grammatical accuracy in Saudi EFL students writing and to examine (1) how ChatGPT and human feedback compare in effectiveness for improving grammatical accuracy, (2) how ChatGPT and human feedback compare in accuracy of error-flagging, and (3) how Saudi EFL learners perceive the efficacy of ChatGPT as a written corrective feedback tool.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses gaps in automated written corrective feedback research by simultaneously examining ChatGPT impacts on linguistic accuracy and ChatGPT own error-flagging accuracy, and by focusing on ChatGPT as a written corrective feedback source for Saudi EFL learners, a context with few prior empirical studies comparing ChatGPT and human feedback."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate EFL learners enrolled in an academic writing course at the University of Jeddah",
		"language_proficiency (语言熟练度水平)": "NR",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in Saudi higher education",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "English language and translation; academic writing course for Saudi EFL undergraduates",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Explanatory sequential mixed-methods design with a quasi-experimental pretest–posttest control group comparison followed by qualitative analyses (error analysis and semi-structured interviews).",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods (quantitative and qualitative)",
		"sampling_method (抽样方法)": "Convenience sampling of 53 Saudi undergraduate EFL learners in an academic writing course, divided into experimental and control groups; procedure for group allocation beyond division into experimental and control groups not reported.",
		"sample_size_and_effect (样本量及效应量)": "Total N=53 Saudi undergraduate EFL learners; experimental group (ChatGPT WCF) n=26, control group (human WCF) n=27. Pretest writing skills scores showed no significant between-group difference: experimental M=54.61, SD=13.10; control M=52.77, SD=11.10; t=0.549, p=0.586. Posttest writing skills scores favored the ChatGPT group: experimental M=66.15, SD=11.34; control M=56.48, SD=11.07; t=3.14, p=0.003, eta square η²=0.68 (indicating 68 percent of variance in writing skills associated with ChatGPT WCF condition). Error-flagging analysis across 53 essays (899 total surface errors) showed human feedback correctly identified 486 errors with 3 missed, ChatGPT correctly identified 403 errors with 7 missed, with false negatives under five percent for both sources.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in written corrective feedback theory emphasizing the role of WCF in promoting linguistic accuracy and L2 writing development, research on automated written corrective feedback and automated writing evaluation tools, and emerging literature on ChatGPT in EFL writing; also informed by prior findings that automated WCF can match or exceed human feedback for surface errors and by concerns about comprehensiveness, accuracy, and learner comprehension of automated feedback.",
		"data_collection_instrument (数据收集工具)": "Pretest and posttest short essay writing tasks (approximately 270–300 words) used to elicit participants compositions; ChatGPT and human evaluators as feedback sources during the 8-week intervention; formula for grammatical accuracy based on Soltanpour and Valizadeh (2018) calculating mean number of grammatical errors per 100 words; error classification into grammatical, punctuation, and lexical errors; semi-structured interview protocol with six experimental-group participants to explore perceptions of ChatGPT as a feedback tool.",
		"data_collection_validity_reliability (工具信度与效度)": "Grammatical accuracy in essays was quantified using a consistent formula ([total number of grammatical errors / total number of words] × 100) to produce scores out of 100. Four Saudi university teachers of English (each with a PhD and 5–8 years of EFL teaching experience plus 4–8 relevant publications) served as human evaluators for grammatical accuracy and corrective feedback. For the posttest, essays were assessed for grammatical accuracy by independent human raters using the same scoring procedure, with Huddleston and Pullum (2002) and Quirk et al. (1985) as reference grammars. An interrater reliability check was conducted on a subset of 12 essays (about 23 percent of the dataset), with two raters independently providing corrective feedback; agreement between them was used to assess consistency between ChatGPT and human evaluators, though specific reliability coefficients were not reported.",
		"data_analysis_method (数据分析方法)": "Quantitative data were analyzed in SPSS. Descriptive statistics (means and standard deviations) were calculated for pretest and posttest writing skills scores. Independent-samples t-tests were used to compare experimental and control groups at pretest and posttest, with p-values and eta squared effect size reported for posttest comparisons. Error analysis was used to compare the error-detection accuracy of ChatGPT and the human rater across lexical, grammatical, and punctuation errors. Qualitative data (ChatGPT and human feedback samples and semi-structured interviews) were analyzed via error analysis and thematic analysis: interview transcripts were read for overall understanding, then manually coded and organized into themes following qualitative coding procedures (e.g., Saldana, 2021).",
		"unit_of_analysis (分析单位)": "Individual student writing samples (essays) and their grammatical accuracy scores; individual surface errors as units in error analysis; individual participants as units in qualitative interview analysis.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Participants were divided into an experimental group (ChatGPT WCF, n=26) and a control group (human WCF, n=27); the article does not report whether assignment was random, so group assignment appears non-random with equality at baseline established through pretest t-test.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame comprised Saudi undergraduate EFL learners enrolled in an academic writing course at the University of Jeddah; sample size was relatively small (N=53) and drawn from one institution, limiting representativeness and generalisability to other Saudi or international EFL populations.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "For pretest and posttest writing skills scores, participants wrote short essays which were then scored for grammatical accuracy by human evaluators using the agreed formula based on number of grammatical errors per 100 words, yielding a score out of 100. Four Saudi university teachers of English with PhDs and substantial EFL teaching and research experience served as human evaluators; they agreed on the scoring procedure and consulted reference grammars (Huddleston and Pullum, Quirk et al.) for judging linguistic accuracy. For the error-detection accuracy analysis, one human rater and ChatGPT feedback were compared on a set of 53 essays, and an interrater reliability check was conducted on 12 essays rated by two human raters; the article notes their credentials but does not specify a formal rater training or calibration session beyond shared use of the scoring formula and grammatical references."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Eight-week intervention with two 1.5-hour writing sessions per week (16 sessions total) in which participants wrote essays and received corrective feedback once per session, followed by a ninth week posttest essay.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (transformer-based large language model by OpenAI; specific model version not reported)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT was used as an automated written corrective feedback tool; the article describes ChatGPT as an AI-based tool that can spot, elucidate, and fix surface errors and provide instant, comprehensive feedback, but does not report the specific version, interface (web UI or API), or parameter settings.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students in the experimental group interacted directly with ChatGPT during writing sessions, using it as a real-time source of written corrective feedback on their essays, with opportunities for dialogic follow-up on the AI feedback; ChatGPT functioned as a stand-alone tool rather than being embedded into an LMS.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Participants submitted their English compositions to ChatGPT and asked it to indicate what was wrong in grammar, word choice, and punctuation and to provide detailed lists of errors, explanations, and corrections; ChatGPT responses typically broke down errors by type (grammar, lexis, punctuation) and then revised erroneous elements, but the exact wording of prompts was not reported.",
		"training_support_llm_literacy (LLM素养与提示培训)": "NR (the article does not describe any explicit training sessions on AI literacy or prompt design for ChatGPT use, though it notes in recommendations that future implementations should train students and instructors to use AWCF tools such as ChatGPT).",
		"intervention_implementation (干预实施流程_步骤与任务)": "After being assigned to experimental (ChatGPT WCF) or control (human WCF) conditions, all 53 participants took a pretest by writing a short essay which was scored for grammatical accuracy. Over the subsequent eight weeks, both groups completed two 1.5-hour writing sessions per week. In each session, participants wrote a short essay in English and then received corrective feedback once per session according to their group: the experimental group obtained written corrective feedback from ChatGPT, while the control group received written corrective feedback from human evaluators. The cycle of writing, receiving feedback, and revising continued across all sessions, with ChatGPT able to provide immediate, interactive feedback and human evaluators providing more traditional written feedback. In the ninth week, all participants completed a posttest essay that was scored for grammatical accuracy by human raters using the same procedure as the pretest, and six experimental-group participants took part in semi-structured interviews about ChatGPT as a WCF tool.",
		"experimental_group_intervention (实验组干预内容)": "Experimental group (n=26) engaged in regular short-essay writing during two weekly sessions and received written corrective feedback from ChatGPT once per session. ChatGPT flagged surface errors (grammar, punctuation, word choice), explained what was wrong, and proposed corrections and revised sentences, with feedback delivered instantly and in an interactive dialogue format that allowed students to ask follow-up questions and obtain clarifications during sessions.",
		"control_group_intervention (对照组干预内容)": "Control group (n=27) engaged in the same writing schedule and tasks but received written corrective feedback from human evaluators once per session. Four Saudi university teachers of English with PhDs and relevant teaching and research experience provided human-derived corrective feedback on grammatical accuracy, identifying surface errors and suggesting corrections, although this feedback could not always match the speed or elaboration achievable with ChatGPT.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Drafting of short essays, receiving written corrective feedback on completed drafts, and revising texts in light of feedback across repeated writing cycles; the intervention focused on post-draft editing and revision for grammatical accuracy rather than initial idea generation or content planning.",
		"writing_genre (写作体裁)": "Short argumentative or opinion essays on topics such as whether phoning while driving should be banned; academic writing tasks focusing on surface accuracy rather than extended genre-specific rhetorical structures.",
		"writing_task_type (写作任务类型)": "Short in-class English essays of approximately 270–300 words on assigned prompts related to general argumentative topics; pretest and posttest were single short essays, and intermediate sessions likewise involved short-essay writing with feedback and revision.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT acted as an automated written corrective feedback provider: it detected surface errors, classified them (grammar, lexis, punctuation), explained why constructions were erroneous, suggested corrections and revised sentences, and engaged in interactive dialogue with learners about their errors and revisions.",
		"role_instructor (教师角色与介入方式)": "Instructors and human evaluators designed and administered the academic writing course and writing tasks, provided written corrective feedback to the control group, scored pretest and posttest essays for grammatical accuracy, and served as human comparison raters for error detection; they did not mediate ChatGPT feedback directly, though teachers and evaluators were used as benchmarks for evaluating AI feedback accuracy and as interview informants about the context.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Academic writing course for Saudi undergraduate EFL learners at the University of Jeddah; the mode of course delivery is classroom based, with writing sessions and feedback occurring as part of regular university instruction.",
		"ethical_consideration (伦理审查与知情同意)": "The article notes that informed consent was obtained from participants and states that ethics approval was granted by the Publication Ethics Committee of the journal publisher, with adherence to COPE core practices; it does not report a separate institutional review board approval number. Data are not publicly available due to privacy or ethical restrictions but can be requested from the author.",
		"llm_access_policy (LLM使用规范_允许与限制)": "ChatGPT was used as an automated written corrective feedback tool for experimental-group participants within the academic writing course; the study recommends that if AWCF tools like ChatGPT are brought into writing classrooms, both students and instructors should be trained in their use, but specific institutional policies or usage restrictions during the study are not detailed.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "NR (no explicit mention of safety filters, content moderation settings, or technical guardrails for ChatGPT use, though the discussion acknowledges general risks of AI tools and recommends training stakeholders to understand strengths and limitations of AI-powered automated written corrective feedback).",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Quantitative results showed that while experimental and control groups did not differ significantly at pretest, the experimental group receiving ChatGPT written corrective feedback significantly outperformed the control group receiving human feedback at posttest on writing skills scores based on grammatical accuracy, with a large effect size (eta squared 0.68). This indicates that ChatGPT WCF was a more effective feedback source for improving Saudi EFL learners grammatical accuracy than human-derived WCF in this study. Error analysis indicated that human feedback was slightly more accurate than ChatGPT in error-flagging (detecting more lexical and grammatical errors and missing fewer errors), but both sources had false negative rates under five percent, and ChatGPT was described as almost on par with humans in error detection. Qualitative findings from interviews with six experimental-group students showed overall positive perceptions of ChatGPT as a facilitator of writing accuracy, praising its immediate, detailed, and mostly clear explanations and corrections, though one interviewee felt that human explanations were sometimes clearer for nuanced grammatical points. The author concludes that ChatGPT can serve as a valuable supplementary or co-feedback provider in Saudi EFL writing, enhancing linguistic accuracy while human feedback remains important for contextual and higher-order issues."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Using pretest and posttest short essays scored for grammatical accuracy based on the mean number of grammatical errors per 100 words, the study found that ChatGPT written corrective feedback led to significantly greater improvement in writing skills scores than human corrective feedback, with a large effect size, indicating strong effectiveness of ChatGPT WCF in remediating surface grammatical errors in Saudi EFL students compositions.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Pretest and posttest short essays scored for grammatical accuracy using the formula [total number of grammatical errors / total number of words] × 100 to calculate errors per 100 words, converted into writing skills scores out of 100; grammatical accuracy judged with reference to Huddleston and Pullum and Quirk et al.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Linguistic accuracy at the surface level, particularly grammatical errors, with additional attention to punctuation and word choice errors; no explicit measures of fluency, complexity, or discourse-level organization were used as primary outcome variables.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Semi-structured interviews with six experimental-group participants to explore their perceptions and attitudes toward ChatGPT as a written corrective feedback tool; no standardized affective questionnaire was used.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Students perceptions of ChatGPT as a facilitator of writing accuracy, including perceived usefulness, clarity and detail of feedback, comparison with human feedback, and overall satisfaction or preference regarding AI-generated versus human-generated corrective feedback.",
		"cognitive_aspect_measure (认知因素测量工具)": "NA",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NA",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "Error analysis comparing error detection by ChatGPT and the human rater across 53 essays (classification of lexical, grammatical, and punctuation errors; counts of correctly identified and missed errors); interrater check on 12 essays by two human raters to assess reliability of ChatGPT and human WCF; thematic analysis of interview transcripts.",
		"other_outcomes_focus (其他结果维度说明)": "Accuracy of error detection by ChatGPT versus human feedback (numbers of lexical, grammatical, and punctuation errors correctly identified and missed, and false negative rates for both sources), as well as the qualitative strengths and weaknesses of AI and human feedback in providing explanations and supporting learners comprehension of errors.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pretest writing skills assessment before the 8-week intervention; multiple writing and feedback sessions during the 8-week period; posttest writing skills assessment in week 9 after the intervention; no delayed follow-up beyond the immediate posttest.",
		"primary_outcome_variables (主要结果变量_因变量)": "Writing skills scores (0–100) based on grammatical accuracy in pretest and posttest essays; change in writing skills scores from pretest to posttest; the number and rate of surface errors (grammatical, punctuation, lexical) per 100 words; error detection counts for ChatGPT and human feedback.",
		"independent_variables_and_factors (自变量与实验因素)": "Feedback condition (ChatGPT written corrective feedback vs human written corrective feedback) as the main between-group independent variable; time (pretest versus posttest) as a within-subjects factor for writing skills scores; for error detection, feedback source (ChatGPT vs human) and error type (lexical, grammatical, punctuation).",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Independent-samples t-test at pretest showed no significant difference between groups for writing skills scores (experimental M=54.61, SD=13.10; control M=52.77, SD=11.10; t=0.549, p=0.586). At posttest, the independent-samples t-test indicated a statistically significant difference favoring the experimental group (ChatGPT WCF) over the control group (human WCF) in writing skills scores (experimental M=66.15, SD=11.34; control M=56.48, SD=11.07; t=3.14, p=0.003). Error-flagging accuracy analysis showed that both human and ChatGPT feedback correctly identified the vast majority of surface errors, with human feedback missing 3 out of 489 errors and ChatGPT missing 7 out of 410 errors, resulting in false negative rates under five percent for both sources.",
		"effect_size_summary (效应量摘要)": "Eta square for the posttest comparison between ChatGPT and human feedback on writing skills scores was reported as η²=0.68, indicating a very large effect size and suggesting that approximately 68 percent of the variance in writing skills scores can be associated with the method of receiving written corrective feedback (ChatGPT versus human) in this study.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study did not report instances of explicit misuse of ChatGPT such as plagiarism or cheating but acknowledged qualitative concerns: one participant noted that ChatGPT explanations of certain fine-grained grammatical points were sometimes less clear than those of human teachers; some errors went undetected by ChatGPT; and in the limitations the author mentions perception bias, where some interviewees may overstate ChatGPT strengths simply because it is an AI-powered tool. The author also cautions that human feedback can better handle nuanced contexts such as stylistically deliberate sentence fragments and non-surface errors, so ChatGPT should not be relied upon as the sole feedback provider.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No statistical subgroup analyses by gender, proficiency, or other demographic variables were reported. The discussion notes that ChatGPT carries a consistent level of expertise across contexts while human evaluators expertise may vary, so feedback effectiveness comparisons between ChatGPT and humans should be interpreted cautiously; however, issues of equity or subgroup differences in outcomes are not directly analyzed.",
		"limitation (研究局限)": "Identified limitations include a relatively small sample size from a single Saudi university, which restricts generalisability; differences in feedback modality and immediacy, with ChatGPT providing interactive and instantaneous feedback while human evaluators could not always match its speed or elaboration, potentially confounding the comparison; perception bias in interview data, as some students may overestimate ChatGPT efficiency because it is an AI tool; interrater reliability being assessed on only 12 essays (about 23 percent of the dataset), limiting robustness of reliability evidence; and potential variability in the quality and thoroughness of human corrective and explanatory feedback across evaluators, which could affect comparison with ChatGPT feedback.",
		"challenge (实施挑战与风险)": "Implementation challenges and risks highlighted include managing differences in the modality and level of elaboration of feedback between ChatGPT and human evaluators, ensuring that students interpret and apply AI-generated feedback correctly, addressing the variability in human feedback quality across evaluators, and preparing both students and instructors to use ChatGPT and similar AWCF tools effectively without over-reliance or misinterpretation.",
		"future_work (未来研究方向)": "The author recommends future work with larger and more diverse samples, exploration of ChatGPT and other AWCF tools across different writing genres and error types, more extensive interrater reliability procedures (for example, applying checks to a larger share of the corpus or full datasets), investigations into how differences in the nature and quality of human feedback across evaluators influence comparative outcomes, and research on training learners and instructors to specify error types of interest and to maximize the pedagogical value of ChatGPT feedback.",
		"implication (理论与教学实践启示)": "Implications include that ChatGPT can serve as a powerful co-feedback provider in EFL writing instruction for surface-level grammatical accuracy, potentially lightening instructors workload and enabling more frequent, detailed feedback on student writing; however, it should complement rather than replace human feedback, especially for nuanced, non-surface, or stylistic aspects of writing. The study suggests that educators consider integrating ChatGPT into academic writing instruction schemes as a supplementary tool while also implementing training for students and teachers on how to use AWCF tools, select error types to focus on, and critically interpret AI-generated explanations, thereby leveraging strengths and understanding limitations of AI in written corrective feedback."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Funded by the University of Jeddah, Jeddah City, Saudi Arabia, under grant No. UJ-24-SHR-3333-1; the author acknowledges technical and financial support from the University of Jeddah.",
		"conflict_of_interest (利益冲突声明)": "The author declares no known competing financial interests or personal relationships that could have appeared to influence the work reported.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Participants were divided into experimental and control groups without explicit description of randomization procedures; baseline equivalence was checked with a pretest t-test showing no significant difference in writing skills scores between groups, but the non-reported allocation method implies potential selection bias.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Blinding procedures are not described; it is unclear whether human evaluators scoring pretest and posttest essays or those providing feedback were blind to group assignment or to the study hypotheses, implying possible detection and performance bias.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The sample size remained 53 in the reported analyses, and no attrition or missing data issues are discussed; procedures for handling potential missing data are not reported.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides a clear description of the research questions, participants, intervention duration and structure, feedback conditions, scoring formula, and main statistical analyses, along with detailed qualitative examples of human and ChatGPT feedback and student interview excerpts; limitations and recommendations are explicitly discussed, although some methodological details such as exact prompts, ChatGPT version, and quantitative reliability indices are not reported.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study identifies ChatGPT as the AWCF tool but does not specify the exact model version, date of access, or configuration parameters; while the general procedure for using ChatGPT to provide feedback is described and sample feedback is presented, lack of specific version information limits strict reproducibility of AI outputs.",
		"baseline_equivalence (基线等同性检验)": "Baseline equivalence between experimental and control groups was evaluated using an independent-samples t-test on pretest writing skills scores, which showed no statistically significant difference (t=0.549, p=0.586), indicating similar initial levels of grammatical accuracy across groups on the measured outcome.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The use of independent-samples t-tests is reported, but explicit tests of normality, homogeneity of variance, or other diagnostic checks are not described in the article.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Grammatical accuracy scores were derived from raw counts of grammatical errors and total words using the formula [total number of grammatical errors / total number of words] × 100, and these values were used as writing skills scores out of 100; error counts for lexical, grammatical, and punctuation errors were aggregated for error analysis; no additional data transformations or outlier treatments are reported."
	}
}