{
	"Basic Identification": {
		"author (作者)": "Ruofei Zhang, Di Zou, Haoran Xie, Fu Lee Wang",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Hong Kong SAR, China",
		"journal_name (期刊名称)": "Computers and Education: Artificial Intelligence",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; concurrent triangulation mixed-method study",
		"doi_or_identifier (DOI或唯一标识)": "10.1016/j.caeai.2025.100489",
		"research_aims (研究目的与问题)": "To investigate how EFL learners engage with GPT responses and learner prompts in ChatGPT-based learning of English argumentative writing logic and why; and to examine how engagement in GPT responses and learner prompts influences learners’ logic knowledge and logic in English argumentative writing and the underlying reasons.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses the scarcity of empirical work on ChatGPT-based learning for English argumentative writing logic specifically (rather than general EFL writing or critical thinking), and the lack of studies separating engagement in GPT responses from engagement in learner prompts; introduces a GPT-4-powered discipline-specific chatbot (LogicalHamster), combines objective eye-tracking data with tests, writing tasks, and interviews, and uses PLS-SEM and regression to model distinct effects of these two engagement components on logic knowledge and argumentative writing logic."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate and postgraduate university students",
		"language_proficiency (语言熟练度水平)": "Upper-intermediate to advanced English proficiency (IELTS 6.0–7.5, self-reported, categorised as competent to good users)",
		"mother_tongue (母语)": "Chinese",
		"sex (性别)": "34 female, 8 male",
		"age (年龄)": "Aged 18–28 years (M=22.90, SD=3.27)",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL learners enrolled in English-as-a-medium instruction (EMI) university courses in Hong Kong",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Students enrolled in English-as-a-medium university courses; specific academic majors NR",
		"prior_experience_llm (既有LLM使用经验)": "All participants were familiar with using ChatGPT in general and reported no technophobia, but none had previous experience with ChatGPT-based instructional activities, logic training, or prompt engineering."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Concurrent triangulation mixed-method design with a single-group pre-test–post-test–delayed-test structure plus eye-tracking during a 45–75-minute GPT-4-based learning session and pre/post writing tasks.",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods (quantitative analyses of eye-tracking metrics, logic tests, and writing scores; qualitative thematic analysis of semi-structured interviews).",
		"sampling_method (抽样方法)": "Voluntary response sampling from a Hong Kong university.",
		"sample_size_and_effect (样本量及效应量)": "Total N=42 Chinese undergraduate and postgraduate students (34 female, 8 male); PLS-SEM showed that engagement in learner prompts explained 17.7% of the variance in argumentative writing logic outcomes (β≈−0.30), while engagement in GPT responses accounted for 11.0% of the variance in logic knowledge outcomes (β≈0.35, t=1.882, f²=0.110); multiple regression indicated that total fixation counts on GPT response area significantly predicted delayed post-test logic knowledge (b=0.001, t=2.37, p=.023, R²=0.286) and average fixation duration on learner prompts significantly negatively predicted post-writing logic scores (b=−22.07, t=−2.83, p=.008, R²=0.658 with predictors).",
		"theoretical_foundation (理论基础_理论框架)": "Sociocultural theory (Vygotsky & Cole, 1978; ChatGPT as a virtual more capable peer), Krashen’s Input Hypothesis and affective filter theory, self-regulated learning theory (Zimmerman, 2013), cognitive load theory (Paas & Sweller, Sweller, 2011), the noticing hypothesis (Schmitt and Schmitt, 2020), Resource Depletion for Output hypothesis (Barcroft, 2006), and Bloom’s taxonomy as background for distinguishing knowledge versus application.",
		"data_collection_instrument (数据收集工具)": "Biographical questionnaire; Tobii X2-30 screen-based eye tracker with Tobii Studio Enterprise to record total fixation counts and average fixation durations on GPT response and learner prompt interface areas; three identical logic knowledge tests on English argumentative writing logic (pre, immediate post, delayed) based on Zhang et al. (2024) requiring identification and explanation of seven logical fallacies and matching them to terminology (score range 0–21); pre-treatment 250-word IELTS-type argumentative essay writing task and post-treatment revision task scored with an analytic rubric for logic in English essays (0–100, adapted from Finken and Ennis’s Illinois Critical Thinking Essay Scoring Rubric and IELTS criteria); semi-structured interviews with 20 participants on engagement and perceived impacts.",
		"data_collection_validity_reliability (工具信度与效度)": "Logic knowledge tests were identical across three time points to ensure internal validity and were blind-scored by the research team with high inter-rater reliability (Cohen’s kappa=.96); original and revised essays were blind-scored by two authors and one colleague with good agreement (Cohen’s kappa=.85); the GPT-4-based chatbot LogicalHamster was piloted with five similar students whose feedback confirmed usability and perceived reliability, and no biases or inaccuracies were found in GPT responses during the pilot; PLS-SEM measurement models showed satisfactory indicator reliability (most loadings >.708), internal consistency (composite reliability and Cronbach’s alpha >.700), and convergent validity (AVE>.50).",
		"data_analysis_method (数据分析方法)": "Descriptive statistics for fixation metrics; paired-sample t-tests comparing total fixation counts and average fixation durations between GPT response and learner prompt areas; PLS-SEM using SEMinR in R with bootstrapping (10,000 subsamples) to model relationships among engagement constructs, prior knowledge, and learning outcomes; multiple linear regressions in SPSS to test predictive power of engagement variables on logic knowledge and writing logic while controlling for prior scores; thematic analysis of semi-structured interviews following a six-step coding process (generating codes, deductive analysis, code frequency calculation, selection of representative quotes) with inter-coder reliability (Cohen’s kappa=.90).",
		"unit_of_analysis (分析单位)": "Individual learner (N=42) for quantitative analyses of eye-tracking metrics, test scores, and writing scores; individual interview transcripts for qualitative thematic analysis.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Single-group design; all volunteers received the same ChatGPT-based learning intervention with no random assignment to different conditions or control group.",
		"power_analysis (功效分析与样本量论证)": "Sample size N=42 was justified against common heuristics (≥30 for paired t-tests; ≥10 observations per predictor for regression; PLS-SEM 10-times rule), and GPower analyses indicated very strong power for R² tests in regressions (≈.98–.99) and strong power for key regression coefficients (≈.98–.99).",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Chinese upper-intermediate to advanced EFL learners (IELTS 6–7.5) enrolled in English-as-a-medium university courses in Hong Kong; voluntary participation and single-institution context with a female-majority sample, which the authors note limits generalisability to other settings, proficiency levels, and more gender-balanced populations.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Logic knowledge tests were blind-scored by the research team using a published scoring system (Zhang et al., 2024) with discrepancies resolved through discussion; original and revised essays were blind-scored by two authors and one colleague using a logic-focused rubric with disagreements reconciled through discussion; specific details of rater training procedures beyond blind scoring and consensus resolution were not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Three-week study: Week 1 orientation and consent; Week 2 pre-writing (40 min), pre-test on logic knowledge (30 min), a single 45–75-minute ChatGPT-based learning session with LogicalHamster and concurrent eye-tracking, post-writing revision task (30 min), and immediate post-test (30 min); Week 3 delayed post-test (30 min).",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT-type GPT-4-powered chatbot (LogicalHamster deployed on the Poe platform).",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "LogicalHamster is a GPT-4-based POE chatbot (https://poe.com/LogicalHamster) accessed via desktop computers;configured via prompt engineering to teach seven common reasoning errors and related strategies using explicit instructions, examples, exercises, discussions, and personalised feedback;adopts a humorous‘ hamster’ persona and natural conversational tone;specific API parameters(e.g., temperature) and system prompt details were not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students individually interacted directly with the GPT-4-powered LogicalHamster chatbot via a web-based interface on desktop computers in a lab with an attached eye-tracker; there was no LMS embedding or teacher-mediated relaying of GPT responses during the learning session.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "The chatbot was designed with prompt engineering to implement five pedagogical approaches: delivering explicit instructions on target logical concepts, presenting illustrative examples with explanations, providing practice exercises with immediate feedback, facilitating open-ended discussions of logical concepts, and analysing logical issues in learners’ submitted arguments to provide personalised feedback; additional strategies included tailoring responses to learner-stated needs and preferences, embodying an engaging humorous hamster persona, providing immediate personalised feedback, and maintaining a natural conversational style; learners generated their own prompts (questions, responses, requests for explanations or examples) without a fixed user-prompt script.",
		"training_support_llm_literacy (LLM素养与提示培训)": "Participants received a 20-minute orientation in Week 1 on English argumentative writing logic and ChatGPT-based learning, and before interviews the researchers clarified the concepts of engagement in GPT responses versus learner prompts using concept-checking questions; participants were already familiar with basic ChatGPT use and reported no technophobia; the study did not report formal training on academic integrity, plagiarism avoidance, or advanced prompt engineering beyond these orientations.",
		"intervention_implementation (干预实施流程_步骤与任务)": "After recruitment and consent in Week 1, participants in Week 2 first wrote a 250-word IELTS-type argumentative essay in 40 minutes, then completed a 30-minute pre-test on logic knowledge. Next, they engaged in a 45–75-minute ChatGPT-based learning session using the GPT-4-powered LogicalHamster chatbot, during which LogicalHamster taught seven common reasoning errors and strategies through explanations, examples, exercises, open-ended questions, and analysis of learners’ own arguments, while a Tobii X2-30 eye tracker recorded fixations on GPT response and learner prompt areas. Immediately afterwards, participants revised their pre-learning essays focusing on logic within 30 minutes using MS Word without internet or external resources and then completed a 30-minute immediate post-test on logic knowledge. Semi-structured interviews with 20 randomly selected participants were conducted after the main session. In Week 3, all participants took a 30-minute delayed post-test on logic knowledge, identical in content and format to the previous tests.",
		"experimental_group_intervention (实验组干预内容)": "All 42 participants received the same ChatGPT-based learning intervention with LogicalHamster to develop knowledge and skills of logic in English argumentative writing, plus pre/post logic tests and pre-writing and post-revision tasks; there was no separate experimental versus control group.",
		"control_group_intervention (对照组干预内容)": "NA",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Initial argumentative essay drafting before the ChatGPT-based learning, followed by a logic-focused revision of the same essay after the LogicalHamster session; AI support was used for learning logic concepts and practising logical reasoning, not during the actual essay revision, which was completed offline.",
		"writing_genre (写作体裁)": "English argumentative essays (IELTS Academic Writing Task 2 style).",
		"writing_task_type (写作任务类型)": "Timed 250-word argumentative essay on an IELTS topic (“Young people who commit crimes should be treated in the same way as adults. To what extent do you agree or disagree?”) followed by a timed revision task focused on argumentative logic.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "The GPT-4-based chatbot functioned as an instructional and dialogic tutor for logic in English argumentative writing: delivering explanations of logical concepts and fallacies, providing examples and exercises, asking and answering questions, analysing learner-submitted arguments, identifying reasoning errors, and giving personalised feedback in a humorous conversational style; it did not grade essays or directly generate full essays for students during the writing tasks.",
		"role_instructor (教师角色与介入方式)": "Researchers/instructors provided orientation on logic and ChatGPT-based learning, administered tests and writing tasks, operated and monitored the eye-tracking equipment, offered necessary technical assistance during the session, and conducted and analysed semi-structured interviews; they did not intervene in or mediate the actual ChatGPT–learner interactions during the learning session.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "University setting in Hong Kong; face-to-face experiment in a computer lab where participants used desktop computers equipped with a Tobii X2-30 screen-based eye tracker to access the online LogicalHamster chatbot; writing tasks were completed on MS Word without internet access.",
		"ethical_consideration (伦理审查与知情同意)": "Before the intervention, each participant received and signed an informed consent form after being assured of no harm, anonymity, no academic consequences tied to performance, and the right to withdraw at any time; the ethics declaration states that the research met ethical guidelines and legal requirements of the study country.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Participants used the GPT-4-powered LogicalHamster chatbot only during the designated 45–75-minute learning session on lab computers; pre-writing and post-revision tasks were completed without internet or external tools; participants’ broader access to ChatGPT outside the study was not restricted but they had no prior ChatGPT-based instructional activities, logic training, or prompt engineering experience.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "LogicalHamster’s instructional prompts and persona were carefully designed to focus on explaining logical fallacies and reasoning in English argumentative writing, and a pilot with five similar students found no biases or inaccuracies in GPT responses; no additional technical safety filters or content moderation mechanisms beyond standard GPT-4 behaviour were reported.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Eye-tracking and interview data showed that learners engaged much more frequently but for shorter durations with GPT responses than with learner prompts, demonstrating frequent short fixations on instructional GPT content and fewer but longer fixations on prompt composition; paired t-tests confirmed significant differences in total fixation counts (t(41)=13.11, p<.001) and average fixation durations (t(41)=−4.08, p<.001) between GPT response and prompt areas. PLS-SEM and multiple regression analyses indicated that, controlling for prior logic knowledge, greater engagement in GPT responses—particularly higher total fixation counts—significantly predicted better long-term retention of logic knowledge (delayed test R²≈0.286; b≈0.001 per additional fixation, p≈.023), whereas deeper engagement in learner prompts—longer average fixation durations—significantly negatively predicted improvements in English argumentative writing logic (R²≈0.658; b≈−22.07, p≈.008). Interview data suggested that GPT responses were perceived as clear, well-organised, and helpful for understanding logic concepts, while prompt generation was sometimes seen as effortful, cognitively exhausting, or of limited added value, although some learners reported that crafting prompts helped them elaborate and reflect on logical concepts and reasoning errors. Overall, the GPT-4-based chatbot effectively supported development and retention of logic knowledge but a single 45–75-minute session did not yield statistically significant positive effects of GPT-response or prompt engagement on logical writing quality, with deep prompt engagement potentially hindering writing logic due to increased extraneous cognitive load."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness was assessed using pre-, immediate post-, and one-week delayed tests of logic knowledge (0–21 points) and pre-writing and post-revision logic scores for argumentative essays (0–100), combined with eye-tracking indices of engagement and interview data; results indicated that the GPT-4-based chatbot substantially aided the acquisition and retention of logic knowledge in English argumentative writing, particularly for learners with greater engagement in GPT responses, while its immediate impact on improving the logic of English argumentative essays was limited and negatively associated with longer fixation durations on learner prompts.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Analytic scoring rubric for logic in English essays (score range 0–100) adapted from Finken and Ennis’s Illinois Critical Thinking Essay Scoring Rubric and IELTS academic writing assessment criteria, evaluating supporting reasons, reasoning, commitment of reasoning errors, and coherence.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Logical quality of English argumentative writing, including accuracy, relevance and sufficiency of supporting reasons, clarity and convincingness of reasoning, frequency and depth of reasoning errors, and overall coherence and logical sequencing of ideas.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Semi-structured interview questions exploring learners’ perceived engagement, enjoyment, comfort, motivation, and attitudes towards GPT responses and learner prompts; no separate quantitative affect questionnaire was administered.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Learners’ perceived usefulness of GPT responses versus learner prompts, enjoyment and reduced boredom due to LogicalHamster’s humorous, friendly persona and emojis, comfort and confidence when interacting with a nonjudgmental chatbot, occasional annoyance at jokes or encouragement perceived as distracting, and scepticism about ChatGPT’s ability to handle complex prompts or in-depth logical discussion.",
		"cognitive_aspect_measure (认知因素测量工具)": "Three identical tests of logic knowledge in English argumentative writing (pre, immediate post, one-week delayed; score 0–21) based on identification and explanation of seven logical fallacies plus term matching; analytic scoring of essay logic; eye-tracking metrics (total fixation counts and average fixation durations) as indicators of cognitive engagement.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Acquisition and retention of logic knowledge (understanding meanings and structures of logical fallacies, associating them with correct terminology), application of logical knowledge to improve reasoning and coherence in English argumentative writing, and metacognitive monitoring of reasoning errors and logical misconceptions as reported in interviews when learners reflected on their prompts and GPT feedback.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Eye-tracking measures using a Tobii X2-30 eye tracker to capture total fixation counts and average fixation durations on GPT response and learner prompt areas of the chatbot interface; counts of learner prompts and qualitative descriptions of prompt types from interviews.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Frequency and duration of visual attention to GPT responses versus learner prompts, behavioural patterns of reading instructional content versus typing and revising prompts, and learners’ choices to focus more on perceived useful parts of GPT responses (e.g., logical explanations, examples) while selectively ignoring jokes or encouragement; differing engagement levels reflecting established passive versus active learning habits and attitudes towards chatbot capabilities.",
		"other_outcomes_measure (其他结果测量工具)": "PLS-SEM path coefficients and effect sizes linking engagement constructs, prior knowledge, and outcome measures; multiple regression models examining predictive power of engagement on logic knowledge and writing logic; thematic coding of semi-structured interview transcripts.",
		"other_outcomes_focus (其他结果维度说明)": "Mechanisms underlying ChatGPT-based learning of English argumentative writing logic, including the role of GPT responses as high-quality input supporting noticing and schema construction, the dual nature of learner prompts in inducing both germane and extraneous cognitive load, and the influence of learner habits and attitudes on engagement patterns and perceived usefulness of ChatGPT-based learning.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pre-test on logic knowledge and pre-writing task before the ChatGPT-based learning session; immediate post-test on logic knowledge and essay revision task right after the learning session; one-week delayed post-test on logic knowledge; no delayed writing assessment.",
		"primary_outcome_variables (主要结果变量_因变量)": "Logic knowledge test scores at pre-, immediate post-, and delayed post-test; logic-focused essay scores at pre-writing and post-revision; post-writing logic quality as an outcome in PLS-SEM and regression models.",
		"independent_variables_and_factors (自变量与实验因素)": "Engagement in GPT responses (total fixation counts and average fixation durations on GPT response area); engagement in learner prompts (total fixation counts and average fixation durations on learner prompt area); learners’ prior logic knowledge (pre-test scores); learners’ prior English argumentative writing logic (pre-writing scores).",
		"followup_length_and_type (随访时长与类型)": "One-week delayed follow-up logic knowledge test; no delayed follow-up writing task.",
		"statistical_significance (统计显著性结果摘要)": "Paired-sample t-tests showed significantly more fixations on GPT responses than on learner prompts and significantly shorter average fixation durations on GPT responses (total fixation count difference: t(41)=13.11, p<.001; average fixation duration difference: t(41)=−4.08, p<.001). PLS-SEM indicated that engagement in learner prompts significantly influenced English argumentative writing logic outcomes (explaining 17.7% of variance, β≈−0.30, 95% bias-corrected confidence interval not crossing zero), whereas engagement in GPT responses showed a near-significant positive effect on logic knowledge outcomes (explaining 11.0% of variance, β≈0.35, t=1.882, BCCI including zero). Multiple regression analyses revealed that, after controlling for pre-test logic knowledge, total fixation counts on GPT response area significantly predicted delayed logic knowledge scores (b=0.001, t=2.37, p=.023, model R²≈0.286), and that, after controlling for pre-writing scores, average fixation duration on learner prompt area significantly negatively predicted post-writing logic scores (b=−22.07, t=−2.83, p=.008, model R²≈0.658); total fixation counts on learner prompts were not significant predictors.",
		"effect_size_summary (效应量摘要)": "PLS-SEM reported that engagement in GPT responses accounted for approximately 11% of the variance in logic knowledge outcomes (f²≈0.110, small-to-medium effect), and engagement in learner prompts accounted for about 17.7% of the variance in English argumentative writing logic outcomes; regression models yielded R²≈0.286 for delayed logic knowledge and R²≈0.658 for post-writing logic when including prior scores and engagement predictors, indicating moderate-to-substantial explained variance; individual regression coefficients for fixation measures had strong statistical power according to GPower analyses.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study did not report plagiarism or overt misuse of ChatGPT, but interviews revealed that some learners perceived LogicalHamster’s jokes and encouragement as annoying or distracting, and many doubted the chatbot’s ability to understand complex prompts or conduct in-depth logical discussions; quantitative analyses showed that deeper engagement in learner prompts (longer fixation durations) was associated with poorer development of English argumentative writing logic, interpreted as a negative effect arising from increased extraneous cognitive load and diversion of attention from instructional content; the authors also highlight broader concerns from the literature about possible over-reliance on ChatGPT and its inconsistent response quality, though these were not directly observed as misuse in the experiment.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No subgroup or equity analyses (e.g., by gender, proficiency band, degree level) were conducted; the authors note as a limitation that the sample consisted solely of Chinese upper-intermediate to advanced EFL learners from one university with a strong female majority and that these learner factors may affect engagement patterns and outcomes, limiting generalisability to other populations.",
		"limitation (研究局限)": "Limitations include the single-institution Chinese sample of upper-intermediate to advanced EFL learners with a marked gender imbalance, limiting generalisability; relatively small sample size (N=42) despite meeting heuristic thresholds; use of a single GPT-4-based chatbot (LogicalHamster) tuned for logic instruction, so findings may not transfer directly to other GPT versions, prompt engineering approaches, disciplines, or interface designs; reliance on one 45–75-minute ChatGPT-based learning session, which may be insufficient to produce robust improvements in English argumentative writing logic; behavioural emphasis of eye-tracking measures, which do not fully capture unobservable cognitive and emotional engagement; absence of explicit measurement of cognitive load in the models despite its theorised importance; and use of only one kind of eye-tracker and no long-term follow-up of writing outcomes.",
		"challenge (实施挑战与风险)": "Implementation challenges include managing learners’ established passive learning habits and negative attitudes towards ChatGPT, which reduced engagement in prompt generation; balancing the cognitive demands of producing prompts with the need to preserve cognitive resources for processing new logical knowledge, as over-engagement in prompts may impose excessive extraneous cognitive load and hinder writing logic; designing chatbot prompts and responses that are sufficiently deep and contextualised to support application of logic knowledge in authentic argumentative writing, addressing concerns that GPT examples may be too simple; ensuring that chatbot personas and humorous elements enhance rather than distract from learning; and dealing with variability in ChatGPT versions, prompt engineering quality, and learners’ technical familiarity when transferring the model to other contexts.",
		"future_work (未来研究方向)": "The authors call for future studies that implement ChatGPT-based learning of English argumentative writing logic over longer timeframes to better capture changes in writing logic, compare different GPT models and prompt-engineering designs and their disciplinary adaptations, explicitly measure cognitive load and its interaction with engagement and outcomes, involve more diverse and gender-balanced learner populations across institutions, proficiency levels, and cultural contexts, and apply similar engagement-analytic frameworks to other domains where ChatGPT is used to teach domain-specific logic or higher-order skills.",
		"implication (理论与教学实践启示)": "Practically, the study suggests that GPT-4-based chatbots can effectively serve as virtual more capable peers for teaching logic knowledge in English argumentative writing when supported by careful prompt engineering and engaging personas; teachers should tailor ChatGPT-based interventions to learners’ prior experiences and attitudes, conduct pre-intervention surveys, and provide orientation to enhance acceptance; long-term, scaffolded use is recommended for improving argumentative writing logic, with early stages focusing on comprehending and internalising logical concepts via GPT responses and later stages on applying logic in authentic tasks; prompt engineering should emphasise high-quality, contextualised examples and clear guidance on applying logic to real writing, while chatbot communication style should foster positive emotions without distracting from content; theoretically, the findings reinforce sociocultural theory, Input Hypothesis, cognitive load theory, and self-regulated learning theory in ChatGPT-based contexts and highlight the need to distinguish between GPT responses and learner prompts when analysing engagement and designing AI-mediated language-learning tasks."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Partially supported by the Teaching Development and Language Enhancement Grant of the University Grant Committee, The Hong Kong Polytechnic University (Project No. PolyU/TDLEG25-28/IICA/P/02), and the Start-up Fund for New Recruits of The Hong Kong Polytechnic University (Project No. P0056518).",
		"conflict_of_interest (利益冲突声明)": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "No randomisation was used; participants were volunteers and all received the same ChatGPT-based intervention in a single group, which introduces potential selection bias and precludes controlled comparisons with alternative instructional conditions.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Participants were aware they were using a GPT-4-based chatbot and that their engagement was being tracked; however, logic tests and writing tasks were blind-scored by raters who did not know test timepoint or participant identities; blinding of analysts to specific hypotheses during data analysis was not reported.",
		"attrition_and_missing_data (流失与缺失数据处理)": "All 42 recruited participants were included in the final sample; no participant attrition or missing data issues were reported, and no specific procedures for handling missing data were described.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of participants, instruments, eye-tracking setup, chatbot design, procedure, and statistical methods; includes appendices with test and rubric descriptions and PLS-SEM measurement indices; and explicitly states that the research meets ethical guidelines; raw data are not publicly shared due to lack of participant consent for public data sharing, but the analytic approach is clearly documented.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that LogicalHamster is a GPT-4-powered chatbot hosted on Poe and outlines its instructional functions, persona, and general prompt-engineering strategies, but does not provide full system prompts, training corpus details, or model parameter settings (e.g., temperature), and acknowledges that variations in GPT versions and implementation may affect transfer of the findings to other contexts.",
		"baseline_equivalence (基线等同性检验)": "Only a single learner group was used; all participants completed pre-tests for logic knowledge and a pre-writing task, which served as covariates in PLS-SEM and regression models, but there were no separate groups to compare for baseline equivalence.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The authors report checking linearity and homoscedasticity via scatterplots and P-P plots, verifying approximate normality with skewness between −2 and +2 and kurtosis between −7 and +7, confirming adequacy for t-tests and regressions; they note that Pearson correlations among predictors were below .90, indicating no severe multicollinearity, and that for PLS-SEM, VIF values were below 3, indicating acceptable collinearity.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Eye-tracking data were processed in Tobii Studio to calculate total fixation counts and average fixation durations within dynamically adjusted GPT response and learner prompt areas; test and writing scores were used directly in statistical analyses without reported transformation; no additional data transformations or sensitivity analyses were described."
	}
}