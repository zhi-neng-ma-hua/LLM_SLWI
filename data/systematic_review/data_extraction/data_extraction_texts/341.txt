{
	"Basic Identification": {
		"author (作者)": "Bart Deygers, Liisa Buelens, David Chan, Laura Schildt, Amaury Van Parys, Marieke Vanbuel",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Belgium (major Belgian university)",
		"journal_name (期刊名称)": "ELT Journal",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article",
		"doi_or_identifier (DOI或唯一标识)": "10.1093/elt/ccaf020",
		"research_aims (研究目的与问题)": "To examine whether the use of ChatGPT as an automated writing evaluation tool in EFL writing classes over a nine-week intervention leads to measurable differences in syntactic and lexical complexity, and to determine if ChatGPT-supported feedback results in sustainable writing gains when access to ChatGPT is removed.",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses the lack of longitudinal research on generative AI-based automated writing evaluation in naturalistic EFL classroom settings, going beyond short-term, redrafting-focused interventions by tracing syntactic and lexical complexity across repeated new writing tasks over nine weeks and comparing a teacher-feedback-only condition with a teacher-plus-ChatGPT feedback condition."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "First-year university students",
		"language_proficiency (语言熟练度水平)": "EFL learners in an English translation programme; none had taken an English writing composition course before; both groups reported similarly low familiarity with ChatGPT at the start (mean 1.8 on a five-point scale).",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "Total N=105; 79 female, 1 non-binary, remaining participants not explicitly specified",
		"age (年龄)": "Average age 19 years (SD=1.5)",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL (English as a foreign language) at a Belgian university",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "English translation programme at a major Belgian university",
		"prior_experience_llm (既有LLM使用经验)": "Both experimental and control groups considered themselves equally unfamiliar with ChatGPT at the start of the project (mean familiarity 1.8 on a five-point scale)."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental longitudinal design with two parallel groups (experimental vs control) over nine weeks; baseline hand-written essay without assistance in week 1, instructional and training phase in weeks 2–4, writing practice in week 5, two in-class essays with differential feedback (teacher plus ChatGPT versus teacher only) in weeks 6 and 8, and a final in-class essay without ChatGPT for both groups in week 9.",
		"research_method (研究方法_定量_定性_混合)": "Primarily quantitative, using automated syntactic and lexical complexity indices and multilevel linear regression; supplemented by a short quantitative survey on ChatGPT usage in the experimental group.",
		"sampling_method (抽样方法)": "Students were first-year English translation majors at a major Belgian university; they were randomly assigned to the experimental or control condition within this intact cohort.",
		"sample_size_and_effect (样本量及效应量)": "Total N=105 first-year students; experimental condition n=66, control condition n=39. The combined corpus comprised 373 essays (161,532 words). Word count analyses showed that during the ChatGPT-supported weeks the experimental group wrote significantly longer texts than the control group, with large effect sizes (week 6: W=480.5, p=.001, rank-biserial correlation Rrb≈−.5; week 8: W=681, p=.01, Rrb≈−.31), while the difference was non-significant once ChatGPT access was removed in week 9 (W=944.5, p=.9, Rrb≈−.01). T-tests for interim weeks indicated small-to-moderate Cohen’s d values for some complexity measures (for example, sentence length d≈.22; clause length d≈−.5 and d≈−.65; age of acquisition d≈−.8; academic words d≈0.2; low-frequency words d≈.48). Multilevel models showed a significant main effect of time on all syntactic and lexical complexity variables but no significant main effect of condition; significant time-by-condition interaction effects were found for mean length of T-unit and mean length of clause only.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in research on automated writing evaluation, corrective feedback, and writing development which links writing gains to syntactic and lexical complexity, and in prior AWE literature indicating that feedback timeliness and specificity are critical and that generative AI tools such as ChatGPT can provide real-time, contextualized feedback as a supplement to human feedback.",
		"data_collection_instrument (数据收集工具)": "Controlled in-class writing tasks (baseline 300-word text about an inspiring celebrity dinner companion; expository essays on UK national newspapers, mental health, and online privacy written in up to 90 minutes); topic factsheets providing background information in multiple modalities (text, quotations, tables, charts) to equalize topic familiarity; Natural Language Processing tools TAALES for lexical sophistication indices and TAASSC for syntactic sophistication and complexity indices; automatic word count as a fluency measure; brief survey in the experimental group after ChatGPT-supported writing sessions (weeks 6 and 8) including Likert-scale items on ChatGPT usage and multiple-choice questions on uses (structure/coherence, grammar/spelling, vocabulary enhancement, idea development, fluency/style).",
		"data_collection_validity_reliability (工具信度与效度)": "All writing was produced under controlled classroom conditions within fixed time limits to ensure compliance with condition-specific access to ChatGPT; baseline hand-written text in week 1 served as a control variable for initial ability; topics for Essays 1–3 were accompanied by factsheets to minimize differences in topic familiarity; established NLP tools TAALES and TAASSC, which include hundreds of validated indices of lexical and syntactic complexity, were used to derive outcome measures; baseline equivalence checks showed no significant differences between groups in gender, age, writing enjoyment, or familiarity with ChatGPT.",
		"data_analysis_method (数据分析方法)": "Descriptive statistics and graphical inspection of trends for word count and complexity indices over weeks; non-parametric Wilcoxon tests for word count differences between groups at key weeks; independent t-tests for selected syntactic and lexical measures at weeks 6 and 8; multilevel linear regression models with random intercepts and slopes for each student to estimate the effect of time (week), condition (experimental vs control), and their interaction on syntactic and lexical complexity while controlling for baseline essay complexity; analyses conducted in R (version 4.1.1) using packages lme4, psych, ggplot2, and sjPlot.",
		"unit_of_analysis (分析单位)": "Individual essays nested within individual students; syntactic and lexical complexity indices and word counts computed for each essay.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Random assignment of participants to experimental (teacher feedback plus ChatGPT for AWE) and control (teacher feedback only) conditions; students were organized in four class groups, each with a different instructor using the same feedback model and standardized teaching materials.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of first-year English translation students at a major Belgian university who had not previously taken an English composition course; the sample is representative of this specific university-level EFL population but not claimed to generalize beyond similar programmes and institutional contexts.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Human raters did not assign holistic or analytic scores; instead, syntactic and lexical features were automatically extracted using TAASSC and TAALES, and word count was computed as a fluency indicator; no rater training or scoring rubric was reported because evaluation relied on automated indices rather than human scoring."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Nine-week intervention: week 1 baseline hand-written essay; weeks 2–4 regular writing instruction for all students plus an online ChatGPT learning path for the experimental group; week 5 writing practice (teacher feedback plus ChatGPT versus teacher-only feedback); week 6 Essay 1 with teacher feedback for both groups and additional ChatGPT access for the experimental group; week 7 feedback on Essay 1 (teacher plus ChatGPT versus teacher only); week 8 Essay 2 with the same differential feedback conditions; week 9 Essay 3 without ChatGPT access for either group.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT3 (generative AI-based large language model used as an AWE tool)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT3 was used between February and May 2023, shortly after its public launch; the paper notes that grammatical feedback in ChatGPT3 focused on correctness and fluency and that lexical feedback targeted word choice and lexical diversity, while text-level organizational feedback was less developed; the specific access modality (for example web interface vs API) and parameter settings were not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students in the experimental condition interacted directly with ChatGPT during in-class writing sessions for automated writing evaluation, using it as a stand-alone feedback tool alongside concurrent teacher feedback; ChatGPT was not embedded in a separate LMS or scoring platform and was not mediated by the teacher before reaching the students.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "The experimental group completed an online ChatGPT learning path with four sessions focusing on the nature and functionalities of ChatGPT, using ChatGPT for feedback on structure and flow, using it for grammar and vocabulary feedback, and using it to generate arguments and ideas; each session began with a five-minute tutorial video followed by exercises; students were explicitly instructed in prompt engineering to request structural, grammatical, lexical, and content-related feedback and to obtain corrections and explanations that would provide insight rather than simply accepting and implementing suggested edits.",
		"training_support_llm_literacy (LLM素养与提示培训)": "During weeks 2–4, the experimental group took an online course on using ChatGPT for AWE and prompt engineering, covering how to use ChatGPT for multiple forms of feedback (structure, grammar, vocabulary, content) and how to formulate prompts so that ChatGPT’s responses could support learning rather than merely produce improved text; after the study, all students (including the control group) were given access to this ChatGPT learning path, indicating an institutional effort to build AI literacy.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Week 1: all students wrote a 300-word hand-written text about an inspiring celebrity dinner companion without any writing assistance; this served as a control measure of initial writing ability. Weeks 2–4: all students received the same writing instruction, while the experimental group also completed an online learning path on ChatGPT for AWE and prompt engineering. Week 5: writing practice where both groups wrote with teacher feedback, and the experimental group additionally used ChatGPT while writing. Week 6: all students wrote Essay 1 in class with a maximum of 90 minutes, with teacher feedback available to both conditions and ChatGPT support for the experimental group; the experimental group completed a short post-writing survey on ChatGPT usage. Week 7: feedback phase on Essay 1, again with teacher feedback for all and ChatGPT as an additional feedback source for the experimental group. Week 8: students wrote Essay 2 under the same differential feedback conditions (teacher plus ChatGPT vs teacher only) and the experimental group completed another survey on ChatGPT use. Week 9: all students wrote Essay 3 in class without any access to ChatGPT, to assess sustained gains when AI support was removed; all writing tasks were performed under controlled classroom conditions with topic factsheets provided to all students.",
		"experimental_group_intervention (实验组干预内容)": "Experimental group received standard writing instruction and teacher feedback plus an online training module on ChatGPT and prompt engineering (weeks 2–4), and during practice and test essays (weeks 5, 6, 7, and 8) they could use ChatGPT in real time for automated writing evaluation, requesting feedback on structure, grammar, vocabulary, content development, and overall fluency and style; they also completed short surveys on ChatGPT usage after ChatGPT-supported writing sessions.",
		"control_group_intervention (对照组干预内容)": "Control group received the same writing instruction, standardized teaching materials, topics, and teacher feedback as the experimental group but had no access to ChatGPT or the ChatGPT learning path during the intervention period; they wrote the same baseline essay and the same expository essays under the same time limits and topic conditions, and only after week 9 were they given access to the ChatGPT learning path.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT was integrated into the drafting and revision stages of in-class essay writing during the intervention weeks, providing immediate feedback as students composed and revised their texts; baseline and final essays were written without AI assistance.",
		"writing_genre (写作体裁)": "Expository essays on academic and social topics (baseline 300-word personal/expository text on an inspiring celebrity dinner companion; later essays on UK national newspapers, mental health, and online privacy).",
		"writing_task_type (写作任务类型)": "Timed in-class essays: week 1 baseline hand-written essay (~300 words) without assistance; three subsequent expository essays (Essays 1–3) on provided topics written in class within a 90-minute time limit, accompanied by topic factsheets to standardize background knowledge.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT functioned as an automated writing evaluation tool providing qualitative feedback rather than scores; it offered grammatical and spelling corrections, suggestions for vocabulary enhancement and lexical variety, comments on text structure and coherence, support for developing ideas and content, and checks for overall fluency and style; survey data indicate that students mainly used ChatGPT to obtain feedback rather than to generate complete sentences or paragraphs.",
		"role_instructor (教师角色与介入方式)": "Instructors provided regular writing instruction, standardized teaching materials, and teacher feedback during writing sessions for both experimental and control groups; four instructors each taught one group but followed the same feedback model; teachers did not mediate ChatGPT output but coexisted with it as a parallel feedback source for the experimental group.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "University EFL writing classes within an English translation programme at a major Belgian university; four class groups taught by different instructors; all writing tasks conducted face-to-face in regular class time under controlled conditions.",
		"ethical_consideration (伦理审查与知情同意)": "NR",
		"llm_access_policy (LLM使用规范_允许与限制)": "During the intervention, only students in the experimental condition were allowed to use ChatGPT in class for AWE while writing; baseline (week 1) and final essay (week 9) were written by all students without access to ChatGPT; in all writing sessions access conditions were monitored through controlled classroom settings; the control group had no access to ChatGPT during the intervention and gained access to the ChatGPT learning path only after the study concluded.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "NR",
		"key_findings (主要研究发现_与LLM写作干预相关)": "The experimental group, which used ChatGPT for AWE alongside teacher feedback, produced significantly longer texts than the control group during the weeks when ChatGPT was available, with large effect sizes for word count differences, but this advantage disappeared when ChatGPT access was removed in week 9. Both groups showed significant improvements over time in syntactic and lexical complexity, indicating strong effects of instructional time. Multilevel analysis revealed no significant main effect of condition (ChatGPT use) on any syntactic or lexical complexity variable after controlling for baseline ability; time-by-condition interactions were significant only for mean length of T-unit and mean length of clause, suggesting that ChatGPT-supported feedback may have contributed to gains in specific aspects of syntactic complexity when combined with traditional instruction. No significant interactions were found for lexical complexity indices, indicating that gains in lexical sophistication and diversity were primarily attributable to instruction rather than ChatGPT use. Survey results showed that experimental group students mainly used ChatGPT for feedback (particularly grammar, spelling, lexical variety, idiomaticity, structure, and content) instead of full text generation, and that reported ChatGPT usage varied slightly between weeks 6 and 8. Overall, the study concludes that although ChatGPT can positively influence text length and some syntactic complexity during use, it does not lead to clear, sustained advantages in syntactic or lexical complexity once AI support is withdrawn, and that its educational potential is best realized as a supplement to teacher feedback rather than a replacement."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness was evaluated using automated indices of syntactic complexity (mean length of sentence, clauses per sentence, mean length of T-unit, mean length of clause), lexical complexity (age of acquisition, concreteness, word frequency, prevalence of academic words), and word count across four key essays. The intervention showed that access to ChatGPT led to longer essays and some increases in syntactic complexity during the active AWE phase, but multilevel models indicated that overall gains in syntactic and lexical complexity over nine weeks were driven mainly by instructional time rather than by ChatGPT itself, and the experimental group did not exhibit sustained superiority over the control group when writing without ChatGPT.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Automated linguistic indices computed by TAASSC and TAALES: syntactic complexity measures including mean length of sentence, clauses per sentence, mean length of T-unit, and mean length of clause; lexical complexity measures including age of acquisition for all words, concreteness ratings for content words, frequency based on the SUBTLEXus corpus, and the proportion of academic words; word count as an indicator of fluency.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Syntactic complexity (overall sentence and T-unit complexity, clausal subordination and coordination, clause-level elaboration), lexical sophistication and diversity (age of acquisition, concreteness, word frequency, academic lexis), and fluency (number of words produced within 90 minutes).",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Baseline self-report items on writing enjoyment and familiarity with ChatGPT measured on five-point Likert scales for both groups to check equivalence; in addition, the experimental group completed brief post-writing surveys in weeks 6 and 8 that included a Likert-scale rating of ChatGPT usage, though these were not used as primary affective outcome measures.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Writing enjoyment and familiarity with ChatGPT were assessed at baseline for group equivalence purposes; beyond this, the study did not systematically analyze affective constructs such as motivation, attitudes, anxiety, or self-efficacy, and affective data were not reported as central outcomes.",
		"cognitive_aspect_measure (认知因素测量工具)": "NR",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NR",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Post-writing surveys in the experimental group in weeks 6 and 8 recorded self-reported intensity of ChatGPT use on a five-point Likert scale and multiple-choice responses about the purposes for which ChatGPT was used (structure/coherence, grammar and spelling, vocabulary and lexical variety, idea development, fluency and style); word count trends across writing tasks also served as behavioral indicators of writing output under different feedback conditions.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Frequency and purposes of ChatGPT use during ChatGPT-supported essays in the experimental group: many students reported using ChatGPT primarily for grammatical and lexical feedback and for idiomatic phrasing as well as for structure and idea development; usage intensity was slightly higher in week 6 than in week 8 on average; students reported mainly using ChatGPT as a scaffolding tool for feedback rather than as a text generator, and some students reduced their use over time.",
		"other_outcomes_measure (其他结果测量工具)": "NR",
		"other_outcomes_focus (其他结果维度说明)": "NR",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Baseline essay in week 1 without assistance; key essays in weeks 6 and 8 written with differential feedback conditions (teacher plus ChatGPT versus teacher only); final essay in week 9 written without ChatGPT for both groups to assess sustained gains; complexity measures from the baseline essay were used as control variables in the multilevel models rather than being directly compared with week 9 scores.",
		"primary_outcome_variables (主要结果变量_因变量)": "Syntactic complexity indices (mean length of sentence, clauses per sentence, mean length of T-unit, mean length of clause), lexical complexity indices (age of acquisition, concreteness, word frequency, proportion of academic words), and word count per essay.",
		"independent_variables_and_factors (自变量与实验因素)": "Experimental condition (teacher feedback plus ChatGPT vs teacher feedback only), time/week (longitudinal measurement across the nine-week intervention), and baseline complexity measures as covariates; the key interaction of interest was time by condition.",
		"followup_length_and_type (随访时长与类型)": "Short-term follow-up within a single nine-week instructional period: after several weeks of ChatGPT access for the experimental group, all students completed a final essay in week 9 without ChatGPT to assess whether any complexity gains observed during the intervention would persist in independent writing; no long-term follow-up beyond the course duration was conducted.",
		"statistical_significance (统计显著性结果摘要)": "During the ChatGPT-supported weeks, the experimental group wrote significantly longer texts than the control group (week 6: W=480.5, p=.001, rank-biserial correlation Rrb≈−.5; week 8: W=681, p=.01, Rrb≈−.31), while the difference in word count in week 9 without ChatGPT was non-significant (W=944.5, p=.9, Rrb≈−.01). Interim t-tests indicated that in some weeks the experimental group wrote significantly longer sentences (for example, week 8 mean length of sentence t(92)=−2.25, p=.03, d≈.22) and longer clauses (week 6 mean length of clause t(91)=−2.3, p=.02, d≈−.5; week 8 mean length of clause t(92)=−3.04, p=.003, d≈−.65), and produced vocabulary with higher age of acquisition (week 6 t(90)=−3.99, p=.001, d≈−.8), more academic words (week 6 t(90)=−2.7, p=.001, d≈0.2), and more low-frequency words (week 6 t(90)=−2.2, p=.03, d≈.48). Multilevel models showed a significant main effect of time on all syntactic and lexical complexity measures for both groups, no statistically significant main effect of condition, and significant time-by-condition interaction effects only for mean length of T-unit and mean length of clause, indicating additional syntactic complexity gains associated with ChatGPT-supported feedback when combined with instruction.",
		"effect_size_summary (效应量摘要)": "Effect sizes for word count differences during ChatGPT use were large (rank-biserial correlations of approximately 0.5 in week 6 and 0.31 in week 8), while the difference vanished at week 9 (Rrb≈0.01). Interim t-tests produced small-to-moderate Cohen’s d values: sentence length d≈.22 in week 8; clause length d≈−.5 and d≈−.65 in weeks 6 and 8; age of acquisition d≈−.8; academic words d≈0.2; low-frequency words d≈.48. Multilevel models reported significant time-by-condition interactions for mean length of T-unit and mean length of clause but did not provide standardized effect sizes beyond regression estimates.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "No explicit instances of ChatGPT misuse such as cheating or plagiarism were reported, but the results indicate that ChatGPT use did not translate into clearly sustained advantages in syntactic or lexical complexity when access to the tool was removed; the authors caution against overestimating ChatGPT’s long-term impact on writing gains and note that, within the scope of this study, lexical complexity gains did not appear to be driven by ChatGPT and that ChatGPT may be of limited effectiveness as a vocabulary-learning tool without targeted interventions.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The study reported that there were no significant differences between the experimental and control groups at baseline in terms of gender distribution, age, writing enjoyment, or familiarity with ChatGPT, but it did not perform or report detailed subgroup analyses by gender, proficiency, or other demographic factors; equity and subgroup effects were not explored beyond establishing baseline group equivalence.",
		"limitation (研究局限)": "The authors acknowledge that although the nine-week intervention is longer than many previous AWE studies, it may still be insufficient to capture long-term writing gains; the outcome measures were limited to syntactic and lexical complexity plus word count and did not include key aspects such as content quality and organization; the study was conducted in real-world classroom conditions, meaning the researchers could not fully control how students used ChatGPT, including any off-task or out-of-class usage; the results are specific to first-year EFL translation students at a single Belgian university and may not generalize to other populations or educational contexts.",
		"challenge (实施挑战与风险)": "Challenges include ensuring that ChatGPT use remains focused on feedback rather than overreliance on AI-generated language, guiding students to use ChatGPT effectively for vocabulary development rather than mainly for grammatical and structural support, and monitoring ChatGPT usage under real-world conditions where students’ interactions with AI cannot be fully controlled; the study suggests that without targeted guidance, ChatGPT may not substantially enhance lexical complexity or lead to sustained gains.",
		"future_work (未来研究方向)": "Future research should employ longer-term designs to assess the durability of AI-induced writing gains, include a broader range of writing constructs such as content and organization, examine how ChatGPT can be better leveraged for vocabulary development through targeted interventions, and more closely monitor or log how students use ChatGPT in and out of the classroom; further work is also needed to refine prompt engineering training and explore under which conditions ChatGPT-supported AWE most effectively complements teacher feedback.",
		"implication (理论与教学实践启示)": "The findings imply that ChatGPT, used as an AWE tool, can support EFL learners’ writing by increasing text length and contributing to specific aspects of syntactic complexity during use, but that its benefits on lexical complexity and sustained writing gains are limited without structured and targeted application; ChatGPT should therefore be integrated as a supplement to, not a replacement for, teacher feedback, with explicit training in prompt engineering and clear guidance on how to use AI feedback for learning rather than mere text improvement; balanced instructional approaches that combine human feedback, AI-supported AWE, and sustained writing practice appear necessary to convert short-term benefits into long-term gains in writing proficiency."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "NR",
		"conflict_of_interest (利益冲突声明)": "NR",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "The study is described as quasi-experimental, but students were randomly assigned to the experimental or control condition, which helps reduce selection bias; however, four different instructors taught the groups, which could introduce instructor-related variance even though a standardized feedback model and teaching materials were used.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "NR (no explicit information on blinding of instructors or analysts; given the classroom setting and the use of ChatGPT-based activities and surveys, instructors and students were likely aware of group assignment, and automated complexity measures were used instead of blinded human ratings).",
		"attrition_and_missing_data (流失与缺失数据处理)": "NR",
		"reporting_transparency (报告透明度与可重复性)": "The article clearly reports the research question, nine-week design in tabular form, participant characteristics, assignment to experimental and control conditions, writing topics and timing, use of topic factsheets, details of ChatGPT training and usage conditions, the NLP tools (TAALES and TAASSC) and complexity variables used, the multilevel modelling approach and control for baseline ability, and key statistical results including p-values and some effect size metrics; however, it does not include raw texts, full code, or detailed ChatGPT prompts, which limits full reproducibility.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that it used ChatGPT3 and that data collection occurred between February and May 2023, just months after the initial public launch of ChatGPT, and describes the general feedback characteristics of that version; it does not report exact model build, interface type, or default settings, and no logs of model responses or prompts are provided, which constrains precise replication of the AI component.",
		"baseline_equivalence (基线等同性检验)": "The authors state that there were no significant differences between the experimental and control groups at baseline in terms of gender, age, writing enjoyment, or familiarity with ChatGPT; baseline writing ability was controlled by including complexity measures from the week 1 essay as covariates in the multilevel models rather than by direct pre–post comparisons.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The researchers inspected data plots and descriptive statistics before modelling and then applied multilevel linear regression using R; beyond this, no detailed tests of model assumptions (such as residual diagnostics, checks for normality or homoscedasticity, or influence analyses) are reported.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "All texts were cleaned and formatted according to the requirements of TAASSC and TAALES before processing; complexity measures from the baseline essay were used as control variables in the multilevel models; no other data transformations or preprocessing steps are described."
	}
}