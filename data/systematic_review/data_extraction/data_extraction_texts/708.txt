{
	"Basic Identification": {
		"author (作者)": "Irene Brainnita Oktarin; Maria Edistianda Eka Saputri; Betty Magdalena; Tommy Hastomo; Aksendro Maximilian",
		"publication_year (发表年份)": "2024",
		"study_region (研究地区)": "Indonesia (one private university)",
		"journal_name (期刊名称)": "Edelweiss Applied Science and Technology",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article; mixed-methods quasi-experimental study with post-test control group design",
		"doi_or_identifier (DOI或唯一标识)": "10.55214/25768484.v8i4.1600",
		"research_aims (研究目的与问题)": "To investigate the impacts of AI chatbots (ChatGPT) on EFL university students’ writing learning outcomes, writing feedback literacy, and writing engagement, and to identify strategies students employ to effectively use AI chatbots to enhance these areas; specifically, to answer whether AI chatbots significantly affect writing learning outcomes, writing engagement and literacy, and what strategies students use with AI chatbots.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses the limited empirical research on the effects of AI chatbots on EFL students’ writing outcomes, engagement, and feedback literacy at the tertiary level; extends writing feedback literacy research beyond mainly case studies and theoretical work; contributes evidence on how AI chatbot-assisted feedback can foster writing feedback literacy and engagement in authentic EFL classroom contexts."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Tertiary-level university students enrolled in an Economic English course",
		"language_proficiency (语言熟练度水平)": "Two intact classes with comparable English writing abilities confirmed by pretest results showing no significant differences; specific proficiency levels not reported",
		"mother_tongue (母语)": "Indonesian",
		"sex (性别)": "NR",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in an Indonesian private university",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Economic English course; students from Management and English Education programs at Indonesian higher education institutions",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Mixed-methods design combining a quantitative post-test-only control-group design with qualitative semi-structured interviews; two intact classes (experimental vs control) received a 16-week intervention and were compared on post-test writing outcomes, feedback literacy, and engagement, with additional interview data from a subsample.",
		"research_method (研究方法_定量_定性_混合)": "Mixed-methods (quantitative and qualitative)",
		"sampling_method (抽样方法)": "Purposive sampling of two intact Economic English classes at a private Indonesian university; inclusion criteria included enrollment in the course, comparable English writing ability based on pretest results with no significant differences between classes, active participation in all study activities (including AI chatbot use for the experimental group), and sufficient time to attend all sessions.",
		"sample_size_and_effect (样本量及效应量)": "Total N=50; experimental class n=25, control class n=25. Writing learning outcomes: experimental mean=85.92, SD=3.60; control mean=77.16, SD=6.30; independent samples t-test showed a significant difference (t(48)=6.028, p=0.000), mean difference=8.76, 95% CI [5.83, 11.68]. Writing feedback literacy: experimental mean=86.48, SD=3.18; control mean=77.28, SD=6.34; t(48)=6.486, p=0.000, mean difference=9.20, 95% CI [6.34, 12.05]. Writing engagement: experimental mean=86.88, SD=3.37; control mean=76.96, SD=5.68; t(48)=7.500, p=0.000, mean difference=9.92, 95% CI [7.26, 12.57]. No standardized effect sizes such as Cohen’s d were reported.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in EFL writing and writing feedback research, educational psychology, higher education, and second language acquisition; adopts the academic literacies perspective that views literacy as a complex social practice and feedback literacy as involving knowing (epistemological), being (ontological), and acting (practical); draws on student feedback literacy frameworks emphasizing actions, emotion management, making judgments, and valuing feedback; uses Molloy et al.’s four elements of student feedback literacy (taking action, managing affect, making judgments, appreciating feedback processes); situates AI chatbots within technology-enhanced feedback and student-centered feedback models.",
		"data_collection_instrument (数据收集工具)": "Writing learning outcomes: two 60-minute English writing tasks as post-test, assessed with an analytical scoring rubric allocating 25 points each for vocabulary, sentence structure, organization and coherence, and format and substance (total 0–100). Writing feedback literacy: 20-item questionnaire on a 5-point Likert scale covering taking action, managing emotions, forming judgments, identifying multiple feedback sources, and valuing feedback, adapted from prior research and translated into Indonesian. Writing engagement: scenario-based event-level method with three scenarios capturing cognitive, emotional, and behavioral engagement using the Motivation and Engagement Scale adapted from Martin (2009), with 0–100 ratings per scenario and averages taken. Qualitative data: semi-structured interviews with eight randomly selected students (four per group), each lasting approximately 20–25 minutes.",
		"data_collection_validity_reliability (工具信度与效度)": "Writing test scores were obtained using an analytical rubric with four equally weighted dimensions (vocabulary, sentence structure, organization/coherence, format and substance), and inter-rater reliability was employed to ensure score validity (specific coefficients not reported). The writing feedback literacy questionnaire was pilot-tested on ten students and showed high reliability with Cronbach’s alpha=0.91. The modified Motivation and Engagement Scale for writing engagement demonstrated high reliability with Cronbach’s alpha=0.92. For qualitative coding, grounded-theory-based thematic analysis used multiple coders; interrater reliability was reported with Cohen’s Kappa coefficients around 0.91–0.92 and an r-value of 0.86, indicating strong agreement.",
		"data_analysis_method (数据分析方法)": "Quantitative analysis: SPSS version 27 was used; descriptive statistics (means, standard deviations, standard errors) were computed; normality and homogeneity tests were conducted; independent samples t-tests were used to examine differences between experimental and control groups for writing learning outcomes, writing feedback literacy, and writing engagement; Levene’s tests for equality of variances and 95% confidence intervals for mean differences were reported. Qualitative analysis: grounded theory and thematic analysis in three stages (theme assignment, content analysis, categorization and labeling); transcripts were repeatedly reviewed to identify themes, similar themes were merged, and categories labeled; interrater reliability for coding was checked using correlation (r≈0.86) and Cohen’s Kappa (≈0.92).",
		"unit_of_analysis (分析单位)": "Individual student (for quantitative analyses of writing outcomes, feedback literacy, and engagement); individual interviewee (for qualitative thematic analysis).",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Non-random assignment of intact classes; two comparable classes (based on pretest writing scores) were designated as control and experimental groups; both classes were taught by the same teacher to avoid instructor bias.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of students enrolled in the Economic English course at a private Indonesian university; two intact classes of 25 students each were selected via purposive sampling; results are representative only of similar EFL university settings and may not generalize to other institutions, disciplines, or proficiency levels.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Two 60-minute writing tasks were scored using an analytical rubric allocating 25 points each for vocabulary, sentence structure, organization and coherence, and format and substance; inter-rater reliability was used to ensure consistency of writing scores, though specific training procedures and numerical reliability indices for writing scores were not reported; the same teacher taught both groups and applied identical evaluation methods and testing instruments for post-tests."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Sixteen-week intervention within an Economic English course; during this period the experimental group used ChatGPT for feedback alongside regular course activities, and the control group followed traditional instruction; after 16 weeks, both groups completed post-test writing tasks and questionnaires.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (AI chatbot; specific GPT version not reported)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT was used as an AI chatbot tool exclusively for receiving feedback on writing; version, interface type (e.g., web UI or app), and parameter settings were not reported; AI content detection software (Turnitin) was used to check the originality of students’ texts and ensure they were free from AI-generated content.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "In the experimental group, students directly used ChatGPT outside class as part of their homework to obtain feedback on online writing exercises; ChatGPT use was integrated alongside the same textbook and lectures as the control group, and it functioned as an external feedback source rather than being embedded in a dedicated learning management system.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Students in the experimental group were encouraged to use ChatGPT during their writing process for online exercises; the teacher provided specific instructions and criteria for its use, including outlining frequencies and procedures for interacting with the chatbot and how these interactions could be measured; detailed prompt wording, roles, or advanced prompting techniques were not reported.",
		"training_support_llm_literacy (LLM素养与提示培训)": "The teacher ensured that students used ChatGPT as a homework strategy outside the classroom by explaining specific instructions, criteria, frequencies, and procedures for using the chatbot and by monitoring student progress and providing feedback; no separate, extensive AI literacy curriculum was described beyond this embedded guidance.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Both groups used the same textbook, “English Textbook for Economics Students” (developed in 2019), and had the same teacher. Control group: traditional face-to-face instruction using the textbook and additional practice exercises assigned as homework. Experimental group: same textbook-based lectures plus ChatGPT as an AI chatbot exclusively for receiving feedback on writing; the teacher assigned online writing exercises and encouraged students to use ChatGPT during their writing process to seek feedback, reflect on revisions, and notice new sentence structures, clauses, synonyms, and vocabulary; the teacher provided specific instructions on how often and how to use ChatGPT, monitored learners’ interactions, and consistently tracked students’ progress while also giving feedback. Turnitin was used to check the originality of students’ texts to ensure they were free from AI-generated content. After the 16-week intervention, both groups took a post-test consisting of two 60-minute writing tasks and completed post-test questionnaires on writing feedback literacy and engagement; eight students were later interviewed.",
		"experimental_group_intervention (实验组干预内容)": "Experimental class (n=25) used the Economic English textbook and attended the same lectures as the control class but additionally used ChatGPT as an AI chatbot exclusively for receiving feedback on writing. The teacher assigned online writing exercises and encouraged students to use ChatGPT during their writing process, guided them to reflect on what they gained from revisions (e.g., new formal sentence structures, clauses, synonyms, vocabulary), provided specific instructions and criteria for chatbot use (including frequency and procedures), monitored students’ interactions with ChatGPT, and tracked their progress while providing feedback. Students were expected to use the chatbot as a homework strategy to enhance their writing and feedback literacy.",
		"control_group_intervention (对照组干预内容)": "Control class (n=25) used the same “English Textbook for Economics Students” and received traditional face-to-face teaching without AI chatbot support; they completed practice exercises from the textbook as homework and were taught by the same instructor, but they did not use ChatGPT or other AI chatbots for writing feedback.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT was used by the experimental group during the writing process for homework-based online writing exercises, primarily at the drafting and revision stages to receive feedback and revise texts; post-test writing tasks were administered after the intervention to assess outcomes, with no detailed description of ChatGPT use during the final post-test tasks.",
		"writing_genre (写作体裁)": "English writing tasks in an Economic English course; writing learning outcomes were assessed in terms of language, organization, communicative achievement, and content; specific genres or task prompts for the two 60-minute writing tasks were not reported.",
		"writing_task_type (写作任务类型)": "Two 60-minute English writing tasks used as post-test to evaluate writing learning outcomes, scored analytically for vocabulary, sentence structure, organization and coherence, and format and substance; task types and topics were not specified.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "In the experimental group ChatGPT served as an AI chatbot providing automated feedback on students’ writing, including language usage and structural aspects; it was employed to support self-directed learning and to help students reflect on revisions and new linguistic resources; ChatGPT was not intended to generate complete texts for students and AI content detection (Turnitin) was used to ensure students’ texts were free from AI-generated content; ChatGPT did not perform grading in this study.",
		"role_instructor (教师角色与介入方式)": "The same teacher taught both control and experimental classes, delivering textbook-based lectures, assigning writing tasks, and assessing students’ work. For the experimental group, the teacher additionally encouraged and guided ChatGPT use, provided specific instructions and criteria for using the chatbot (including frequency and procedures), required students to consider what they learned from their revisions (e.g., new sentence structures and vocabulary), ensured students used ChatGPT as a homework strategy, monitored their interactions with the chatbot, tracked their progress, and provided feedback; the teacher also used Turnitin to check originality of students’ texts.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Economic English course at a private university in Indonesia; face-to-face classroom instruction with traditional textbook-based teaching in both groups; experimental group additionally used ChatGPT online as homework for writing feedback.",
		"ethical_consideration (伦理审查与知情同意)": "NR",
		"llm_access_policy (LLM使用规范_允许与限制)": "Only the experimental group was allowed to use ChatGPT, and solely for receiving feedback on writing; ChatGPT was to be used as a tool to provide feedback and support self-directed learning, not to generate final texts; Turnitin AI content detection software was used to ensure students’ submitted texts were free from AI-generated content, emphasizing the requirement to maintain originality and avoid direct copying from ChatGPT.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "The main safeguard reported was the use of AI content detection software (Turnitin) to verify that students’ texts were free from AI-generated content, aiming to prevent plagiarism or over-reliance on generated text; teacher monitoring of chatbot use and progress further supported appropriate use; no additional technical content filtering or explicit safety parameters for ChatGPT were described.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Quantitatively, the experimental group using ChatGPT as an AI chatbot for writing feedback significantly outperformed the control group on writing learning outcomes, writing feedback literacy, and writing engagement. Writing outcomes: experimental mean=85.92 vs control mean=77.16, with t-tests indicating a significant difference (p=0.000). Writing feedback literacy: experimental mean=86.48 vs control mean=77.28, also significantly higher for the experimental group (p=0.000). Writing engagement: experimental mean=86.88 vs control mean=76.96, again significantly higher for the experimental group (p=0.000). These results indicate that AI chatbot implementation was more effective than traditional methods in enhancing writing outcomes, feedback literacy, and engagement. Qualitative findings showed that students used ChatGPT for interactive writing practice, personalized learning plans, guidance on reading materials and writing tools, acting on feedback via specific steps suggested by the chatbot, participating in peer feedback mediated by chatbots, engaging in gamified learning elements, benefiting from interactive lessons, and using chatbots to help build a sense of community; these strategies contributed to improved writing skills, feedback literacy, and engagement."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "The study demonstrates that implementing ChatGPT as an AI chatbot significantly improved EFL university students’ writing learning outcomes, writing feedback literacy, and writing engagement compared to traditional instruction without AI; effectiveness was measured using an analytical writing test (two 60-minute tasks scored with a 0–100 rubric), a 20-item feedback literacy questionnaire on a 5-point Likert scale, a scenario-based Motivation and Engagement Scale with 0–100 ratings, and semi-structured interviews analyzed via thematic analysis.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Two 60-minute English writing tasks assessed by an analytical scoring rubric allocating 25 points each for vocabulary, sentence structure, organization and coherence, and format and substance (total score range 0–100), with inter-rater reliability procedures applied.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Writing performance focused on language, organization, communicative achievement, and content; analytically, it was broken down into vocabulary, sentence structure, organization and coherence, and format and substance, reflecting overall writing quality.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Motivation and Engagement Scale adapted from Martin (2009) for writing, used in a scenario-based, event-level format where students rated their engagement on a 0–100 scale in three scenarios; the adapted scale showed high reliability (Cronbach’s alpha=0.92).",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Emotional and motivational aspects of writing engagement, including enjoyment, motivation, and affective involvement in writing tasks as captured by the Motivation and Engagement Scale scenarios.",
		"cognitive_aspect_measure (认知因素测量工具)": "Writing feedback literacy questionnaire with 20 items on a 5-point Likert scale, covering taking action, managing emotions, making judgments, identifying various feedback sources, and valuing feedback, adapted from previous feedback literacy research; analytical writing test scores also reflect cognitive aspects of writing performance.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Students’ writing feedback literacy, including their understanding of feedback’s significance, ability to interpret and evaluate feedback, capacity to act upon feedback, and awareness of different feedback sources; cognitive engagement with feedback as a process, as well as the development of writing knowledge as reflected in writing test performance.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Scenario-based event-level method using the adapted Motivation and Engagement Scale to capture behavioral engagement in writing (e.g., participation, task focus, persistence) through 0–100 ratings averaged across scenarios; qualitative data from interviews further described behavior, such as interactive practice, acting on feedback, and peer collaboration with chatbots.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Writing engagement behaviors including active participation in writing tasks, sustained focus, use of AI chatbots for practice and feedback, involvement in peer review sessions, and engagement in gamified and community-based learning activities mediated by chatbots.",
		"other_outcomes_measure (其他结果测量工具)": "Semi-structured interviews with eight students (four from each group), audio-recorded, transcribed, member-checked, and analyzed using grounded theory and thematic analysis with interrater reliability checks (correlation r≈0.86, Cohen’s Kappa≈0.92).",
		"other_outcomes_focus (其他结果维度说明)": "Qualitative outcomes included students’ reported strategies for using AI chatbots to improve writing learning outcomes, feedback literacy, and engagement, such as interactive writing practice, personalized learning plans, guidance on reading materials, acting on feedback via chatbot-suggested steps, peer feedback sessions supported by chatbots, gamified learning elements, interactive lessons, and community-building activities.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "A pretest was used to confirm that the two classes had comparable English writing abilities, but main outcomes were evaluated at a single post-test timepoint after completion of the 16-week intervention; writing outcomes, feedback literacy, and engagement were all measured post-intervention; interviews were conducted following these assessments.",
		"primary_outcome_variables (主要结果变量_因变量)": "Post-test writing learning outcome scores (0–100) from the analytical writing rubric; post-test writing feedback literacy scores (20-item Likert questionnaire); post-test writing engagement scores (scenario-based Motivation and Engagement Scale ratings).",
		"independent_variables_and_factors (自变量与实验因素)": "Instructional condition (implementation of AI chatbot vs traditional instruction) with two levels: experimental class using ChatGPT for writing feedback and control class taught without AI chatbots; no other experimental factors were manipulated.",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Writing learning outcomes: independent samples t-test showed a significant difference between experimental and control classes, with Sig. (2-tailed)=0.000 and mean difference=8.76 points; Levene’s test indicated F=3.57, Sig.=0.065, and the t-test results were t(48)=6.028, p=0.000, 95% CI [5.83, 11.68]. Writing feedback literacy: t-test showed a significant difference with Sig. (2-tailed)=0.000 and mean difference=9.20; Levene’s test F=8.70, Sig.=0.005; t(48)=6.486, p=0.000, 95% CI [6.34, 12.05]. Writing engagement: t-test indicated a significant difference with Sig. (2-tailed)=0.000 and mean difference=9.92; Levene’s test F=3.21, Sig.=0.079; t(48)=7.500, p=0.000, 95% CI [7.26, 12.57]. In all three domains, the experimental group significantly outperformed the control group.",
		"effect_size_summary (效应量摘要)": "The authors reported t-values, mean differences, standard errors, and 95% confidence intervals for group comparisons but did not report standardized effect sizes such as Cohen’s d or eta squared; quantitative results indicate substantial mean differences favoring the experimental group across writing outcomes, feedback literacy, and engagement, but exact effect size statistics were not provided.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "No direct negative effects or misuse of ChatGPT were reported; to prevent overreliance on AI-generated text and to safeguard academic integrity, the researchers used AI content detection software (Turnitin) to ensure students’ texts were free from AI-generated content; the discussion emphasizes that AI chatbots should be designed to enhance, not replace, human skills and teacher feedback, and that students need sufficient understanding of writing feedback to interpret and use AI suggestions effectively; the study recommends using AI chatbots alongside face-to-face instruction rather than as a standalone replacement.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The study did not report subgroup analyses by gender, proficiency level, or other demographic variables; the authors note that as AI chatbots become more accepted and popular, researchers should consider cultural differences in education when applying these tools; no detailed equity-focused statistics or differential impact analyses were presented.",
		"limitation (研究局限)": "The designs and strategies discussed are based specifically on ChatGPT as an AI chatbot and may not transfer directly to other chatbots with different characteristics; methods to enhance feedback literacy should be tailored to the particular educational context and chatbot implementation; AI chatbot adoption at different learning levels may be affected by curriculum changes; the area of AI chatbot-supported writing feedback literacy is relatively new, and this study is grounded in a limited literature base rather than an extensive prior evidence base; additional contextual limitations such as sample size and setting are implied but not explicitly discussed.",
		"challenge (实施挑战与风险)": "Challenges include the need to tailor AI chatbot-based feedback literacy methods to different course topics and objectives, the dependence of AI chatbot implementation on curricular structures and changes, and the practical requirements for adequate resources, infrastructure, and training for successful integration; the study also notes that while AI chatbots can provide rapid, personalized feedback, students still need foundational feedback literacy to interpret and apply this feedback, and AI chatbots should be used alongside teacher-led instruction to balance quick assistance with human expertise.",
		"future_work (未来研究方向)": "Future research should provide further empirical evidence on how AI chatbots enhance feedback literacy among EFL learners, consider cultural differences in educational contexts when implementing AI chatbots, and explore how teachers can use AI chatbots to build English skills frameworks and high-quality feedback practices; studies are encouraged to examine different course topics and learning levels, develop feedback literacy methods tailored to specific AI chatbot applications, and investigate how integrating ChatGPT into language and writing courses, including peer review and collaborative activities, can foster sustained writing development and community-building.",
		"implication (理论与教学实践启示)": "The study suggests that ChatGPT as an AI chatbot can meaningfully enhance EFL students’ writing learning outcomes, engagement, and feedback literacy by providing personalized, precise, and prompt feedback that complements teacher feedback; AI chatbots can bridge gaps between learners’ expectations and traditional feedback, support dialogic teaching, and position learners as active participants in feedback processes; integrating technology-enhanced feedback with face-to-face instruction allows teachers to save time and focus on other essential tasks while helping students develop effective writing habits and techniques; ChatGPT is presented as a valuable tool for supporting students in understanding and acting on feedback during the writing process when used within a well-resourced, well-trained, and pedagogically grounded environment."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Directorate General of Higher Education, Ministry of Education and Culture of the Republic of Indonesia.",
		"conflict_of_interest (利益冲突声明)": "NR",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Assignment to experimental and control groups was based on intact classes selected via purposive sampling rather than individual randomization; pretest writing results were used to ensure that the two classes had comparable English writing abilities before designating them as control and experimental groups; both groups were taught by the same teacher, which helps control instructor-related bias but does not eliminate potential selection biases due to non-random assignment.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "There is no report of blinding for participants, instructors, or raters regarding group assignment; the same teacher taught both groups and likely knew which class was assigned to use ChatGPT; rater blinding for writing assessments was not discussed.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The study reports that each class consisted of 25 students and presents post-test data for 25 students in each group; there is no discussion of participant attrition or missing data during the 16-week intervention or how any missing data may have been handled.",
		"reporting_transparency (报告透明度与可重复性)": "The article clearly reports the study aims, research questions, sampling criteria, group sizes, course context, use of the same textbook and teacher for both groups, inclusion of ChatGPT for feedback in the experimental group, instruments for writing outcomes, feedback literacy, and engagement, and key statistical results with t-values, significance levels, and confidence intervals; reliability coefficients for questionnaires and qualitative coding are reported; however, detailed prompts used with ChatGPT, full scoring rubrics, and raw data are not provided, and no data-sharing statement is included.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study identifies ChatGPT as the AI chatbot but does not specify the underlying GPT version or deployment details; the authors note that designs and strategies are based on ChatGPT’s fundamental principles and that characteristics of different AI chatbots may vary depending on course objectives and context, implying that findings may not be fully reproducible with other chatbot systems or future ChatGPT versions.",
		"baseline_equivalence (基线等同性检验)": "Baseline equivalence was addressed by administering a pretest of English writing ability, with results indicating no significant differences between the two classes selected as control and experimental groups; detailed pretest statistics are not reported, but the study states that both groups had comparable writing abilities at the outset.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Normality and homogeneity tests were conducted before performing independent samples t-tests; Levene’s tests for equality of variances are reported in the tables; no further information about normality tests or other diagnostics is provided.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Post-test scores for writing outcomes, feedback literacy, and writing engagement were summarized using means and standard deviations and analyzed with independent samples t-tests; writing engagement scores were calculated as averages across three scenario-based 0–100 ratings; no additional data transformations or sensitivity analyses were described."
	}
}