{
	"Basic Identification": {
		"author (作者)": "Yang Yang, Supyan Hussin, Harwati Hashim",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "China (Chinese public university)",
		"journal_name (期刊名称)": "Forum for Linguistic Studies",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article",
		"doi_or_identifier (DOI或唯一标识)": "https://doi.org/10.30564/fls.v7i7.9845",
		"research_aims (研究目的与问题)": "To compare ChatGPT-supported feedback with teacher feedback in an EFL writing course in terms of revision productivity, revision types (surface vs meaning-level, microstructure, macrostructure), writing self-efficacy, and learners’ cognitive and affective engagement with feedback.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Prior AI-assisted writing research has mainly examined surface-level accuracy gains or anxiety reduction and rarely traced how ChatGPT feedback shapes behavioral, cognitive, and affective dimensions of revision over multiple tasks compared to teacher feedback; this study addresses that gap using a quasi-experimental mixed-methods design and a multidimensional feedback engagement framework."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "First-year undergraduate students",
		"language_proficiency (语言熟练度水平)": "Intermediate EFL learners approximately aligned with CEFR B1 and approaching the CET-4 benchmark",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL in a Chinese public university College English Writing I course",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Non-English majors enrolled in College English Writing I at an undergraduate university in China",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental mixed-methods design with two intact classes assigned to a ChatGPT-supported feedback group and a teacher-feedback group over an 11-week intervention including pretest, three draft–feedback–revision writing cycles, and posttest.",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods (quantitative analyses of revisions and self-efficacy plus qualitative thematic analysis of interviews).",
		"sampling_method (抽样方法)": "Convenience sampling of two intact first-year College English Writing I classes at a Chinese public university.",
		"sample_size_and_effect (样本量及效应量)": "Total N=82; ChatGPT group n=40, teacher group n=42 after excluding two students (one missing pretest, one prolonged absence). Baseline comparison showed no significant difference in average number of language-related errors (ChatGPT M=15, SD=3; teacher M=14, SD=3) and similar feedback coverage (ChatGPT 93.0%, teacher 92.1%). Revision productivity: ChatGPT group made more revisions for all tasks (Task 1: 11.55 vs 7.67; Task 2: 13.20 vs 8.88; Task 3: 14.82 vs 10.26; Mann–Whitney U=387.5, 356.0, 340.0 respectively, all p<0.001). Revision types: ChatGPT group produced significantly more microstructure and macrostructure changes across tasks with large effect sizes (r≈0.50–0.72), while formal and meaning-preserving revisions did not differ significantly (p>0.05). Writing self-efficacy mixed-design ANOVA showed significant time × group interactions for SRE and DSS (F(1,80)=10.55, p=0.002, partial η²=0.117) and for WCS (F(1,80)=27.29, p<0.001, partial η²=0.254). Within the ChatGPT group, pre–post gains had very large Cohen’s d (SRE≈2.56, DSS≈2.68, WCS≈2.60), compared with medium-to-large effects in the teacher group (SRE≈0.93, DSS≈1.39, WCS≈0.86).",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in a multidimensional feedback engagement framework (behavioral, cognitive, affective) informed by sociocognitive theory (self-efficacy and self-regulated revision) and sociocultural theory (mediated scaffolding and zone of proximal development), positioning ChatGPT as a dialogic scaffold that can support self-regulated, meaning-oriented revision.",
		"data_collection_instrument (数据收集工具)": "Writing self-efficacy questionnaire adapted from Zhan and Yan with three subscales (Substantive Revision Efficacy, Discourse Synthesis Self-Efficacy, Writing Conventions Self-Efficacy); pretest and three in-class argumentative writing tasks with specified prompts; structured revision coding scheme (formal changes, meaning-preserving changes, microstructure changes, macrostructure changes); semi-structured interview protocol focusing on feedback understanding, usefulness, emotional reactions, and revision decision-making; Chaoxing online platform for distributing materials and collecting assignments; screenshots of ChatGPT interactions in the experimental group.",
		"data_collection_validity_reliability (工具信度与效度)": "The writing self-efficacy instrument demonstrated high internal consistency (Cronbach’s α=0.946–0.963 across subscales) and strong construct validity (three-factor structure with good fit indices CFI=0.987, TLI=0.985, RMSEA=0.042) and established discriminant and criterion validity. Four expert raters coded pretest errors and feedback coverage and later revision types with high inter-rater reliability (Cohen’s κ=0.87 for pretest error and feedback coverage coding; κ=0.81 for revision categorization). Baseline analyses confirmed comparable error loads and feedback coverage across groups. All writing tasks were done under exam-like, supervised conditions using the same instructor, syllabus, and materials for both groups.",
		"data_analysis_method (数据分析方法)": "Non-parametric Mann–Whitney U tests for between-group comparisons of revision productivity and revision types due to non-normal data; mixed-design (two-way repeated measures) ANOVA for self-efficacy scores with time (pre/post) as within-subject factor and group as between-subject factor; additional paired-samples t-tests within groups and independent-samples t-tests between groups at posttest; effect sizes reported as Cohen’s d, partial η², and non-parametric r; thematic analysis of interview transcripts following Braun and Clarke’s framework to derive themes related to feedback clarity, strategic engagement, and dialogic scaffolding.",
		"unit_of_analysis (分析单位)": "Individual students and their texts; revision counts and types per essay; self-efficacy scores at the participant level; interview data at the individual case level.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Non-random assignment of two intact classes to experimental (ChatGPT-supported feedback) and control (teacher feedback) conditions, with both groups taught by the same instructor using identical instructional materials.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of two intact classes of first-year non-English majors in a required College English Writing I course at one Chinese public university; convenience sampling limits representativeness to similar institutional and curricular contexts.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Four expert raters independently coded baseline writing for number of language errors and feedback coverage, and coded revisions in intervention tasks into formal, meaning-preserving, microstructure, and macrostructure changes; inter-rater agreement was high (κ≈0.87 and κ≈0.81); raters were described as expert but specific training procedures beyond use of the coding scheme were not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "11 weeks in total: Week 1 pretest (self-efficacy questionnaire and baseline writing task); Weeks 2–4 Task 1 (draft, feedback, revision); Weeks 5–7 Task 2 (draft, feedback, revision); Weeks 8–10 Task 3 (draft, feedback, revision); Week 11 posttest self-efficacy questionnaire and interviews.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (large language model; specific version not specified)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT was accessed via an online interface during supervised in-class sessions between Weeks 2 and 10; students in the experimental group used a standardized role-based prompt instructing ChatGPT to act as an English writing teacher and provide clear bullet-point revision suggestions on grammar/word use, clarity of meaning, paragraph structure, and overall coherence without rewriting the essay; students could ask follow-up questions in English or Chinese; interactions were captured via screenshots; API access, model parameters, and exact version identifiers were not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students in the experimental group interacted directly with ChatGPT in a browser or similar interface as a stand-alone feedback tool during the revision stage, under teacher supervision, with no teacher mediation of the AI output and no integration into the LMS beyond uploading screenshots.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A standardized role prompt framed ChatGPT as an English writing teacher and requested clear, specific bullet-point revision suggestions focused on grammar and lexical choices, clarity of meaning, paragraph structure, and overall coherence, explicitly instructing the model not to rewrite the essay; brief training on effective prompt usage encouraged students to use this prompt and to pose follow-up questions when needed.",
		"training_support_llm_literacy (LLM素养与提示培训)": "The ChatGPT group received brief in-class training on how to use ChatGPT for revision, including how to apply the standardized prompt, how to ask clarifying questions, and how to respect institutional rules that restricted ChatGPT to feedback only; AI-use regulations were reiterated at the start of each session; no extended critical AI literacy or citation training was reported.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Week 1: both groups completed a writing self-efficacy questionnaire and a baseline argumentative essay under exam-like conditions (30 minutes, no AI or peer support), which was used for expert-coded comparisons of error load and feedback coverage. Weeks 2–10: for each of three argumentative writing tasks, all students drafted essays in class, then received feedback and revised; the only systematic difference was feedback source. The ChatGPT group used ChatGPT, with students submitting screenshots of AI interactions, whereas the teacher group received teacher annotations and short small-group clarification sessions before revising. All materials and submissions were managed via the university’s Chaoxing platform. Week 11: all students completed the post-intervention self-efficacy questionnaire and 14 purposively selected students (7 per group) participated in semi-structured interviews.",
		"experimental_group_intervention (实验组干预内容)": "Experimental group (n=40) drafted essays for each task in class and then obtained feedback exclusively from ChatGPT using the standardized prompt, engaging in multi-turn interactions as needed; feedback focused on grammar and word use, clarity, paragraph structure, and coherence, and was used to guide their revisions; teacher presence was for supervision and rule enforcement, not for direct feedback on drafts.",
		"control_group_intervention (对照组干预内容)": "Control group (n=42) drafted the same essays and received conventional teacher feedback consisting of written annotations on organization, clarity, coherence, and language use on their drafts, supplemented by brief (3–5 minute) small-group clarification sessions delivered primarily in Mandarin, after which they revised; they had no access to ChatGPT or other AI tools during the intervention.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "ChatGPT and teacher feedback were integrated at the revision stage after an initial draft had been produced; students first drafted under timed, supervised conditions and then used either ChatGPT or teacher comments to revise and improve their texts; the pretest baseline essay involved drafting only without a revision cycle used in the outcome analyses.",
		"writing_genre (写作体裁)": "Short argumentative essays on course-related topics, including issues such as whether college education should be free, whether college students should take part-time jobs or focus on studies, why young people prefer online shopping and its effects, and the impacts and solutions for excessive social media use.",
		"writing_task_type (写作任务类型)": "Timed in-class argumentative writing tasks (approximately 150–200 words for the pretest, similar length implied for intervention tasks) completed individually under exam-like conditions, followed by feedback and revision cycles for three intervention tasks.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT functioned as an automated writing feedback provider in a dialogic mode, giving qualitative, bullet-point suggestions on grammar, word choice, clarity, paragraph structure, and coherence, and responding to follow-up questions; it did not assign scores and was explicitly restricted from rewriting essays or generating full text.",
		"role_instructor (教师角色与介入方式)": "The same instructor taught both classes, delivered identical writing instruction and materials, provided written and oral feedback to the teacher group, supervised in-class writing and ChatGPT use in the experimental group, enforced AI-use policies, and verified compliance by checking screenshots; the instructor did not provide written draft-level feedback to the ChatGPT group.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Face-to-face College English Writing I course at an undergraduate university in China, supported by the Chaoxing online learning platform for distributing materials and collecting assignments; all writing, feedback, and revision activities were carried out in-class under teacher supervision.",
		"ethical_consideration (伦理审查与知情同意)": "The study reports that ethical approval was obtained and that all participants signed informed consent forms; anonymity and confidentiality were guaranteed and participation was voluntary; an Institutional Review Board statement of 'Not applicable' is also noted.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students in the ChatGPT group were explicitly instructed to use ChatGPT only for revision feedback, not for full-text generation; all writing tasks were completed in class under supervision; AI-use rules were reinforced at each session; students were required to submit screenshots of their ChatGPT interactions to verify compliance with these institutional AI-use policies.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "Safety and integrity were managed procedurally by restricting ChatGPT use to supervised classroom revision, prohibiting full-text generation, and requiring screenshot evidence of interactions; no additional technical content filtering or safety configuration of the model was reported.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Compared to teacher feedback, ChatGPT-supported feedback led to significantly higher revision productivity across all three writing tasks and promoted more meaning-oriented revisions, particularly microstructure and macrostructure changes, while both groups made similar numbers of formal and meaning-preserving surface changes; the ChatGPT group also showed significantly larger gains in all three dimensions of writing self-efficacy (substantive revision, discourse synthesis, and writing conventions), with large effect sizes; qualitative data indicated that students perceived ChatGPT feedback as clearer, more actionable, and more dialogic, which fostered revision ownership, strategic engagement, and confidence, whereas teacher feedback was valued for credibility but experienced as less interactive and sometimes vague, leading to more surface-level revision."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of ChatGPT-supported feedback was assessed using expert-coded revision productivity and revision types across three writing tasks together with pre/post writing self-efficacy scores; the results show that ChatGPT significantly increased revision frequency, especially content-based and macrostructural changes, and yielded larger improvements in writing self-efficacy than teacher feedback, suggesting stronger behavioral, cognitive, and affective engagement with writing revision.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Expert-coded measures of writing-related performance included the number of language-related errors in the baseline pretest and, for each intervention task, counts of four revision types (formal changes such as grammar and punctuation, meaning-preserving lexical changes, microstructure changes at sentence or local content level, and macrostructure changes at discourse and paragraph level); no holistic or analytic writing score rubric was used as a primary outcome.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Focus was on revision behavior and depth of revision as indicators of writing development, particularly the shift from surface-level formal changes to meaning-level microstructure and macrostructure revisions, alongside initial language accuracy (pretest error counts); there was no direct assessment of fluency, syntactic complexity indices, or holistic text quality scores.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Writing self-efficacy questionnaire adapted from Zhan and Yan with three subscales—Substantive Revision Efficacy (SRE), Discourse Synthesis Self-Efficacy (DSS), and Writing Conventions Self-Efficacy (WCS)—administered at pretest and posttest; each subscale demonstrated high internal consistency (Cronbach’s α=0.946–0.963) and strong construct validity.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Writing self-efficacy regarding substantive revision (confidence in revising content and ideas), discourse synthesis and organization (confidence in structuring arguments and integrating information), and writing conventions (confidence in applying grammar, vocabulary, and mechanics); interviews also explored emotional responses to feedback, perceived confidence, and feelings of agency and security when revising.",
		"cognitive_aspect_measure (认知因素测量工具)": "Semi-structured interviews probing how students understood, interpreted, and acted on feedback, including questions about feedback clarity, specificity for complex issues like argument flow and paragraph structure, perceived interaction with feedback, and changes in revision behavior; revision type coding (particularly microstructure and macrostructure changes) served as a behavioral indicator of deeper cognitive processing of feedback.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Learners’ interpretation of feedback, strategic use of feedback in reorganizing and elaborating ideas, metacognitive monitoring of text structure and argumentation, sense of revision ownership, experimentation with alternative expressions and structures, and perceptions of dialogic scaffolding and clarity in ChatGPT- versus teacher-supported environments.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Counts of total revisions per task for each student; counts of formal, meaning-preserving, microstructure, and macrostructure revisions; baseline counts of language errors and feedback coverage; screenshot artifacts of ChatGPT interactions verifying revision-focused AI usage.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Revision productivity across three tasks (frequency of changes), distribution of revision types (surface vs content-level; micro vs macrostructural), patterns of increasing revision sophistication over time in the ChatGPT group, evidence of development of revision habits, and behavioral engagement with feedback as reflected in the number and depth of changes made after receiving ChatGPT or teacher feedback.",
		"other_outcomes_measure (其他结果测量工具)": "NR",
		"other_outcomes_focus (其他结果维度说明)": "NR",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pretest in Week 1 (self-efficacy questionnaire and baseline argumentative essay without AI or peer support); three successive writing tasks with draft–feedback–revision cycles during Weeks 2–10; posttest self-efficacy questionnaire and interviews in Week 11; no delayed follow-up beyond the 11-week course.",
		"primary_outcome_variables (主要结果变量_因变量)": "Total number of revisions per task; counts of formal changes, meaning-preserving changes, microstructure changes, and macrostructure changes; self-efficacy scores on Substantive Revision Efficacy, Discourse Synthesis Self-Efficacy, and Writing Conventions Self-Efficacy at pretest and posttest.",
		"independent_variables_and_factors (自变量与实验因素)": "Feedback condition (ChatGPT-supported vs teacher-supported), time/task (three writing tasks and pre/post measurement), and their interaction; baseline language error counts and feedback coverage used to assess initial equivalence but not treated as manipulated factors.",
		"followup_length_and_type (随访时长与类型)": "Short-term follow-up within an 11-week course; outcomes assessed immediately after the intervention with no extended delayed posttest.",
		"statistical_significance (统计显著性结果摘要)": "Baseline comparisons showed no statistically significant group differences in number of language errors or feedback coverage. For revision productivity, Mann–Whitney U tests showed significant advantages for the ChatGPT group in all three tasks (Task 1: U=387.5, p<0.001; Task 2: U=356.0, p<0.001; Task 3: U=340.0, p<0.001). For revision types, ChatGPT-supported students made significantly more microstructure and macrostructure changes than teacher-supported students across tasks (all p<0.001), while differences in formal and meaning-preserving changes were non-significant (p>0.05). Mixed-design ANOVAs for self-efficacy showed significant main effects of time (all p<0.001) and significant time × group interaction effects for SRE and DSS (F(1,80)=10.55, p=0.002, partial η²=0.117) and for WCS (F(1,80)=27.29, p<0.001, partial η²=0.254), indicating that the ChatGPT group improved significantly more than the teacher group across all self-efficacy dimensions; subsequent tests confirmed significant pre–post gains within both groups (ps<0.001) and large between-group differences favoring the ChatGPT group at posttest.",
		"effect_size_summary (效应量摘要)": "Revision-type analyses reported large non-parametric effect sizes for ChatGPT-supported gains in microstructure and macrostructure changes (r≈0.50–0.72). Self-efficacy ANOVAs yielded substantial interaction effect sizes (partial η²≈0.117 for SRE and DSS; partial η²≈0.254 for WCS). Within-group pre–post self-efficacy improvements in the ChatGPT group were very large (Cohen’s d≈2.56, 2.68, and 2.60 for SRE, DSS, and WCS respectively), compared to medium-to-large effects in the teacher group (d≈0.93, 1.39, 0.86). Between-group posttest self-efficacy differences were also large (Cohen’s d≈2.10–2.27 depending on subscale).",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study reports that explicit rules and supervision, including in-class monitoring and required screenshots, effectively minimized unauthorized AI-generated writing and ensured that ChatGPT was used only for revision feedback; no instances of plagiarism or severe misuse are mentioned. The authors note in the discussion that future research should examine ethical concerns around possible overreliance on AI tools and the need for critical feedback literacy, but no major negative effects of ChatGPT use were reported within this intervention.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "Groups were checked for baseline equivalence in language error load and feedback coverage, but no subgroup analyses by gender, proficiency level, or other demographic variables were reported; equity and subgroup effects are not discussed beyond ensuring pre-intervention comparability of the two intact classes.",
		"limitation (研究局限)": "Limitations include the quasi-experimental design with non-random assignment of intact classes; a single institutional context involving first-year non-English majors at one Chinese university; focus on a limited set of argumentative writing tasks and on revision measures and self-efficacy rather than holistic writing scores or long-term writing development; reliance on self-report and interview data for affective and cognitive engagement; and the use of ChatGPT alone, with the authors acknowledging that other generative AI tools (e.g., DeepSeek or KIMI) may differ in interactional capabilities, which constrains generalizability.",
		"challenge (实施挑战与风险)": "Implementation challenges highlighted include the need to ensure that students comply with AI-use policies restricting ChatGPT to revision feedback rather than text generation, the burden of monitoring and verifying usage through screenshots, potential variability in students’ AI literacy and ability to craft effective prompts, and broader concerns about teacher readiness to integrate generative AI tools and avoid overreliance while maintaining pedagogical control.",
		"future_work (未来研究方向)": "The authors recommend future research on how learners with different proficiency levels, learning goals, and feedback preferences respond to AI-mediated revision support; longitudinal studies tracking how repeated engagement with dialogic AI feedback shapes genre competence and writing development over longer periods; comparative work involving other generative AI platforms beyond ChatGPT; and investigations into how students develop critical feedback literacy and reflective authorship when interacting with AI tools.",
		"implication (理论与教学实践启示)": "The findings suggest that ChatGPT can function as a cognitive and emotional scaffold in EFL writing by providing clear, elaborated, and interactive feedback that promotes greater revision productivity, deeper content-level changes, and enhanced writing self-efficacy, supporting theories of feedback engagement, self-regulation, and mediated learning; pedagogically, the authors advocate integrating generative AI into process-based writing instruction as a complement rather than a replacement for teacher feedback, using AI to support lower-stakes, iterative revision cycles while teachers focus on higher-order concerns, and designing tasks that foster feedback literacy so learners critically interpret and strategically adapt AI feedback instead of passively accepting it."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "The authors state that the work received no external funding.",
		"conflict_of_interest (利益冲突声明)": "The authors declare no conflict of interest.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Because intact classes were conveniently sampled and non-randomly assigned to conditions, there is risk of selection bias, although baseline comparisons on language errors and feedback coverage suggested initial equivalence; both groups were taught by the same instructor, which reduced but did not eliminate design-related confounds.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Teachers and students were aware of group assignments and feedback source; expert raters who coded errors, feedback coverage, and revision types were not reported as blinded to condition; self-efficacy and interview data were self-reported, so no blinding applied.",
		"attrition_and_missing_data (流失与缺失数据处理)": "Two participants were excluded from the initial sample (one in the experimental group due to missing pretest, one in the control group due to prolonged absence), resulting in final Ns of 40 and 42; beyond these exclusions, no further attrition rates, missing data patterns, or imputation methods were reported.",
		"reporting_transparency (报告透明度与可重复性)": "The article reports the research questions, design, participant characteristics, instruments, feedback protocols, coding schemes, and main analytical procedures with key statistics and effect sizes; reliability and validity indices are provided for the self-efficacy instrument and inter-rater coding; timelines and group procedures are summarized in a table; raw data, coding manuals, and analysis scripts are not shared, and there is at least one internal inconsistency in the direction of reported group differences in one part of the self-efficacy analysis, which slightly limits transparency.",
		"preregistration_or_protocol (预注册或研究方案)": "The study was not reported as preregistered and no formal protocol registration is mentioned; the Institutional Review Board statement is reported as 'Not applicable.'",
		"llm_version_reproducibility (LLM版本与可复现性)": "The LLM is identified generically as ChatGPT and the study period and standardized prompt are described, but the specific model version, provider account, and configuration parameters are not reported; while screenshot records document the nature of interactions, evolving behavior of ChatGPT over time may limit exact reproducibility of AI-generated feedback.",
		"baseline_equivalence (基线等同性检验)": "Baseline analysis using pretest writing samples showed no statistically significant difference between groups in mean number of language-related errors (ChatGPT M=15, SD=3; teacher M=14, SD=3) and highly similar feedback coverage (93.0% vs 92.1%) according to four expert raters with high agreement (κ=0.87), supporting initial equivalence on writing accuracy demands and feedback exposure.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The authors note that revision-related data violated normality assumptions and thus used Mann–Whitney U tests; they report that self-efficacy variables met normality requirements for ANOVA; no additional diagnostics such as checks for homogeneity of variance, residual analyses, or robustness checks are described.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Pretest and intervention writing samples were coded by expert raters for errors, feedback coverage, and revision types according to predefined categories; self-efficacy subscale scores were computed from questionnaire responses (likely as means or sums of Likert items, though exact computation is not detailed); no further data transformations or outlier treatments are reported."
	}
}