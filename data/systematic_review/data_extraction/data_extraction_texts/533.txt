{
	"Basic Identification": {
		"author (作者)": "Sirin Sawangwan",
		"publication_year (发表年份)": "2024",
		"study_region (研究地区)": "Thailand",
		"journal_name (期刊名称)": "International Journal of Computer-Assisted Language Learning and Teaching",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; mixed-method quasi-experimental two-group pre-test–post-test comparison of ChatGPT-based revision versus teacher instruction for EFL writing",
		"doi_or_identifier (DOI或唯一标识)": "10.4018/IJCALLT.361235",
		"research_aims (研究目的与问题)": "To statistically compare the effects of ChatGPT versus teacher instruction on EFL learners’ English writing performance, examining pre- and post-revision scores in each condition, comparing revision outcomes between the ChatGPT and teacher instruction groups, and qualitatively identifying the roles of ChatGPT and teachers in developing EFL writing based on observational records.",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses ongoing controversy about whether AI tools like ChatGPT can replace or replicate teachers’ roles in EFL writing and clarifies the distinct contributions of ChatGPT versus teachers by directly comparing their impact on TOEFL-iBT-rated writing performance and by documenting teacher roles in understanding writing criteria, developing critical thinking, and providing ethical guidance."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Undergraduate EFL learners at a public university",
		"language_proficiency (语言熟练度水平)": "Learners had previously scored 17.5–18.5 out of 25 on the writing part of Fundamental English midterm and final exams; pre-test TOEFL-iBT writing raw scores averaged about 2.5 out of 5 (scaled score 17/30, mapped to CEFR B1 Intermediate), with post-revision scores reaching scaled 25 (ChatGPT) and 29 (teacher instruction), mapped to CEFR C1 Advanced.",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "NR",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL undergraduate English language course at a public university; learners had completed a compulsory Fundamental English course including writing.",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Various undergraduate programs at a public university; specific majors not systematically reported.",
		"prior_experience_llm (既有LLM使用经验)": "Participants were comfortable using computer-based tests; prior experience with ChatGPT or other LLMs was not reported explicitly."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental two-group pre-test–post-test design with a ChatGPT revision group and a teacher instruction revision group, combined with qualitative analysis of observational field notes.",
		"research_method (研究方法_定量_定性_混合)": "Mixed-method: quantitative analysis of TOEFL-iBT writing scores using paired-samples and independent-samples t-tests, and qualitative analysis of field notes following Merriam’s four-step process (reviewing, analyzing, categorizing, reporting).",
		"sampling_method (抽样方法)": "Purposive sampling; from approximately 1,000 learners in 40 sections, five sections with the highest numbers of learners scoring 17.5–18.5 out of 25 on the writing part of Fundamental English exams were identified, and 50 volunteer learners meeting this criterion were recruited and then divided into two groups.",
		"sample_size_and_effect (样本量及效应量)": "Total N=50 EFL undergraduate learners; ChatGPT group n=25, teacher instruction group n=25. ChatGPT group: pre-test mean=2.46 (SD=0.54), ChatGPT revision mean=3.92 (SD=0.96), mean gain=1.46; paired t(24)=−11.78, p<.001. Teacher instruction group: pre-test mean=2.57 (SD=0.40), revision mean=4.87 (SD=0.26), mean gain=2.30; paired t(24)=−24.72, p<.001. Independent-samples t-test on revision scores: ChatGPT mean=3.92 (SD=0.96), teacher mean=4.87 (SD=0.26), t(48)=−4.73, p<.001, mean difference=−0.94. TOEFL-iBT scaled scores improved from 17 to 25 (+8 points; 47.05% development) with ChatGPT and from 17 to 29 (+12 points; 70.59% development) with teacher instruction.",
		"theoretical_foundation (理论基础_理论框架)": "Based on Flower and Hayes’s (1980) writing process model (long-term memory, writing processor, task environment), Vygotsky’s (1978) sociocultural scaffolding and Zone of Proximal Development, Swain’s (1995) emphasis on process over product in L2 output, historical CALL frameworks (Levy; Warschauer), and the ISTE (2024) standards for educators’ technology competencies, with a focus on how ChatGPT and teachers function as processors and facilitators in EFL writing.",
		"data_collection_instrument (数据收集工具)": "300-word independent essay tasks modeled on TOEFL-iBT (topic: 'What is your favorite subject, and why?') written in 30 minutes using Microsoft Word; ETS TOEFL-iBT Independent Writing Rubrics (raw scores 0–5) focusing on development, organization, and language use; ChatGPT (OpenAI Version 3.5) for automated revision; observation records and field notes documenting errors, technical issues, and ethical issues; IBM SPSS Statistics v26 for quantitative analysis.",
		"data_collection_validity_reliability (工具信度与效度)": "Writing performance was assessed using official TOEFL-iBT Independent Writing rubrics from ETS to ensure construct validity, with three expert inter-raters (experienced in testing and teaching English writing) independently scoring pre-test essays and revised essays under both conditions; raters also provided detailed comments and feedback which were used for teacher instruction. Raw scores were converted to scaled TOEFL-iBT scores and then mapped to CEFR levels using published conversion tables; inter-rater reliability statistics were not reported.",
		"data_analysis_method (数据分析方法)": "Paired-samples t-tests (SPSS) compared pre-test and post-revision TOEFL raw scores within the ChatGPT group and within the teacher group; an independent-samples t-test compared revision scores between the two groups. Quantitative results were further expressed as scaled TOEFL-iBT scores and CEFR levels. Qualitative analysis of field notes followed Merriam’s four-step procedure (reviewing, analyzing, categorizing, reporting) to derive five report types: comparative error percentages, examples of writing development, misuse of gender pronouns, technical issues, and ethical issues.",
		"unit_of_analysis (分析单位)": "Individual learner essays (pre-test and revised versions) for quantitative scoring; groups of learners (ChatGPT vs teacher instruction) for between-group comparisons; individual essays and episodes in field notes for qualitative analysis.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "After purposive sampling based on prior writing exam scores, the 50 consenting learners were divided into two groups of 25 (ChatGPT group and teacher instruction group); the article does not report random assignment or allocation concealment.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame was approximately 1,000 EFL undergraduates enrolled in English courses at a public university, divided into 40 sections; five sections with many learners scoring 17.5–18.5/25 in writing on prior Fundamental English exams were targeted. From these, 50 volunteers meeting the score criterion were recruited. Representativeness beyond this single-institution EFL context was not discussed.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Three expert inter-raters graded all pre-test essays and revised essays using the TOEFL-iBT writing raw score rubric (development, organization, language use) and provided comments and feedback; their marked scores, comments, and feedback were returned to the researcher and used to compute mean scores and to structure teacher-guided revision. The article does not specify the exact procedure for combining rater scores or any formal rater training or calibration sessions, and no inter-rater reliability coefficients are reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Single-task intervention per group within one academic semester: all 50 learners wrote a 300-word independent essay in 30 minutes (pre-test) and then either had the essay revised once via ChatGPT (ChatGPT group) or revised once under teacher instruction (teacher group); revisions were scored immediately after each respective revision process.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (OpenAI, Version 3.5)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT Version 3.5 accessed through an online interface in a computer-based environment; each learner in the ChatGPT group was instructed to input the standardized prompt 'Revise this essay to 300 words, adhering to TOEFL iBT criteria: development, organization, and language use' followed by their pre-test essay; no additional settings (e.g., temperature, max tokens) or access dates were reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Direct learner use of ChatGPT as an external web-based revising tool; learners in the ChatGPT group copied their pre-test essays into ChatGPT with the given prompt and received AI-generated revised essays; ChatGPT was not embedded into an LMS and teachers did not intermediate the AI output, aside from providing the standard prompt and observing usage.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A single, uniform revision prompt was prescribed for all learners in the ChatGPT group: asking ChatGPT to revise the essay to 300 words according to TOEFL-iBT criteria for development, organization, and language use; learners generally used only this one-step prompt. The discussion notes that some issues (omitting the subject in the essay or failing to reach word count) might be mitigated by additional prompts such as specifying the subject in the topic or requesting expansion to a specific word count, but these more complex prompt strategies were suggested rather than implemented in the study.",
		"training_support_llm_literacy (LLM素养与提示培训)": "Learners were familiarized with computer-based tests and given explicit instructions on how to use the provided ChatGPT prompt to revise their essays, but there was no systematic training in AI literacy or prompt engineering; the study later notes that learners should be taught to craft more effective and complex prompts (e.g., explicitly including the essay topic or word-length requirements).",
		"intervention_implementation (干预实施流程_步骤与任务)": "Step 1: All 50 learners wrote a 300-word English essay in 30 minutes on the topic 'What is your favorite subject, and why?' using Microsoft Word (pre-test). Step 2: Three expert inter-raters scored all pre-test essays using TOEFL-iBT raw score criteria, adding comments and feedback. Step 3: In the ChatGPT group (25 learners), each participant submitted their pre-test essay to ChatGPT 3.5 with the standard revision prompt; field notes were taken on issues during ChatGPT use. Step 4: AI-generated revised essays from the ChatGPT group were scored again by the three inter-raters with comments and feedback. Step 5: Mean pre-test and ChatGPT revision scores were compared. Step 6: All comments and feedback from both pre-test and ChatGPT revisions were compiled by the research team. Steps 7–9: In the teacher instruction group (25 learners), compiled comments and feedback were returned; learners revised their essays under teachers’ guidance, with teachers training them in how to interpret and use the feedback to improve development, organization, and language use. Revised essays were then scored by the same three inter-raters and mean pre-test versus teacher-revision scores were compared. Finally, independent-samples t-tests compared revision scores between ChatGPT and teacher groups.",
		"experimental_group_intervention (实验组干预内容)": "ChatGPT group: after writing a 300-word pre-test essay, learners revised their essays by using ChatGPT Version 3.5 with a standard prompt requesting a 300-word revision aligned with TOEFL-iBT criteria for development, organization, and language use; the AI-generated versions were taken as the revised essays and scored by inter-raters.",
		"control_group_intervention (对照组干预内容)": "Teacher instruction group: after writing the same 300-word pre-test essay and being scored, learners received collated comments and feedback from the three inter-raters; their English writing teachers then provided instruction on how to revise based on this feedback, guiding learners in improving development, organization, and language use; learners revised their own essays under this teacher guidance, and the revised essays were scored by inter-raters.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Initial draft generation (pre-test independent essay writing) followed by a single revision stage: either revision via ChatGPT (automatic rewriting) or revision under teacher instruction (student-driven rewriting guided by feedback and teaching).",
		"writing_genre (写作体裁)": "Independent 300-word English essay responding to the prompt 'What is your favorite subject, and why?' (personal expository or opinion essay).",
		"writing_task_type (写作任务类型)": "Timed individual independent writing task (30 minutes, 300 words) for pre-test, followed by one-time revision tasks (ChatGPT-based revision or teacher-guided revision) of the same essay; all writing was done via computer using Microsoft Word.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT Version 3.5 functioned as an automated reviser and language assistant: it rewrote learners’ essays to improve grammar and vocabulary and partially improved development and organization in line with TOEFL-iBT criteria, but it sometimes failed to identify or explicitly respond to the essay’s subject, sometimes produced texts below the required word count, misused gender pronouns in some cases, and did not provide explicit instruction on writing criteria or ethical citation.",
		"role_instructor (教师角色与介入方式)": "Teachers and researchers selected participants, administered tasks, and provided technical support; three expert inter-raters scored essays and gave detailed comments and feedback. For the teacher instruction group, English writing teachers used these comments and feedback to explicitly instruct learners on understanding writing criteria, planning, organizing ideas, revising language, and addressing errors, as well as to foster critical thinking and ethical awareness in academic writing. Teachers also served as observers documenting technical and ethical issues in field notes.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Formal university EFL setting at a public university; writing and revision tasks were computer-based, using Microsoft Word and online ChatGPT access in a controlled environment under teacher supervision.",
		"ethical_consideration (伦理审查与知情同意)": "All learners were volunteers who gave consent to the ethical considerations of the study and expressed comfort with computer-based tests; the article reports that there were no conflicts of interest and no funding, but it does not detail formal institutional ethics approval procedures.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Within the study, learners in the ChatGPT group were permitted to use ChatGPT Version 3.5 only for revising their pre-test essays according to the provided prompt; broader institutional policies about ChatGPT use are discussed (e.g., bans and concerns about misuse), but specific restrictions beyond the defined revision task and teacher supervision are not described.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "No explicit technical safety guardrails or content filters for ChatGPT are reported; instead, the study frames teachers as responsible for addressing issues of cheating, academic integrity, plagiarism, and ethical citation, acknowledging that ChatGPT cannot independently provide ethical guidance or enforce integrity standards.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Both ChatGPT-based revision and teacher instruction significantly improved EFL learners’ TOEFL-iBT writing scores relative to pre-test performance, but teacher instruction produced higher post-revision scores and greater gains than ChatGPT. Paired t-tests showed significant improvements within each group (p<.001), and independent t-tests indicated that the teacher group’s revision scores (mean 4.87) were significantly higher than the ChatGPT group’s (mean 3.92). Scaled TOEFL-iBT scores increased from 17 (pre-test) to 25 (ChatGPT revision) and to 29 (teacher revision), corresponding to a shift from CEFR B1 to C1 in both groups, with a larger gain under teacher instruction. Error analysis revealed that ChatGPT substantially reduced errors in development (from 95% to 13%), organization (97% to 19%), and language use (99% to 4%), while teacher instruction reduced development and organization errors further (to 5% and 12%) and eliminated language use errors (0%). Qualitative findings showed that ChatGPT sometimes failed to identify or state the favorite subject in the essay, often under-produced the requested word count, misused gender pronouns, and did not provide proper citations, whereas teachers helped learners understand writing criteria, develop critical thinking, reorganize disorganized essays, correct grammar comprehensively, and address ethical citation and plagiarism issues. The study concludes that ChatGPT is a powerful tool for improving the product of EFL writing but cannot replicate the process-oriented, critical, and ethical roles of teachers; it recommends integrating ChatGPT into EFL writing curricula under teacher supervision and providing AI-related professional development for teachers."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Using TOEFL-iBT Independent Writing Rubrics (raw scores 0–5, scaled scores 0–30, mapped to CEFR levels), the study found that both ChatGPT-based revision and teacher instruction significantly improved EFL learners’ overall writing performance, with larger gains for teacher-guided revision; effectiveness was evaluated through pre-test versus post-revision score comparisons within each group, between-group comparisons of revision scores, scaled score conversions, CEFR level mapping, and comparative error analysis in development, organization, and language use.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "TOEFL-iBT Independent Writing Raw Score Rubrics (Educational Testing Service, 2023), with three primary criteria: development, organization, and language use; raw scores (0–5) were converted to scaled scores (0–30) and further compared with CEFR levels using published conversion schemes.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Overall English writing performance in 300-word independent essays, focusing on the quality of development (topic adherence, elaboration of ideas), organization (coherent structure, paragraphing, linking sub-ideas), and language use (grammar, vocabulary, clarity, appropriate academic language).",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "NA",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "NA",
		"cognitive_aspect_measure (认知因素测量工具)": "No separate standardized cognitive measures were administered; cognitive aspects such as understanding of writing criteria and critical thinking were inferred qualitatively from field notes and examples of writing development rather than measured by distinct tests or questionnaires.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Understanding and application of writing criteria (development, organization, language use), ability to generate and elaborate on appropriate main ideas and sub-ideas, and critical thinking about essay content and structure; these were discussed in the context of teachers’ roles in guiding planning, revising, and reasoning, contrasted with ChatGPT’s focus on producing a finished product.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Observation records and field notes documenting learners’ use of ChatGPT (e.g., how they applied the revision prompt, whether they identified the essay topic in the prompt, whether they prompted for increased word count) and their behavior under teacher instruction during revision; counts and percentages of writing errors pre- and post-revision in development, organization, and language use were derived from rater comments.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Patterns of learner behavior when using ChatGPT, such as submitting essays without specifying the subject, accepting AI-generated revisions even when they did not fully address the prompt, failing to request extended word counts, and the resulting technical issues; behavioral changes in error patterns (e.g., reduction in ambiguous development, disorganized structure, and language mistakes) after ChatGPT revision versus teacher-guided revision; instances of pronoun misuse and lack of citation in ChatGPT outputs noted by observers.",
		"other_outcomes_measure (其他结果测量工具)": "Comparative analysis of the percentage of writing errors in development, organization, and language use based on rater comments for pre-test, ChatGPT revisions, and teacher revisions; illustrative essay extracts before and after revision; qualitative reports of technical issues and ethical issues from field notes.",
		"other_outcomes_focus (其他结果维度说明)": "Error profiles and qualitative differences between ChatGPT and teacher revisions: pre-test essays showed high rates of ambiguous development (95%), organizational deficiencies (97%), and language use issues (99%); after ChatGPT revision, comments on development and organization dropped to 13% and 19% and language-use issues to 4%; after teacher instruction, development and organization comments dropped further to 5% and 12% and language-use issues to 0%. Extracts illustrated that ChatGPT often improved language but sometimes failed to address the essay subject, did not fully develop ideas, misused gender pronouns, under-produced the word count, and omitted citations, whereas teacher-guided revisions addressed topic relevance, improved organization and coherence, and aligned better with writing criteria.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Single pre-test writing assessment for all participants, followed by immediate post-revision assessments: ChatGPT group essays scored before and after ChatGPT revision, teacher group essays scored before and after teacher-guided revision; no delayed or longitudinal follow-up assessments were conducted.",
		"primary_outcome_variables (主要结果变量_因变量)": "TOEFL-iBT Independent Writing raw scores (0–5) for pre-test and revised essays in each group, corresponding scaled scores (0–30), CEFR level categories derived from these scores, and percentages of errors in development, organization, and language use based on rater feedback.",
		"independent_variables_and_factors (自变量与实验因素)": "Revision condition (ChatGPT-based revision versus teacher instruction), time (pre-test versus post-revision), and group membership (ChatGPT group versus teacher instruction group).",
		"followup_length_and_type (随访时长与类型)": "No follow-up period; only immediate post-revision outcomes were measured.",
		"statistical_significance (统计显著性结果摘要)": "For the ChatGPT group, the increase from pre-test mean 2.46 to revision mean 3.92 was statistically significant, paired t(24)=−11.78, p<.001. For the teacher instruction group, the increase from pre-test mean 2.57 to revision mean 4.87 was statistically significant, paired t(24)=−24.72, p<.001. An independent-samples t-test on revision scores showed that teacher-guided revisions significantly outperformed ChatGPT revisions, with t(48)=−4.73, p<.001, mean difference −0.94 (teacher group higher).",
		"effect_size_summary (效应量摘要)": "The study did not report standardized effect sizes such as Cohen’s d, but the large mean gains in raw scores (ChatGPT +1.46, teacher +2.30 out of 5) and scaled-score improvements (ChatGPT +8, teacher +12 out of 30, corresponding to 47.05% vs 70.59% development) indicate substantial improvements in writing performance in both groups, with markedly greater improvement under teacher instruction.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "Field notes revealed several limitations and risks associated with ChatGPT use: in some cases, ChatGPT revisions failed to explicitly address the assigned subject, resulting in essays that did not fully respond to the prompt; some outputs were far below the required word count (around 50–100 words) despite the 300-word requirement; misuses of gender pronouns were observed (e.g., referring to a female teacher with 'his'); ChatGPT did not provide or enforce proper academic citations, raising concerns about plagiarism and ethical writing; and it struggled to reorganize disorganized drafts or to generate critical, personalized content without highly specific prompts. The wider discussion also notes institutional concerns about cheating, honesty, misleading privacy, manipulation, and potential negative impacts on critical thinking and authentic writing if ChatGPT is used without teacher guidance.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No analyses or discussions of subgroup or equity effects (e.g., by gender, proficiency level, or socio-economic status) were reported.",
		"limitation (研究局限)": "The study was conducted with a relatively small sample of 50 EFL undergraduates from a single public university, with participants selected based on a narrow band of prior writing exam scores to ensure similar competence. The number of participants was explicitly constrained by time and manageability considerations. The design involved non-random group assignment, a single writing task on one topic, and only immediate post-revision assessments without longer-term follow-up; inter-rater reliability statistics and formal assumption checks were not reported.",
		"challenge (实施挑战与风险)": "Implementation challenges highlighted include learners’ varied ability to use ChatGPT effectively (e.g., failure to specify the essay subject or to prompt for adequate length), technical issues such as under-length AI outputs and misused pronouns, and ethical challenges such as ChatGPT’s inability to instruct on proper citation or to prevent plagiarism. Teachers must also develop AI-related professional competencies to integrate ChatGPT appropriately while safeguarding academic integrity and fostering critical thinking, which requires time, training, and ongoing professional development.",
		"future_work (未来研究方向)": "The author recommends integrating ChatGPT into EFL writing curricula under teacher supervision and emphasizes the need for ongoing professional development in AI literacy, data analytics, and digital literacy for teachers. The discussion suggests that learners should be trained to craft more effective prompts and to use ChatGPT as a scaffolding tool while relying on teachers for writing criteria, critical thinking, and ethical guidance, implying future work on designing teacher–AI collaborative pedagogies, AI-informed assessment, and training programs to support both teachers and students in AI-assisted writing.",
		"implication (理论与教学实践启示)": "The findings imply that ChatGPT can substantially accelerate improvements in EFL writing products by rapidly enhancing language accuracy and partially improving development and organization, thereby shortening the path from intermediate to advanced writing proficiency; however, teachers remain essential for process-oriented instruction, including helping learners understand and apply writing criteria, develop critical thinking, organize ideas, and adhere to ethical standards. Practically, EFL programs are encouraged to implement a combined approach where ChatGPT handles initial drafting or revision and teachers focus on higher-order concerns, ethical guidance, and individualized support, with teacher roles extending beyond the ISTE technology standards to encompass responsibilities that AI cannot replicate."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "No funding was received for this work.",
		"conflict_of_interest (利益冲突声明)": "The author reports that there are no known conflicts of interest associated with the publication.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Participants were purposively selected based on prior writing exam scores and then divided into a ChatGPT group and a teacher instruction group; the article does not report random allocation or allocation concealment, so selection and allocation biases cannot be ruled out.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Three expert inter-raters scored pre-test and revised essays, but the paper does not state whether they were blinded to group assignment (ChatGPT versus teacher) or timepoint (pre-test versus revision); participants and teachers necessarily knew which condition they were in.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The study reports 50 participants divided into two groups of 25, with all analyses based on n=25 per group; there is no explicit discussion of attrition, exclusions, or handling of missing data.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides detailed descriptions of the population and sampling criteria, group sizes, writing tasks and topics, TOEFL-iBT rubrics, ChatGPT version and prompt, data collection procedures, and statistical analyses, along with tables of means, standard deviations, t-test results, and error percentages; qualitative findings are illustrated with representative essay extracts and summarized technical and ethical issues. Conflicts of interest and funding status are explicitly stated.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study identifies the AI tool as ChatGPT organized by OpenAI (Version 3.5) and reports the exact revision prompt used, which supports partial reproducibility; however, no information is given about the date or environment of access, and no model parameters are specified, so exact replication of AI outputs may be limited by future changes in the ChatGPT system.",
		"baseline_equivalence (基线等同性检验)": "Baseline writing competence was controlled by including only learners who scored 17.5–18.5 out of 25 on prior Fundamental English writing exams; pre-test TOEFL-iBT writing raw scores were similar in the two groups (means 2.46 vs 2.57), but no formal statistical test of baseline equivalence between groups was reported.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The article does not report formal checks of statistical assumptions (e.g., normality, homogeneity of variance) for the t-tests, though Levene’s test results are presented in the independent-samples t-test table; additional diagnostics such as normality plots or tests are not described.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "A deviated data point in the ChatGPT revision scores is noted in a table footnote and discussed in the context of AI professional development, but no formal outlier-detection criteria or sensitivity analyses (e.g., analyses with and without the deviant case) are reported.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Pre-test and revised essays were scored on the TOEFL-iBT writing raw scale (0–5) by three inter-raters; raw scores were then converted to scaled TOEFL-iBT scores (0–30) using published scoring tables and further interpreted using CEFR levels by multiplying writing scores across four skills to derive approximate total scores. Error percentages in development, organization, and language use were computed from rater comments. No additional data transformations, such as normalization or log transformation, are described."
	}
}