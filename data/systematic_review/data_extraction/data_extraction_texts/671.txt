{
	"Basic Identification": {
		"author (作者)": "Jennifer Meyer; Thorben Jansen; Ronja Schiller; Lucas W. Liebenow; Marlene Steinbach; Andrea Horbach; Johanna Fleckenstein",
		"publication_year (发表年份)": "2024",
		"study_region (研究地区)": "Germany (upper secondary schools in the federal state of Schleswig-Holstein)",
		"journal_name (期刊名称)": "Computers and Education: Artificial Intelligence",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article; randomized controlled classroom experiment",
		"doi_or_identifier (DOI或唯一标识)": "10.1016/j.caeai.2023.100199",
		"research_aims (研究目的与问题)": "To provide empirical evidence on the effectiveness of feedback generated by large language models in upper secondary EFL writing; to compare LLM generated feedback versus no feedback on students revision performance in an argumentative essay, performance on a new similar writing task, and affective motivational reactions (task motivation, positive emotions, perceived feedback usefulness).",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses the lack of randomized controlled evidence on LLM generated feedback for student writing, moving beyond traditional automated writing evaluation; it is presented as the first randomized controlled trial in secondary education that uses GPT 3.5 turbo to generate evidence based written feedback for EFL argumentative essays and examines both cognitive (revision and transfer performance) and affective motivational outcomes in authentic classroom settings."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "Upper secondary school, Grade 10, academic-track schools",
		"language_proficiency (语言熟练度水平)": "Upper secondary students of English as a foreign language; specific proficiency levels (e.g., CEFR) not reported; tasks drawn from TOEFL iBT argumentative writing.",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "Total n=459; approximately 52% female, 40% male, 7.6% nonbinary or missing.",
		"age (年龄)": "Mean age approximately 16.01 years.",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL (English as a foreign language) instruction in German upper secondary schools.",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "General upper secondary English courses focusing on argumentative writing; academic-track schools in Schleswig-Holstein.",
		"prior_experience_llm (既有LLM使用经验)": "NR (students were not explicitly told that feedback came from an LLM and no prior LLM use data were reported)."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Randomized controlled trial with parallel groups; students were individually randomized within classrooms to either an LLM feedback condition or a control condition with no feedback before revision.",
		"research_method (研究方法_定量_定性_混合)": "Quantitative experimental study.",
		"sampling_method (抽样方法)": "Schools: principals of randomly drawn academic track schools in Schleswig-Holstein were invited, restricted to schools that had not participated in other studies in the previous three months; Students: all Grade 10 students in participating schools took part; from the initial N=552, 93 students were excluded due to technical issues resulting in missing feedback, yielding a final N=459; random assignment to feedback versus control conditions was conducted within classrooms.",
		"sample_size_and_effect (样本量及效应量)": "Final analytic sample N=459 upper secondary EFL students (feedback group n=203, control group n=256). Groups did not differ significantly on initial writing performance or demographics (gender, parental education, books at home, last English grade). For revision performance, LLM generated feedback led to higher scores than revising without feedback with standardized group difference d≈0.19 (p=.042; robustness check controlling for initial score: coefficient≈0.18, p=.013). Task motivation for future similar writing tasks was higher in the feedback group (d≈0.36, p=.001). Positive emotions after feedback were also higher (d≈0.34, p=.004). Perceived feedback usefulness was much higher in the feedback group (d≈1.19, p<.001). Performance on a new writing task did not differ significantly between groups (d≈0.13, p=.259). Overall, across both conditions, revision scores were higher than first draft scores with a within person effect size of about d≈0.78.",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in feedback theories (Hattie and Timperley framework on effective feedback focusing on goals, current progress, and next steps; Shute conditions for effective feedback), writing as a complex cognitive process model (Flower and Hayes), cognitive evaluation theory and self determination theory for viewing feedback as a verbal reward influencing motivation (Deci and Ryan), control value theory of achievement emotions to link feedback, control appraisals, and emotions (Pekrun), and situated expectancy value theory for task motivation; situated in research on automated writing evaluation and recent discussions of large language models in education.",
		"data_collection_instrument (数据收集工具)": "Writing tasks: two TOEFL iBT argumentative writing prompts (Task A about teacher subject knowledge versus ability to relate to students; Task B about banning television advertising directed at very young children). Writing performance: holistic scores from 0 (low) to 5 (high) assigned by a support vector machine based automated scoring algorithm trained on TOEFL iBT texts. Task motivation: four item scale adapted from Busse et al. 2020 assessing intrinsic value for future similar writing tasks on a 6 point Likert scale. Positive emotions: items adapted from the German version of the Epistemically Related Emotion Scales (ERES) by Pekrun et al., using positive emotion items (curiosity, enjoyment, pride) on a 5 point Likert scale. Feedback usefulness: three item scale from Strijbos et al. 2021 on a 7 point Likert scale. Background questionnaire: gender, parental education, number of books at home, last English grade.",
		"data_collection_validity_reliability (工具信度与效度)": "Task motivation scale showed acceptable internal consistency (Cronbach alpha≈0.66). Positive emotions scale (curiosity, enjoyment, pride) showed alpha≈0.62. Feedback usefulness scale showed high internal consistency (alpha≈0.95). The automated scoring algorithm was trained on a corpus of 2420 TOEFL texts rated by rigorously trained expert raters on a 0–5 scale, with inter rater exact agreement of about 62.5 percent and quadratic weighted kappa≈0.67; in cross validation the algorithm achieved exact agreement with human ratings on 43.9 percent of texts and quadratic weighted kappa≈0.76. Questionnaires were adapted from validated instruments and expert reviewed.",
		"data_analysis_method (数据分析方法)": "Descriptive statistics and bivariate correlations were computed. Primary analyses used regression models in Mplus with the group indicator (feedback vs control) as predictor and standardized outcome variables (revision score, new task score, motivation, positive emotions, feedback usefulness) as criteria; for revision performance, initial writing score was included as covariate in a robustness check. Models were estimated with maximum likelihood and robust standard errors using a complex design specification to adjust for classroom clustering. Missing questionnaire data (1–7 percent per item) were handled via full information maximum likelihood. A preliminary paired t test (in SPSS) examined overall pre to revision improvement without accounting for clustering.",
		"unit_of_analysis (分析单位)": "Individual student.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Random assignment of students within classrooms to feedback (LLM generated feedback) or control (no feedback) conditions.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of Grade 10 students in academic track secondary schools in Schleswig-Holstein that had not recently participated in other studies; participation depended on school and class willingness to take part; sample is reasonably large and includes multiple schools but is limited to one German federal state, one track type, and one grade level, so broader representativeness is limited.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "The automated scoring algorithm for texts used training data from 2420 TOEFL iBT essays rated by rigorously trained expert raters on holistic quality; raters underwent structured training, achieved inter rater quadratic weighted kappa around 0.67, and disagreements were resolved by a master rater; the ESCRITO toolkit with a support vector machine using lexical, syntactic, and length complexity features was used to train and validate the scoring model; in the present study, this validated algorithm was implemented in the test environment to score first drafts, revisions, and new task essays live."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Single 90 minute classroom session per class; first argumentative writing task for 20 minutes, immediate feedback or no feedback followed by revision within the same session, then administration of motivation and emotion questionnaires and a second argumentative writing task.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "GPT-3.5-turbo (OpenAI large language model)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "Feedback was generated via the GPT Playground using model GPT-3.5-turbo with temperature set to 0 and maximum length of 1800 tokens; prompts were fixed by the researchers and included instructions on feedback content (structure, content, language), format (table) and learner profile (upper secondary EFL learners); the full prompt text is provided in the study’s supplementary material.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Researcher mediated automated feedback: the system submitted each student’s first draft to GPT-3.5-turbo using a predefined prompt, then delivered the resulting feedback message directly to the student in structured tabular form; students did not interact conversationally with the chatbot or craft their own prompts.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Prompt engineering followed feedback design principles from the literature: GPT-3.5-turbo was instructed to provide feedback that answered three key questions about goals, current performance, and next steps, and to include hints and examples for three aspects of text quality (structure, content, language); feedback had to be in a table to reduce cognitive load and to contain short excerpts from the student’s text to individualize comments; the prompt also specified that the feedback target group were upper secondary foreign language learners and that feedback should be supportive and usable for revision; parameters were fixed (temperature 0) to reduce variability.",
		"training_support_llm_literacy (LLM素养与提示培训)": "No explicit LLM literacy or prompt training was provided to students; they were not informed in advance that the feedback would be generated by an LLM, and they did not receive instruction on how to interact with AI tools in this study.",
		"intervention_implementation (干预实施流程_步骤与任务)": "In each participating class, researchers conducted a 90 minute session using tablets with keyboards. After an introduction, all students wrote an individual TOEFL style argumentative essay in English for 20 minutes (Task A or B in randomized order). The experimental group then received individualized LLM generated feedback on their first draft, presented as a table with comments and examples on structure, content, and language, and were instructed to revise their text based on this feedback. The control group received only a generic instruction telling them to reread and revise their essay as best they could, without specific feedback. After the revision phase, all students completed self report scales on task motivation, positive emotions related to the feedback and revision phase, and perceived usefulness of the feedback or revision instruction. All students then wrote a second TOEFL style argumentative essay on the other task. An automated scoring algorithm scored the first draft, the revision, and the second task live.",
		"experimental_group_intervention (实验组干预内容)": "Students in the feedback group received individualized, tabular GPT-3.5-turbo generated feedback on their first argumentative essay, including hints, comments, and examples on structure, content, and language tailored to their text; they then revised their essays guided by this feedback and subsequently rated their motivation, emotions, and feedback usefulness before writing a second essay.",
		"control_group_intervention (对照组干预内容)": "Students in the control group did not receive specific feedback on their first essay before revision; instead they saw a generic message instructing them to reread their argumentative essay and revise it as well as they could, then revised their essays, completed the same motivation, emotion, and usefulness questionnaires (with usefulness referring to the general instruction), and wrote the second essay.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Initial drafting of an argumentative essay in L2 English, followed by feedback or no feedback and a guided revision stage on the same text, and then writing of a new argumentative essay on a different but comparable prompt.",
		"writing_genre (写作体裁)": "Argumentative essays based on TOEFL iBT independent writing prompts.",
		"writing_task_type (写作任务类型)": "Timed individual L2 English argumentative essay writing tasks (first draft, revision of that draft, and a new essay on a different prompt).",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "LLM served as an automated feedback generator that produced individualized formative written feedback on students first drafts, focusing on structure, content, and language, presented in a tabular format with examples; it did not perform scoring in this study, which was handled by a separate machine learning scoring algorithm.",
		"role_instructor (教师角色与介入方式)": "Teachers and researchers introduced the study in the classroom, monitored the 90 minute session, provided instructions for writing, revision, and questionnaire completion, and ensured smooth administration; they did not provide individualized feedback on the essays during the experiment, as feedback in the experimental group came from GPT-3.5-turbo and the control group received only a generic revision instruction.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "In person classroom sessions in academic track upper secondary schools in Schleswig-Holstein, Germany, using tablets with keyboards; regular English as a foreign language classes with a one off experimental writing and feedback session.",
		"ethical_consideration (伦理审查与知情同意)": "The study was reviewed by the Ministry of General Education and Vocational Training, Science, Research and Culture in Schleswig-Holstein and approved by the ethics committee at the Leibniz Institute for Science and Mathematics Education; participation was voluntary and data collection followed ethical standards, with anonymized data shared via an open repository; specific consent procedures beyond this description were not detailed.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students did not directly access the LLM interface; GPT-3.5-turbo was accessed and configured by the research team, which controlled prompts and parameters; students simply received feedback messages; no detailed institutional policy on broader LLM use in coursework was provided in the article.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "No specific technical safety guardrails or content filters beyond using temperature 0 and a carefully designed prompt were reported; the authors discussed in general that LLM feedback can be false, biased, and opaque and highlighted the need for AI literacy and awareness of potential biases and inaccuracies, but they did not describe additional automated safety systems used during feedback generation.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Compared to revising without specific feedback, receiving GPT-3.5-turbo generated feedback on an English argumentative essay significantly improved students revision performance with a small effect (standardized difference around d=0.19) while controlling for initial draft quality; however, the LLM feedback did not significantly improve performance on a new argumentative writing task written after the revision phase, suggesting that a single feedback instance improved task specific revision but did not yield measurable transfer in this short intervention. The LLM feedback had larger positive effects on affective motivational outcomes: students who received LLM feedback reported higher task motivation for future similar writing tasks (d≈0.36), more positive emotions such as curiosity, enjoyment, and pride (d≈0.34), and greater perceived usefulness of the feedback or revision instruction (d≈1.19) than students in the control group, although the absolute usefulness rating in the feedback group was only moderate on the 7 point scale. The results indicate that LLM generated feedback can be used to deliver timely, individualized feedback at scale in upper secondary EFL classrooms, supporting both revision quality and students motivation and positive emotions, while longer term learning effects and optimal feedback design features remain to be investigated."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "LLM generated feedback was effective in improving immediate revision quality of argumentative essays and in enhancing task motivation, positive emotions, and perceived feedback usefulness compared to no feedback, as measured by an automated essay scoring algorithm and self report scales; no significant effect was observed on performance in a new writing task written in the same session.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Automated holistic essay scores from 0 (low quality) to 5 (high quality) produced by a support vector machine based scoring algorithm (ESCRITO toolkit) trained on TOEFL iBT essays rated by expert human raters; three scores per student were used: Score 1 (first draft), Score 2 (revision), Score 3 (new essay).",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Overall holistic writing quality in English argumentative essays, reflecting integrated aspects such as organization, argumentation, language control, and content as represented in the TOEFL iBT writing quality scale; outcomes focus on quality gains in revision and quality on a new similar task.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Task motivation measured with a four item intrinsic value scale for future similar writing tasks adapted from Busse et al. 2020 on a 6 point Likert scale (alpha≈0.66); positive emotions (curiosity, enjoyment, pride) measured with items adapted from the Epistemically Related Emotion Scales (German version) on a 5 point Likert scale (alpha≈0.62); perceived feedback usefulness measured with a three item scale from Strijbos et al. 2021 on a 7 point Likert scale (alpha≈0.95).",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Students intrinsic motivation and anticipated enjoyment of future similar writing tasks, positive emotional responses (curiosity, enjoyment, pride) when reading and working with the feedback or revision instruction, and perceived usefulness of the feedback or instruction for improving their writing.",
		"cognitive_aspect_measure (认知因素测量工具)": "Cognitive outcomes (revision performance and performance on a new writing task) were assessed via the automated 0–5 holistic essay scores for first draft, revision, and new essay; no separate cognitive strategy or metacognitive scales were used.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Improvement in writing quality during revision of the same text (task specific performance) and performance on a similar new argumentative writing task (potential transfer or learning effect); metacognitive and strategy aspects were discussed conceptually but not directly measured.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "Perceived feedback usefulness scale (three items from Strijbos et al. 2021), treated as an attitudinal outcome related to feedback effectiveness.",
		"other_outcomes_focus (其他结果维度说明)": "Students perception of how helpful and useful the LLM generated feedback or the generic revision instruction was for them, which is considered important for feedback uptake and subsequent motivational and cognitive responses.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Within a single 90 minute session: first essay (pre revision performance), immediate revision after feedback or no feedback (post revision performance), immediate self report of motivation, emotions, and feedback usefulness after revision, and a subsequent new writing task performance score at the end of the session.",
		"primary_outcome_variables (主要结果变量_因变量)": "Revision performance score (Score 2), new task performance score (Score 3), task motivation for future similar writing tasks, positive emotions (curiosity, enjoyment, pride) after feedback, and perceived feedback usefulness.",
		"independent_variables_and_factors (自变量与实验因素)": "Experimental condition (LLM feedback vs control with no specific feedback) as the main independent variable; initial writing score (Score 1) used as a covariate in robustness analyses for revision performance; classroom clustering accounted for in analyses; background variables such as gender, parental education, books at home, and last English grade were examined for baseline equivalence but not included as primary factors.",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "Groups did not differ at baseline on first draft writing scores or demographic variables. Across all students, revision performance improved significantly from first draft to revision (mean from 2.45 to 2.77 on the 0–5 scale, t≈−8.50, p<.001, d≈0.78). Comparing conditions, LLM feedback significantly increased revision performance relative to generic revision (standardized difference for Score 2≈0.19, p=.042; robustness check controlling for Score 1 yielded coefficient≈0.18, p=.013). No significant group difference was found for new task performance (Score 3 difference≈0.13, p=.259). Task motivation was significantly higher in the feedback group (standardized difference≈0.36, p=.001). Positive emotions were significantly higher after feedback (difference≈0.34, p=.004). Perceived feedback usefulness was substantially higher in the feedback group (difference≈1.19, p<.001).",
		"effect_size_summary (效应量摘要)": "LLM generated feedback had a small but meaningful positive effect on revision performance (d≈0.19) and no measurable effect on new task performance in this single session. Effects on affective motivational outcomes were larger: d≈0.36 for task motivation and d≈0.34 for positive emotions, with a very large effect on perceived feedback usefulness (d≈1.19). The overall improvement from first draft to revision across both conditions was large (within person d≈0.78).",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The authors note that LLM generated feedback can be false, biased, and non transparent, which may affect effectiveness and could mislead students; due to the inherent variability of LLM outputs, feedback quality likely varied across students, and this could represent a limitation or risk; they emphasize that students and teachers need AI literacy to recognize potential errors and biases and to use AI feedback appropriately; however, the study did not empirically document instances of misuse or adverse effects on writing performance or motivation in this sample.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The study did not conduct subgroup analyses by gender, background, or proficiency for the LLM feedback effects; the authors explicitly call for future research on fairness and equity, noting that AI generated feedback might not be equally effective for different student groups and that algorithmic biases toward certain groups are possible, but no differential effects or inequities were empirically reported here.",
		"limitation (研究局限)": "Key limitations include that feedback was delivered only once in a single 90 minute session, limiting conclusions about long term learning and sustained motivation; LLM generated feedback quality may have varied and was not systematically analyzed or validated at the individual feedback level; performance gains were task specific and did not translate into significant improvements on a new writing task within the same session; students were not informed about the AI source of feedback, so potential effects of source awareness on engagement are unknown; the study did not compare LLM feedback with teacher feedback, making it unclear whether LLM feedback is better or simply comparable to other feedback sources; the design does not isolate which specific feedback features or prompt characteristics drive effectiveness; the automated scoring algorithm, while validated, may differ from alternative scoring methods such as transformer based models; and the sample is limited to Grade 10 students in academic track schools in one German state, which restricts generalizability.",
		"challenge (实施挑战与风险)": "Practical challenges include designing effective prompts and feedback formats that align with feedback theory while controlling cognitive load, ensuring the reliability and accuracy of LLM feedback given known issues of hallucinations and bias, training teachers and students in AI literacy so they can critically engage with AI generated comments, avoiding over reliance on AI that might reduce the development of independent writing and revision skills, integrating AI feedback into classroom routines in ways that respect teacher roles and instructional goals, and addressing fairness concerns and potential algorithmic biases that could advantage or disadvantage particular groups of students.",
		"future_work (未来研究方向)": "Future research directions include conducting longitudinal studies to examine cumulative and long term effects of repeated LLM feedback on writing development and grades, exploring the impact of different prompt and feedback designs (for example varying level of detail, format, focus on higher versus lower level features), comparing LLM feedback directly with teacher feedback and other feedback sources, analyzing the quality and accuracy of LLM feedback at the message level, investigating how different student characteristics (proficiency, gender, motivation, prior AI experience) moderate the effects of AI feedback, examining fairness and bias issues in LLM feedback and scoring, and studying how to effectively build AI literacy among teachers and students for safe and productive use of LLM based feedback in classrooms.",
		"implication (理论与教学实践启示)": "The study suggests that LLM generated feedback can operationalize principles from feedback theory in a scalable way, providing individualized, timely feedback that supports revision and bolsters motivation and positive emotions in EFL writing without large additional time costs for teachers; theoretically, it extends feedback research into the domain of transformer based AI while confirming that feedback that addresses goals, current performance, and next steps can be beneficial even when delivered by a machine; practically, it implies that schools can use LLMs as part of intelligent tutoring systems to increase the frequency and individualization of writing feedback, provided that teachers remain actively involved in guiding interpretation and use of feedback, that AI is treated as a support rather than a replacement for teacher feedback, and that ethical, fairness, and literacy considerations are addressed in implementation."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Funded by the German Federal Ministry of Education and Research (BMBF), grant number 01JG2104.",
		"conflict_of_interest (利益冲突声明)": "The authors declare that they have no known competing financial interests or personal relationships that could have influenced the work.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Students were randomized to feedback or control conditions within classrooms, and baseline equivalence checks indicated no significant group differences in initial writing performance or key demographics; randomization reduces selection bias, although implementation details (for example randomization procedure) are not extensively described.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Students knew whether they received specific feedback or only a generic revision instruction, so blinding at the participant level was not possible; however, scoring of essays was fully automated by the algorithm, which removes rater expectancy bias for writing outcomes; self report measures could be influenced by participants awareness of their condition; there is no indication of blinding of analysts.",
		"attrition_and_missing_data (流失与缺失数据处理)": "From an initial sample of 552 students, 93 were excluded due to technical difficulties that prevented them from receiving feedback for revision, leaving N=459 for analyses; missing data on questionnaire items ranged from 1 to 7 percent and were handled using full information maximum likelihood; reasons for item level non response are not further detailed, and no differential attrition by condition beyond the technical exclusions is reported.",
		"reporting_transparency (报告透明度与可重复性)": "The article provides extensive methodological detail on tasks, scoring algorithm training, feedback prompt design, measures, and analytic procedures; it explicitly states that the study was not preregistered but that all data and syntax are available on an open OSF repository for replication; the LLM prompt is included in supplementary materials; effect sizes and p values are reported for main outcomes.",
		"preregistration_or_protocol (预注册或研究方案)": "No preregistration; authors state that the study was not preregistered.",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that GPT-3.5-turbo was used with temperature 0 and maximum length 1800 and provides the full prompt text in the supplement, which supports reproducibility; however, the authors acknowledge more generally that evolving LLM models and potential variability in outputs pose challenges for exact replication in future work.",
		"baseline_equivalence (基线等同性检验)": "Groups were compared on first draft writing scores and demographic variables (gender, parental education, books at home, last English grade) and no significant differences were found, supporting baseline equivalence between feedback and control conditions.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Models used robust maximum likelihood estimation with a complex design option to adjust for classroom clustering; assumptions such as normality of residuals or homoscedasticity were not explicitly discussed, and no additional diagnostics (for example tests for influential cases) were reported.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "No specific procedures for identifying or handling outliers were described; a robustness check for revision performance including the first draft score as a covariate was conducted and yielded similar conclusions, but no broader sensitivity analyses were reported.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Automated essay scores from the SVM algorithm trained on TOEFL corpora were used directly on the 0–5 scale; questionnaire items were aggregated into composite scales (means) for motivation, positive emotions, and feedback usefulness; missing item responses were handled using full information maximum likelihood in Mplus; no additional transformations (e.g., standardization) were reported beyond the use of standardized outcome variables in regression models."
	}
}