{
	"Basic Identification": {
		"author (作者)": "Arifi Waked, Muhammad W. Ashraf, Hanadi AbdelSalam, Khadija El Alaoui, Maura Pilotti",
		"publication_year (发表年份)": "2024",
		"study_region (研究地区)": "Saudi Arabia",
		"journal_name (期刊名称)": "Proceedings of ICRES 2024 – International Conference on Research in Education and Science",
		"study_type (研究类型_期刊论文_会议论文等)": "Conference proceedings paper; quasi-experimental field study with treatment and control sections in an EFL academic writing course",
		"doi_or_identifier (DOI或唯一标识)": "NR",
		"research_aims (研究目的与问题)": "To examine whether in-class error-detection exercises using text generated by ChatGPT can aid EFL students’ English academic writing, by comparing the quantity and quality of descriptive essays produced after ChatGPT-based error analysis versus traditional error analysis without ChatGPT, and to test whether such exercises enhance students’ confidence (longer essays) and attention to writing quality (more low-frequency and abstract words, more complex syntactic structures).",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses questions about whether AI tools such as ChatGPT can support foreign language writing by using ChatGPT output itself as the object of student-centered error analysis in an EFL academic writing class of native Arabic speakers and by comparing this AI-based error-detection practice with instructor-generated error texts, linking ChatGPT exercises to objective text quantity and quality indices."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "University freshmen in an introductory EFL academic writing/communication course",
		"language_proficiency (语言熟练度水平)": "IELTS total score at least 6.0 and writing score at least 5.5 (range 0–9), corresponding to modest to competent users of English according to IELTS standards",
		"mother_tongue (母语)": "Arabic",
		"sex (性别)": "NR",
		"age (年龄)": "18–25 years old",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL (English as a Foreign Language) academic writing course in a Saudi Arabian university",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "Freshmen enrolled in a communication course devoted to EFL academic writing at Prince Mohammad Bin Fahd University; specific academic majors NR",
		"prior_experience_llm (既有LLM使用经验)": "NR"
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental between-groups field study with two conditions (ChatGPT-based error analysis vs instructor-generated error analysis) implemented across eight course sections in two consecutive semesters",
		"research_method (研究方法_定量_定性_混合)": "Quantitative quasi-experimental study using automated text analysis and one-way ANOVAs on writing indices; no separate qualitative component reported",
		"sampling_method (抽样方法)": "Convenience sampling of all students enrolled in eight sections of a required communication course devoted to EFL academic writing; sections were assigned by semester to treatment or control",
		"sample_size_and_effect (样本量及效应量)": "Two conditions: treatment condition with ChatGPT-based error-detection exercises n=101, control condition with instructor-generated text n=112, total N=213. The authors also state that 29 students (11.65%) who did not complete the assignment or whose output showed more than 15% similarity to other sources or more than 40% suspected AI-generated content were excluded; the final analytic N per group is not explicitly reported. Significant ANOVA results: number of sentences F(1,211)=5.26, MSE=119.15, p=0.023, partial η²=0.024 (ChatGPT group wrote more sentences); mean words per sentence F(1,211)=4.63, MSE=78.24, p=0.033, partial η²=0.021 (ChatGPT group wrote shorter sentences); word unfamiliarity (use of low-frequency words) differed significantly at p<.05 (exact F and effect size NR). The total number of words, word concreteness, syntactic complexity, and lexical cohesion did not differ significantly between conditions (Fs≤3.71).",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in literature on AI-powered tools and ChatGPT in education, ethical and pedagogical implications of LLM use, grade versus learning orientation and academic agency, and cross-linguistic transfer effects between Modern Standard Arabic and English in EFL writing (e.g., subject-verb agreement, punctuation, word order); the study also draws on research showing how error analysis can improve writing and how ChatGPT feedback can affect foreign language writing in other contexts.",
		"data_collection_instrument (数据收集工具)": "ChatGPT (version 3.0) used to generate a five-paragraph descriptive essay for the treatment group error-detection exercise; an instructor-generated error-laden essay for the control group; student-written five-paragraph descriptive essays as the main outcome data; ETS TextEvaluator software to compute quantity and quality indices (number of words, number of sentences, mean words per sentence, word unfamiliarity, word concreteness, syntactic complexity, lexical cohesion); SafeAssign plagiarism detection; an AI detection program to flag potentially AI-generated content; institutional records for comparing demographic variables such as number of enrolled hours.",
		"data_collection_validity_reliability (工具信度与效度)": "Writing indices were computed using ETS TextEvaluator, which bases word frequency on large corpora (400 million-word ETS corpus and a 17 million-word corpus by Zeno et al.) and concreteness ratings following Coltheart’s method; syntactic complexity and lexical cohesion scores are composite indices ranging from 1 to 100. The authors note that AI-detection programs have high false positive rates, which motivated the choice of a 40% suspicion threshold, but they do not report additional validity or reliability statistics for the instruments in this sample or any calibration against human ratings.",
		"data_analysis_method (数据分析方法)": "One-way Analyses of Variance (ANOVA) with Condition (ChatGPT vs control) as the between-subjects factor on each quantitative writing measure (number of words, number of sentences, mean words per sentence, word unfamiliarity, word concreteness, syntactic complexity, lexical cohesion); reporting of F statistics, p-values, and partial η² for significant outcomes; descriptive statistics (means and standard errors) presented in a summary table.",
		"unit_of_analysis (分析单位)": "Individual student descriptive essay",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Non-random assignment by semester: four sections in the first semester served as the control condition and four sections in the subsequent semester served as the ChatGPT treatment condition; all sections were taught by the same bilingual instructor.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of freshman EFL learners at Prince Mohammad Bin Fahd University who met minimum IELTS requirements and enrolled in the introductory academic writing course; the authors describe the sample as a homogeneous group of native Arabic speakers with writing competence above novice level and report that institutional records indicated no significant differences in enrolled credit hours between semesters [F(1,211)<2.29, ns]; generalizability to other L1 backgrounds or proficiency levels is not claimed.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "No human raters or rater training are reported; all essays were automatically processed by ETS TextEvaluator to generate writing indices, and these automated scores formed the basis of the quantitative analyses."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Students received three weeks of instruction on five-paragraph essay structure and morphosyntactic differences between English and Modern Standard Arabic, followed in week 4 by a single in-class error-analysis exercise (ChatGPT- or instructor-based) and an immediately subsequent descriptive essay writing assignment in the next class; the overall course lasted a semester, but the ChatGPT-specific intervention consisted of this one exercise and one post-exercise essay.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (LLM, version 3.0)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "ChatGPT version 3.0 was accessed by the instructor to generate a five-paragraph descriptive essay used in printed or digital form for classroom error analysis; the access mode (web interface vs API), time of access, and parameter settings were not specified.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Teacher-mediated: the instructor interacted with ChatGPT to generate a sample essay and distributed the resulting text to students for in-class error-detection exercises; students did not directly use ChatGPT for their own writing within the study.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "The instructor used a single explicit task-oriented prompt asking ChatGPT to write a five-paragraph descriptive essay on haloumi cheese, including an introduction, three body paragraphs, and a conclusion; no additional prompt engineering strategies such as few-shot examples or iterative refinement were reported.",
		"training_support_llm_literacy (LLM素养与提示培训)": "During the treatment exercise, the instructor highlighted specific strengths and limitations of ChatGPT as a writer and emphasized students’ agency and competence in error detection and composing their own texts, but no systematic training in prompt design, AI literacy, or critical engagement with AI outputs beyond this discussion was reported.",
		"intervention_implementation (干预实施流程_步骤与任务)": "After three weeks of instruction on essay structure and relevant English–Arabic morphosyntactic differences, the ChatGPT condition began with an instructor prompt to ChatGPT 3.0 to generate a five-paragraph descriptive essay on haloumi cheese; students received this output in a Word document, spent 15 minutes independently identifying errors, and then engaged in a guided class discussion where they shared errors and corrections while the instructor pointed out additional grammatical, semantic, cohesion, and structural issues and stressed ChatGPT’s limitations and students’ agency. In the control condition, the same procedure was followed but the text was an instructor-generated essay with similar types of intentional errors instead of ChatGPT output. In the following class, all students who gave informed consent wrote a five-paragraph descriptive essay on a topic of their choice, knowing that their work would be checked for plagiarism and AI-generated content; these essays were then analyzed with TextEvaluator.",
		"experimental_group_intervention (实验组干预内容)": "ChatGPT-based error-detection exercise in which students analyzed and corrected grammatical, semantic, cohesion, and structural errors in a ChatGPT 3.0-generated five-paragraph descriptive essay on haloumi cheese through individual work and whole-class discussion guided by the instructor, followed by writing their own descriptive essays.",
		"control_group_intervention (对照组干预内容)": "Parallel error-detection exercise using an instructor-authored five-paragraph descriptive essay intentionally seeded with grammatical, semantic, cohesion, and structural errors, analyzed and corrected through the same individual and class discussion process, followed by writing descriptive essays; no ChatGPT output was used.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "Focus on revising and editing at the error-detection stage (identifying and correcting errors in model texts), followed by planning and generating a full five-paragraph descriptive essay as a first-course assignment; no iterative revision of students’ own essays with AI or teacher feedback was reported within the study.",
		"writing_genre (写作体裁)": "Five-paragraph descriptive essays",
		"writing_task_type (写作任务类型)": "Individual descriptive essay on a self-chosen topic written in standard five-paragraph format (introduction, three body paragraphs, conclusion)",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT functioned as a generator of a model five-paragraph descriptive essay whose output contained errors; this AI-generated text served as the material for student-centered error detection and class discussion on grammar, semantics, lexical cohesion, paragraph connectivity, and essay structure. ChatGPT did not directly provide feedback on students’ own essays and was not used for scoring.",
		"role_instructor (教师角色与介入方式)": "A single bilingual instructor taught all sections, provided initial explicit instruction on essay structure and English–MSA differences, generated the ChatGPT prompt and the instructor-written control text, orchestrated individual and class error-detection activities, highlighted additional errors and the strengths and limitations of ChatGPT as a writer, emphasized students’ agency as EFL writers, assigned and collected the descriptive essays, and oversaw SafeAssign and AI-detection-based integrity checks.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Prince Mohammad Bin Fahd University, Saudi Arabia; freshman-level communication course devoted to EFL academic writing, delivered in a face-to-face classroom setting with digital tools (ChatGPT in the treatment condition and plagiarism/AI detection software in both conditions).",
		"ethical_consideration (伦理审查与知情同意)": "The sample included only students who provided informed consent and completed all phases of the study; students were explicitly informed that their essays would be checked with SafeAssign and an AI detection program and that high similarity or suspected AI-generated content would lead to a score of 0 unless they could demonstrate authorship; formal institutional ethics committee approval is not mentioned.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Within the study, students were allowed to work with ChatGPT output only in the controlled in-class error-analysis exercise in the treatment condition, and they were explicitly warned that any use of AI to generate their own essays would be detected and penalized: essays with more than 15% similarity to other sources or more than 40% suspected AI-generated content received a score of 0 unless students could prove the work was their own.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "Guardrails consisted of academic integrity policies enforced via SafeAssign and an AI detection program with specified thresholds, coupled with instructor emphasis on students’ learning orientation and agency rather than grade orientation; specific technical safety or content-filter settings within ChatGPT were not described.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Relative to students who completed a comparable error-detection exercise on an instructor-generated text, those who engaged in ChatGPT-based error analysis wrote essays with significantly more sentences (approximately 29.25 vs 25.81 per essay, F(1,211)=5.26, p=0.023, partial η²=0.024) and significantly fewer words per sentence on average (approximately 17.60 vs 20.21 words, F(1,211)=4.63, p=0.033, partial η²=0.021), while the total number of words per essay did not differ significantly, suggesting that ChatGPT exercises encouraged segmentation into shorter sentences more aligned with English conventions without changing overall essay length. The ChatGPT group also achieved higher word unfamiliarity scores, indicating greater use of low-frequency words (p<.05), whereas word concreteness, syntactic complexity, and lexical cohesion scores did not differ significantly between groups. The authors interpret these findings as showing that a single ChatGPT output error-analysis exercise yields minor but meaningful benefits by enhancing EFL students’ attention to sentence length and lexical richness, though more extensive and broader exercises may be needed to influence other writing quality dimensions such as syntactic complexity and cohesion."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of the ChatGPT-based pedagogical intervention was evaluated through ETS TextEvaluator indices of essay quantity (number of words, number of sentences, mean words per sentence) and quality (word unfamiliarity, word concreteness, syntactic complexity, lexical cohesion) compared between ChatGPT and control groups using ANOVAs; results show that ChatGPT error-analysis exercises produced statistically significant gains in number of sentences, shorter sentence length, and greater use of low-frequency words, while other quality indices remained unchanged, indicating modest but positive effects on EFL writing.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "ETS TextEvaluator indices: number of words per essay, number of sentences per essay, mean number of words per sentence, word unfamiliarity (use of low-frequency words), word concreteness (use of concrete vs abstract words), syntactic complexity (composite of sentence-length and clause-related measures), and lexical cohesion (word repetition and connective use across sentences), all scaled from 1 to 100 for quality indices.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Writing fluency and quantity (essay length in words, number of sentences, average sentence length) and writing complexity (lexical sophistication via low-frequency and abstract words, syntactic complexity via clause and modifier measures, and cohesion via lexical overlap and connectives), with specific interest in whether ChatGPT-based exercises affect sentence segmentation and vocabulary richness in EFL descriptive essays.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "NA",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "NA",
		"cognitive_aspect_measure (认知因素测量工具)": "NA",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "NA",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "NA",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "NA",
		"other_outcomes_measure (其他结果测量工具)": "NA",
		"other_outcomes_focus (其他结果维度说明)": "NA",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Single post-intervention assessment: one descriptive essay written immediately after the in-class error-analysis exercise in week 4; there was no pre-test writing measure or delayed post-test.",
		"primary_outcome_variables (主要结果变量_因变量)": "Number of words per essay, number of sentences per essay, mean words per sentence, word unfamiliarity score, word concreteness score, syntactic complexity score, lexical cohesion score as computed by ETS TextEvaluator.",
		"independent_variables_and_factors (自变量与实验因素)": "Instructional condition (ChatGPT-based error analysis vs instructor-generated text error analysis).",
		"followup_length_and_type (随访时长与类型)": "NA",
		"statistical_significance (统计显著性结果摘要)": "ChatGPT condition essays contained significantly more sentences than control essays [F(1,211)=5.26, MSE=119.15, p=0.023, partial η²=0.024] and significantly fewer words per sentence [F(1,211)=4.63, MSE=78.24, p=0.033, partial η²=0.021], while total word count did not differ significantly between groups. Word unfamiliarity (low-frequency vocabulary) was significantly higher in the ChatGPT group (p<.05; exact F and η² NR). Word concreteness, syntactic complexity, and lexical cohesion did not significantly differ between conditions (Fs≤3.71, ns).",
		"effect_size_summary (效应量摘要)": "Partial η²≈0.024 for number of sentences and partial η²≈0.021 for mean words per sentence indicate small effect sizes; the effect size for the significant difference in word unfamiliarity (low-frequency vocabulary usage) is not reported.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "The study notes potential negative effects of ChatGPT and other AI tools in education, including increased dependency on automation that may reduce opportunities to develop critical thinking and academic agency, a shift toward grade orientation, and higher risks of plagiarism and ethically problematic use when assignments can be easily completed by LLMs. Within the study, essays with more than 15% similarity to external sources or more than 40% suspected AI-generated content were penalized with a score of 0 unless authorship could be demonstrated, resulting in the exclusion of 29 students from the sample; this reflects both the risk of misuse and the challenge of accurately detecting AI-generated writing given known false positive rates of detection tools.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No subgroup or equity analyses were conducted; the authors explicitly describe the sample as a homogeneous group of native Arabic-speaking EFL learners with writing competence above novice level and note that differences faced by learners with other first languages are a limitation, but no gender-, proficiency-, or other subgroup-specific results are reported.",
		"limitation (研究局限)": "Limitations stated include a homogeneous sample of native Arabic-speaking EFL learners with IELTS-defined proficiency above novice, limiting generalizability to other L1 backgrounds and lower proficiency levels; the focus on a single five-paragraph descriptive essay assignment as the only outcome, which may not capture broader or long-term effects; the limited breadth and duration of the ChatGPT error-analysis exercises, which may have constrained impact on syntactic complexity and cohesion; and the use of AI-generated and automated text analysis tools without human rating benchmarks. The design also relied on non-random assignment of course sections across semesters and did not include a writing pre-test, which restricts causal inference to between-group comparisons at a single post-test time point.",
		"challenge (实施挑战与风险)": "Challenges highlighted include the danger that the availability of ChatGPT may foster student dependency on automation, reduce practice in critical thinking and academic agency, and encourage grade-oriented rather than learning-oriented approaches; the ease with which LLMs can complete writing assignments that do not require personal or course-specific content, making plagiarism and academic dishonesty harder to detect; and the technical difficulty of reliably distinguishing AI-generated from student-generated text given false positives in AI detection programs, which complicates enforcement of ethical AI use policies.",
		"future_work (未来研究方向)": "The authors recommend investigating whether exposing learners to ChatGPT output, as in this study, is more or less effective than asking ChatGPT to improve student-produced text, as in related research; examining the breadth and duration of ChatGPT-based exercises as independent variables to see whether more extensive practice can affect additional writing qualities such as syntactic complexity and coherence; expanding samples to include learners with different first languages, proficiency levels, and contexts; and further exploring how different ways of integrating ChatGPT into writing instruction influence student outcomes and ethical use.",
		"implication (理论与教学实践启示)": "The study suggests that ChatGPT-based error-analysis exercises can be used as a pedagogical tool to improve specific aspects of EFL writing, notably sentence segmentation according to English conventions and lexical richness through increased use of low-frequency words, while also providing a platform to highlight the limitations of AI as a writer and to reinforce students’ sense of agency. It implies that educators should not ignore AI tools but rather integrate them ethically into instruction to help students critically analyze and improve their writing, design assignments that encourage learning orientation and critical thinking, and combine AI support with explicit instruction on academic integrity and responsible AI use."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "NR",
		"conflict_of_interest (利益冲突声明)": "NR",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Randomization was not used; course sections were assigned to control or ChatGPT conditions by semester, and all sections were taught by the same instructor, which may introduce confounding by cohort or temporal effects despite efforts to select demographically similar semesters.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "No blinding procedures are reported; the instructor was aware of the condition of each section, and while essays were scored automatically by software, there is no indication that analysts were blind to group membership when conducting ANOVAs.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The authors report that 29 students (11.65%) who did not complete the assignment or whose output exceeded plagiarism or AI-detection thresholds were excluded from the sample; analyses were conducted on the remaining students, though the exact final N per group is not clearly specified. No imputation or additional missing data procedures are described.",
		"reporting_transparency (报告透明度与可重复性)": "The paper describes the instructional context, inclusion criteria, condition assignments across semesters, the ChatGPT prompt and version, exclusion thresholds for plagiarism and AI-generated content, the ETS TextEvaluator indices used, and the ANOVA results including F, p, and partial η² for significant findings; descriptive statistics for all measures are presented in a table. However, some details such as the final analytic sample size per group and full statistics for all indices are not fully elaborated.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that ChatGPT version 3.0 was used and provides the exact content and structural requirements of the prompt used to generate the model essay, which supports partial reproducibility of the LLM output, but does not state access date, platform, or parameter settings, limiting full replication.",
		"baseline_equivalence (基线等同性检验)": "The authors state that institutional records indicated that the two semester samples were demographically similar, including number of enrolled credit hours [F(1,211)<2.29, ns], but no baseline writing performance measures or other academic variables are statistically compared between conditions before the intervention.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "Assumption checks for ANOVA (e.g., normality of residuals, homogeneity of variance) and other data diagnostics are not reported.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "Aside from excluding students with incomplete assignments or high plagiarism/AI-detection scores, the study does not report any additional outlier detection, removal, or sensitivity analyses for the writing indices.",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Student essays were submitted to ETS TextEvaluator, which computed quantity measures and scaled quality indices based on corpus-derived word frequencies, concreteness ratings, syntactic complexity metrics, and lexical cohesion measures; no further data transformations are described before conducting ANOVAs."
	}
}