{
	"Basic Identification": {
		"author (作者)": "Thi-Ngoc-Anh Duong, Hsiu-Ling Chen",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Hanoi, Northern Vietnam",
		"journal_name (期刊名称)": "Journal of Educational Computing Research",
		"study_type (研究类型_期刊论文_会议论文等)": "Peer-reviewed journal article (quasi-experimental mixed-methods study)",
		"doi_or_identifier (DOI或唯一标识)": "10.1177/07356331241312363",
		"research_aims (研究目的与问题)": "To develop and implement the Writing Assistant Bot (WAB), an AI chatbot based on GPT-3.5 Turbo, to support high school EFL learners’ home writing practice, and to investigate (1) how students at different writing proficiency levels use the AI chatbot across writing stages, (2) how integrating the AI chatbot into EFL writing practice affects writing performance of higher- and lower-level students, and (3) whether there are differences between higher- and lower-level students’ perceptions of the chatbot.",
		"research_gap_or_novelty (研究创新性与知识空白)": "The study addresses a gap in research on generative AI chatbots for EFL writing, particularly in high school settings and over a relatively longer intervention (11 weeks). Prior work focused mainly on university students, short interventions, or retrieval-based chatbots and often lacked detailed analysis of usage by proficiency level. This study contributes by using a GPT-3.5 Turbo based chatbot, examining usage tendencies at different writing stages and aspects, comparing writing performance changes for higher- and lower-level learners, and exploring differential perceptions of the chatbot."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "11th-grade high school students",
		"language_proficiency (语言熟练度水平)": "Two proficiency groups based on IELTS writing pretest scores: higher-level group with IELTS writing 6.0–7.0 (competent and good users), lower-level group with IELTS writing 4.0–5.0 (modest and limited users).",
		"mother_tongue (母语)": "NR",
		"sex (性别)": "NR",
		"age (年龄)": "All participants were 16 years old.",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in a high school in Hanoi, Northern Vietnam.",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "General secondary education; regular high school English coursework.",
		"prior_experience_llm (既有LLM使用经验)": "None of the students had prior experience using AI chatbots for learning."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Quasi-experimental pretest–posttest design without a non-AI control group, comparing two proficiency-defined groups (higher-level vs lower-level writers) who all used the AI chatbot; mixed-methods approach integrating quantitative and qualitative data over an 11-week intervention.",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods: quantitative (timed-writing pretest and posttest scores, chat log statistics, Likert-scale questionnaire responses, non-parametric inferential tests) and qualitative (content analysis of chat logs, open-ended questionnaire responses, semi-structured interviews).",
		"sampling_method (抽样方法)": "Convenience sampling of 47 11th-grade students from one high school in Hanoi; participants were then categorized into higher- and lower-level groups based on standardized IELTS writing pretest scores.",
		"sample_size_and_effect (样本量及效应量)": "Total N=47; higher-level group n=25, lower-level group n=22. Wilcoxon signed-rank tests showed significant pre-to-post improvements for both groups in mechanics, language use, vocabulary, organization, content, and overall writing scores (e.g., overall scores: lower-level group z=−3.990, p=.000; higher-level group z=−4.162, p=.000). Mann-Whitney U tests indicated no significant differences between groups in total chatting time (U=248.50, p=.572) or number of meaningful inquiries (U=220.00, p=.241). For perceptions, perceived usefulness did not differ significantly between groups, while perceived ease-of-use was significantly higher for the higher-level group (U=237.00, z=−2.061, p=.039, r≈0.30).",
		"theoretical_foundation (理论基础_理论框架)": "Social Constructivism (Vygotsky 1978) as the overarching theory emphasizing learning through interaction and scaffolding by a more knowledgeable other; Flower and Hayes’ (1981) cognitive process model of writing (Planning, Translating, Reviewing); Jacobs’ (1981) five-component model of writing (content, organization, vocabulary, language use, mechanics); Technology Acceptance Model (Davis 1989) for perceived usefulness and perceived ease-of-use of the chatbot.",
		"data_collection_instrument (数据收集工具)": "Chat logs from the WAB platform (total chatting time and meaningful inquiries related to weekly writing tasks); two 40-minute timed-writing tasks based on IELTS Writing Task 2 used as pretest and posttest; writing scores rated with Jacobs’ (1981) rubric (mechanics, language use, vocabulary, organization, content) by two experienced raters; a 7-point Likert-scale perception questionnaire based on the Technology Acceptance Model with 10 items on perceived usefulness, 6 items on perceived ease-of-use, plus two open-ended questions; semi-structured interviews with 10 students (5 per proficiency group).",
		"data_collection_validity_reliability (工具信度与效度)": "Chat log coding of meaningful inquiries by writing stages (Flower and Hayes model) and writing aspects (Jacobs model) achieved high inter-rater reliability with Cohen’s kappa=0.93 between the first author and an expert. Writing pretest and posttest scores rated by the instructor and another experienced teacher showed high agreement with Cohen’s kappa=0.83 (pretest) and 0.87 (posttest). The perception questionnaire demonstrated high internal consistency with Cronbach’s alpha=0.92. Instruments (IELTS Task 2 prompts, Jacobs rubric, TAM-based questionnaire) are established tools in writing and technology acceptance research.",
		"data_analysis_method (数据分析方法)": "Quantitative data were analyzed using SPSS 21.0. Shapiro–Wilk tests indicated non-normal distributions and small sample size, so non-parametric tests were used: Mann–Whitney U tests for between-group comparisons (e.g., chatting time, meaningful inquiries, questionnaire scales), Wilcoxon signed-rank tests for within-group pre–post comparisons of writing performance. Descriptive statistics summarized chat log usage, questionnaire responses, and open-ended items. Qualitative data (open-ended responses and interviews) were analyzed via content analysis and thematic analysis using Excel and NVivo 14, with repeated readings, coding, theme identification, and expert review to enhance credibility.",
		"unit_of_analysis (分析单位)": "Individual learner (writing scores, questionnaire responses, chat usage patterns) and individual chat inquiries as coded units in content analysis.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Non-random assignment; participants were categorized into higher-level and lower-level groups based on IELTS writing standardized pretest scores (higher-level: 6.0–7.0; lower-level: 4.0–5.0); both groups received the same AI chatbot intervention.",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame was one high school in Hanoi, Northern Vietnam; 47 11th-grade students in one context participated via convenience sampling, so the sample is limited to a single school and age group and may not be representative of other regions, grade levels, or educational contexts.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Timed-writing tasks were scored by the class instructor and another experienced English teacher using Jacobs’ writing rubric for mechanics, language use, vocabulary, organization, and content; Cohen’s kappa was used to confirm high inter-rater agreement (0.83 pretest, 0.87 posttest). Specific details of rater training and score aggregation (e.g., averaging scores) are not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "11 weeks total from November 2023 to January 2024: week 1 pretest and WAB orientation; weeks 2–9 weekly home writing practice with the chatbot (one essay per week); week 10 posttest and perception questionnaire; week 11 semi-structured interviews.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "Writing Assistant Bot (WAB) built on GPT-3.5 Turbo, a ChatGPT family large language model.",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "WAB is an online platform powered by the GPT-3.5 Turbo language model that students accessed via login; it presented weekly IELTS-style writing topics, included a Write function for composing essays, and an AI chatbot interface for interactive assistance; specific API configuration details and parameter settings are not reported.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Students interacted directly with the GPT-3.5 Turbo based chatbot via a web platform (WAB) for their home writing practice; the LLM was embedded in the WAB writing system and engaged in dynamic text-based conversations with learners, answering questions and providing suggestions throughout the writing process.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "Students initiated queries in natural language to ask the chatbot for vocabulary, ideas, elaboration, refinement, feedback, and other writing-related support; chat logs show usage aligned with Flower and Hayes’ Planning, Translating, and Reviewing stages and Jacobs’ writing aspects. The teacher provided guidance and prompt examples on how to effectively interact with the chatbot and students learned to give more detailed and specific commands over time, but no formal prompt-engineering framework (e.g., few-shot demonstrations, explicit role instructions) is described.",
		"training_support_llm_literacy (LLM素养与提示培训)": "In week 1, students received guidance on using WAB, including logging in, accessing weekly topics, composing essays, and consulting the chatbot. During the study, they were given clear guidance and prompt examples to maximize chatbot benefits and were explicitly informed that the chatbot might occasionally make errors, so they should critically evaluate its suggestions and cross-check information. Interviews and discussion indicate that students gradually learned how to formulate clearer prompts, but there is no explicit mention of systematic instruction on issues such as plagiarism, citation, or broader AI literacy.",
		"intervention_implementation (干预实施流程_步骤与任务)": "In week 1, all students took a 40-minute IELTS Task 2 based writing pretest and then received training on how to use WAB. From weeks 2–9, one IELTS-like essay topic was assigned each week through the WAB platform; students logged in from home, checked the weekly topic, interacted with the chatbot to brainstorm ideas, generate vocabulary, refine language, and seek feedback, and then wrote and submitted one essay per week via the platform. Chat logs recorded total chatting time and meaningful inquiries for each student. In week 10, students completed a writing posttest under the same timed conditions as the pretest and filled out a TAM-based perception questionnaire with additional open-ended items. In week 11, semi-structured interviews were conducted with a subset of students from each proficiency group to explore usage strategies, perceived improvements, and perceptions of the chatbot.",
		"experimental_group_intervention (实验组干预内容)": "All participants, both higher- and lower-level groups, received the same intervention: weekly home writing practice supported by the WAB chatbot built on GPT-3.5 Turbo. They used the system to view weekly topics, compose and submit essays, and interact with the chatbot for idea generation, vocabulary expansion, language refinement, and feedback across planning, translating, and reviewing stages; group status reflected writing proficiency levels rather than different instructional treatments.",
		"control_group_intervention (对照组干预内容)": "NA (no non-AI or non-chatbot control group; both higher- and lower-level groups used the same GPT-3.5 Turbo based WAB intervention).",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "The intervention covered all three writing stages defined by Flower and Hayes: Planning (generating ideas, planning structure, setting goals), Translating (turning planned ideas into written language), and Reviewing (evaluating and revising text). Lower-level learners primarily used the chatbot during Planning to generate vocabulary and brainstorm ideas, while higher-level learners mainly used it during Translating to elaborate ideas and refine language; both groups also used it in Reviewing for feedback and revision.",
		"writing_genre (写作体裁)": "Argumentative or discursive essays modeled on IELTS Writing Task 2 prompts.",
		"writing_task_type (写作任务类型)": "Timed essay writing tasks based on IELTS Writing Task 2 (pretest and posttest, 40 minutes each) and weekly home essay assignments on provided topics, requiring students to present and develop arguments with appropriate organization and support.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "The GPT-3.5 Turbo based chatbot functioned as an interactive writing assistant: it generated and expanded ideas, supplied topic-related vocabulary and synonyms, suggested deeper explanations and examples, refined wording for clarity and diversity, provided feedback on content, organization, vocabulary, and language use, answered questions, and assisted at different stages of the writing process; it was not used for automatic scoring or grading.",
		"role_instructor (教师角色与介入方式)": "The same English teacher instructed all participants in class sessions, administered the pretest and posttest, introduced and guided students in using WAB, and later scored the timed-writing tasks along with another teacher. In-class instruction continued as usual, while writing practice with the chatbot was done at home; the teacher did not mediate chatbot responses in real time but was suggested in the implications to provide explicit guidance on crafting effective prompts and critically evaluating AI outputs.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "High school EFL setting in a secondary school in Hanoi, Northern Vietnam; students attended regular English classes at school but used the WAB platform for writing practice at home via computers or mobile phones; the chatbot was available online for asynchronous, out-of-class writing support.",
		"ethical_consideration (伦理审查与知情同意)": "The study adhered to ethical standards: informed consent was obtained via a signed form detailing objectives, procedures, benefits, and potential risks. Students were informed that the chatbot might occasionally make errors that could cause minor frustration or confusion, but these risks were considered manageable. Participation was voluntary, with the right to withdraw at any time without penalty. Anonymity was ensured using pseudonyms, and all data (tests, questionnaires, chat histories, recordings) were stored in password-protected files accessible only to the research team.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Students accessed WAB with personal accounts, received one weekly writing topic through the platform, and were expected to write one essay per week with chatbot support. They were explicitly informed that the chatbot could sometimes provide incorrect information and were encouraged to critically evaluate its suggestions rather than accept them uncritically. Usage occurred at home on their own time; there were no explicit restrictions on frequency of chatbot use beyond the weekly tasks, but students were cautioned against overreliance and reminded that the chatbot’s responses could contain errors.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "The article notes that students were informed that the chatbot might occasionally make mistakes and that they should critically assess its output; however, there is no mention of explicit technical safety features such as content filtering, toxicity blocking, or bias mitigation mechanisms built into the WAB interface.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "Chat log analysis showed that total chatting time and number of meaningful inquiries did not differ significantly between higher- and lower-level groups, but usage patterns differed: lower-level students predominantly used the chatbot during Planning (particularly for generating vocabulary and ideas), while higher-level students mainly used it during Translating to elaborate ideas and refine language; both groups focused heavily on vocabulary and content queries, with minor attention to mechanics. Wilcoxon signed-rank tests revealed significant pre–post improvements for both proficiency groups in all writing components (mechanics, language use, vocabulary, organization, content) and in overall writing scores, indicating that integrating the GPT-3.5 Turbo based chatbot enhanced writing performance. Questionnaire results showed that both groups perceived the chatbot as useful, with no significant difference in perceived usefulness, but higher-level students rated perceived ease-of-use significantly higher than lower-level students. Open-ended responses and interviews indicated that students valued prompt and idea generation, interactive and detailed feedback, and vocabulary enhancement; they viewed the chatbot as accessible, versatile, and a powerful supplementary writing tutor. Some challenges were reported, including initial difficulty formulating effective prompts, lower-level learners’ occasional confusion and overload from lengthy or complex responses, technical issues (lag, login, chat history not retained), and individual concerns about potential overreliance on the chatbot for ideas and language."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness of the GPT-3.5 Turbo based WAB was evaluated using pretest–posttest timed-writing scores (IELTS Task 2, Jacobs rubric), chat log based usage metrics and coded inquiries, a TAM-based perception questionnaire, and qualitative data from open-ended responses and interviews. Both higher- and lower-level students showed significant improvements in all writing components and overall scores after the 11-week intervention, reported positive perceptions of usefulness and ease-of-use, and described the chatbot as a versatile and helpful tool for planning, translating, and reviewing their writing.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Timed-writing pretest and posttest tasks (40-minute IELTS Writing Task 2 essays) scored using Jacobs’ (1981) writing rubric assessing mechanics, language use, vocabulary, organization, and content, rated by two experienced teachers.",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Writing performance was analyzed across five dimensions: content (relevance and development of arguments), organization (structure and coherence), vocabulary (range and appropriateness of word choice), language use (grammar and syntax), and mechanics (spelling, punctuation, capitalization, formatting), as well as overall writing score.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Students’ perceptions and attitudes toward the AI chatbot were measured with a 7-point Likert-scale questionnaire based on the Technology Acceptance Model (10 items on perceived usefulness and 6 items on perceived ease-of-use, plus two open-ended questions) and supplemented with semi-structured interviews exploring experiences, satisfaction, and perceived helpfulness.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Perceived usefulness of the chatbot for accomplishing writing tasks more quickly and effectively, perceived ease-of-use and simplicity of interaction with the chatbot, overall satisfaction, feelings of confidence and support during writing, perceived helpfulness and practicality, and user experience in terms of interface and responsiveness.",
		"cognitive_aspect_measure (认知因素测量工具)": "Content analysis and coding of meaningful chat inquiries according to Flower and Hayes’ (1981) writing stages (Planning, Translating, Reviewing) and Jacobs’ (1981) writing aspects (content, organization, vocabulary, language use, mechanics) to capture learners’ focus of attention and cognitive engagement at different stages of the writing process.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Writers’ cognitive focus and strategies at different writing stages, such as generating ideas and vocabulary in Planning, elaborating and refining ideas and expressions in Translating, and seeking feedback and revising in Reviewing; differential strategic use of the chatbot by proficiency level (lower-level learners relying on it mainly for planning and vocabulary comprehension; higher-level learners using it more for translation refinement, synonym selection, and coherence).",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "WAB platform chat logs containing total chatting time, number of meaningful inquiries per student, and coded categories of inquiries (by writing stage and writing aspect); these were analyzed via descriptive statistics, Mann–Whitney U tests, and content analysis.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Frequency and distribution of chatbot usage across writing stages (Planning, Translating, Reviewing), frequency of queries related to vocabulary, content, organization, language use, and mechanics, overall engagement levels (total chat time, number of meaningful inquiries), and differences in usage patterns between higher- and lower-level students, showing distinct interaction modes and focus even with similar overall engagement.",
		"other_outcomes_measure (其他结果测量工具)": "Open-ended questionnaire items about favorite features and improvement needs and semi-structured interviews with 10 students analyzed using thematic analysis to identify perceived benefits, strategies, difficulties, and suggestions.",
		"other_outcomes_focus (其他结果维度说明)": "Perceived strengths of the chatbot (prompt generation and idea inspiration, interactive and detailed feedback, vocabulary enhancement, accessibility and immediacy of support), reported technical and design issues (lagging, login problems, lack of chat history retention, non-updated information), difficulties for some lower-level learners in understanding long or complex responses, and user suggestions (saving chat history, citing information sources, regularly updating knowledge, adjusting response complexity to proficiency level).",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pretest writing assessment and WAB training in week 1; continuous chat log collection during weeks 2–9; posttest writing assessment and perception questionnaire in week 10; semi-structured interviews in week 11; no delayed follow-up test.",
		"primary_outcome_variables (主要结果变量_因变量)": "Primary quantitative outcomes were pretest and posttest writing scores on mechanics, language use, vocabulary, organization, content, and overall writing score for each proficiency group; additional outcomes included perceived usefulness and perceived ease-of-use scores from the questionnaire and usage metrics from chat logs (chatting time, number and categories of meaningful inquiries).",
		"independent_variables_and_factors (自变量与实验因素)": "Proficiency group (higher-level vs lower-level writers) as a between-group factor; time (pretest vs posttest) as a within-subject factor; chatbot usage was constant across groups, with differences examined in usage patterns and perceptions rather than in exposure vs non-exposure to the chatbot.",
		"followup_length_and_type (随访时长与类型)": "No long-term follow-up; outcomes were measured immediately after the 8-week writing practice phase (posttest, questionnaire, interviews within the 11-week study period).",
		"statistical_significance (统计显著性结果摘要)": "Mann–Whitney U tests showed no significant differences between higher- and lower-level groups in total chatting time (U=248.50, p=.572, r=.08) or number of meaningful inquiries (U=220.00, p=.241, r=.17). Wilcoxon signed-rank tests indicated significant pre–post gains in all writing aspects for both groups: for the lower-level group, mechanics z=−3.500, p=.000; language use z=−2.959, p=.003; vocabulary z=−3.505, p=.000; organization z=−3.075, p=.002; content z=−3.685, p=.000; overall score z=−3.990, p=.000. For the higher-level group, mechanics z=−2.236, p=.025; language use z=−2.627, p=.009; vocabulary z=−3.611, p=.000; organization z=−3.183, p=.001; content z=−3.461, p=.001; overall score z=−4.162, p=.000. For perceptions, Mann–Whitney U tests showed no significant group difference in perceived usefulness (U=228.50, p=.481, r=.10) but a significant difference in perceived ease-of-use favoring the higher-level group (U=237.00, z=−2.061, p=.039, r≈.30).",
		"effect_size_summary (效应量摘要)": "Reported effect sizes include non-parametric r values for Mann–Whitney tests (e.g., r≈.08 for total chatting time, r≈.17 for meaningful inquiries, r≈.30 for perceived ease-of-use), indicating a small to medium effect for the difference in perceived ease-of-use between proficiency groups; effect sizes for Wilcoxon tests are not explicitly reported, but the consistently significant z-values across all writing components and overall scores suggest meaningful improvements for both proficiency groups.",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "No explicit cases of academic dishonesty or plagiarism related to chatbot use are reported, but one higher-level student expressed concern about becoming overly reliant on the chatbot for ideas and vocabulary, feeling it became difficult to write independently, and accordingly reduced chatbot use to restore balance. Several lower-level students reported feeling overwhelmed by the length and complexity of some responses and confused by too many ideas or advanced vocabulary. Students occasionally experienced frustration due to technical issues (lagging, login problems) or misunderstanding the chatbot’s suggestions; the researchers also cautioned that the chatbot may provide erroneous information, underscoring the need for critical evaluation of its outputs.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "The study’s central comparison was between higher- and lower-level writing proficiency subgroups, both using the same chatbot. Both groups improved significantly in all writing components and overall scores, with no quantitative evidence that one group benefited more than the other in performance gains. However, usage patterns differed: lower-level learners relied more on the chatbot during Planning and for vocabulary translation and idea generation, while higher-level learners used it more during Translating for elaboration and synonym selection and showed more balanced use across stages. Perception data showed similar perceived usefulness but significantly higher perceived ease-of-use among higher-level students. The study did not examine equity by gender, socioeconomic status, or other demographic factors.",
		"limitation (研究局限)": "Limitations include the small sample size (N=47) drawn from a single high school in Northern Vietnam, limiting generalizability; absence of a non-chatbot control group, which prevents isolating the chatbot’s effects from other influences such as normal writing development or classroom instruction; relatively short intervention duration (11 weeks) that does not address long-term sustainability of improvements or continued chatbot use; and lack of comparison with other writing support tools such as grammar checkers or online tutoring platforms. The study focused exclusively on the WAB, so its relative effectiveness and user preference compared with alternative tools remain unknown.",
		"challenge (实施挑战与风险)": "Implementation challenges and risks included students’ initial difficulty in formulating precise and effective prompts to obtain appropriate chatbot responses, lower-level learners’ struggles in understanding long or complex outputs and managing cognitive load from numerous ideas and advanced vocabulary, technical issues such as lagging responses, login difficulties, and lack of chat history retention, and concerns about potential overreliance on the chatbot that might impede independent writing; additionally, the non-randomized, single-site design and lack of a control group pose methodological challenges for causal inference.",
		"future_work (未来研究方向)": "Future research should include larger and more diverse samples across different schools and contexts, incorporate a control group or alternative treatment groups to better isolate chatbot effects, extend the duration of interventions to examine long-term sustainability and continued engagement, and conduct comparative studies between WAB-like chatbots and other writing support tools (e.g., grammar checkers, online tutors). It is also suggested to improve chatbot design so that responses are more adaptive to learners’ proficiency levels (simplified language and bilingual support for lower-level learners, more nuanced feedback for advanced learners), to allow saving and reviewing chat history, to regularly update information and incorporate source citation or traceability, and to further integrate explicit instruction on prompt formulation and critical evaluation of AI suggestions.",
		"implication (理论与教学实践启示)": "The study implies that a GPT-3.5 Turbo based chatbot can serve as a versatile writing companion that supports EFL learners across planning, translating, and reviewing stages, enhancing writing performance in content, organization, vocabulary, language use, and mechanics for both higher- and lower-level students. For educators, chatbot usage data can highlight common writing challenges at different proficiency levels (e.g., planning support for lower-level learners and translation refinement for higher-level learners), informing targeted instruction and prompting teachers to provide explicit guidance on crafting prompts and evaluating AI feedback. For students, the chatbot offers flexible, on-demand support for generating ideas, expanding vocabulary, and revising texts but should be used strategically and critically to avoid overreliance. For AI developers, findings underscore the importance of adaptive response complexity, user-friendly interfaces, chat history preservation, timely updates, and mechanisms to enhance trustworthiness and usability, reinforcing the role of LLM-based chatbots as scaffolding tools within a social constructivist framework for EFL writing."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "Supported by the National Science and Technology Council, Taiwan (NSTC 113-2410-H-011-003-MY3) and the Empower Vocational Education Research Center of National Taiwan University of Science and Technology under the Featured Areas Research Center Program within the Higher Education Sprout Project by the Ministry of Education in Taiwan.",
		"conflict_of_interest (利益冲突声明)": "The authors declared no potential conflicts of interest with respect to the research, authorship, or publication of the article.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "There was no random assignment; participants were grouped by writing proficiency based on IELTS pretest scores, and all received the same chatbot intervention. The lack of randomization and absence of a non-AI control group increase the risk of selection bias and limit causal interpretations of the chatbot’s effects.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "Blinding procedures are not reported; it is not specified whether raters of writing tests were blind to pretest or posttest status or to proficiency group membership, and participants necessarily knew they were using a chatbot, so both performance and detection bias cannot be ruled out.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The study reports 47 participants with no mention of attrition, dropout, or missing data handling; analyses appear to be based on the full sample of 25 higher-level and 22 lower-level students, but explicit reporting of retention and data completeness is absent.",
		"reporting_transparency (报告透明度与可重复性)": "The article clearly reports context, participant characteristics, grouping criteria, intervention duration and procedure, details of the WAB functions, measurement instruments, reliability coefficients, and main statistical tests with test statistics and p-values, and includes tables summarizing chat log usage and writing scores. Qualitative analysis procedures, coding, and thematic findings are described, and supplemental material is indicated online; however, some implementation specifics (e.g., precise API configuration of GPT-3.5 Turbo) are not detailed.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The study specifies that WAB is built on the GPT-3.5 Turbo language model and describes the time frame (November 2023 to January 2024) and core platform functions (topic presentation, writing interface, chatbot assistance), but it does not provide details on model prompts, parameter settings, fine-tuning, or system messages, which constrains exact replication of the LLM behavior in future studies.",
		"baseline_equivalence (基线等同性检验)": "Baseline equivalence between proficiency groups was not tested because groups were explicitly formed based on differing IELTS writing pretest scores (higher-level vs lower-level); both groups were then tracked over time with the same chatbot intervention rather than compared as treatment and control groups.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The authors used the Shapiro–Wilk test to assess normality and, given the small sample size and non-normal distributions, chose non-parametric tests (Mann–Whitney U, Wilcoxon signed-rank). They report this rationale but do not provide detailed diagnostics for other assumptions or discuss outliers.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Data preprocessing included coding chat log inquiries according to predefined schemes for writing stages and aspects, computing total chatting time and counts of meaningful inquiries, and calculating writing component scores and overall scores using Jacobs’ rubric. No transformations of variables or formal sensitivity analyses are reported; qualitative data were transcribed, translated into English, and coded using content and thematic analysis procedures."
	}
}