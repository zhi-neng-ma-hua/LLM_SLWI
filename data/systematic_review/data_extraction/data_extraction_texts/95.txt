{
	"Basic Identification": {
		"author (作者)": "Dunming (Jason) Lu and Yutian Zeng",
		"publication_year (发表年份)": "2025",
		"study_region (研究地区)": "Southeast China (mainland Chinese high school)",
		"journal_name (期刊名称)": "Innovation in Language Learning and Teaching",
		"study_type (研究类型_期刊论文_会议论文等)": "Journal article; quasi-experimental pretest–post-test study with three groups and mixed quantitative–qualitative analysis",
		"doi_or_identifier (DOI或唯一标识)": "10.1080/17501229.2025.2525341",
		"research_aims (研究目的与问题)": "To investigate the usefulness of ChatGPT-generated model texts as a feedback instrument by examining (1) whether ChatGPT-generated model texts significantly enhance mainland Chinese EFL students’ text quality in rewritten argumentative essays and (2) how these students perceive the use, effectiveness, strengths and weaknesses of ChatGPT-generated model texts as a feedback instrument.",
		"research_gap_or_novelty (研究创新性与知识空白)": "Addresses gaps in existing research where ChatGPT has been examined mainly as a written corrective feedback tool and where model-text feedback research (MTFI) has rarely used AI or been conducted outside European contexts; provides the first empirical evidence on ChatGPT-generated model texts as a feedback instrument for EFL writing, comparing them directly with teacher-generated models and no-feedback conditions in a mainland Chinese high school setting."
	},
	"Participant Information": {
		"educational_level (教育阶段)": "High school Grade 11 (upper secondary)",
		"language_proficiency (语言熟练度水平)": "English proficiency ranged from B1 to B2 on the CEFR scale as determined by the Oxford Placement Test (OPT); one-way ANOVA on OPT scores showed no significant proficiency differences among the three classes (F(2,101) = 1.292, p = .279).",
		"mother_tongue (母语)": "Mandarin or Cantonese as first language",
		"sex (性别)": "55 males, 49 females",
		"age (年龄)": "16–17 years old",
		"learning_context (学习语境_ESL_EFL_ELL等)": "EFL context in a mainland Chinese high school",
		"target_language (目标语言)": "English (L2)",
		"discipline (学科背景)": "General secondary education; high school English writing classes",
		"prior_experience_llm (既有LLM使用经验)": "All participants had prior experience with both human-authored and AI-generated model texts as a feedback instrument in their regular writing instruction; prior experience with ChatGPT specifically was not detailed."
	},
	"Methodology": {
		"research_design (研究设计_实验_准实验_纵向等)": "Pretest–post-test quasi-experimental design with three groups (control group with no model, experimental group 1 with teacher-generated model text, experimental group 2 with ChatGPT-generated model text) across a three-stage writing task: composing (pretest), comparing with model (feedback), and rewriting (post-test).",
		"research_method (研究方法_定量_定性_混合)": "Mixed methods, combining quantitative analyses of writing scores and questionnaire responses with qualitative content analysis of open-ended questionnaire answers and textual analysis of students’ draft–rewrite changes.",
		"sampling_method (抽样方法)": "Convenience sampling of three intact Grade 11 classes at one high school in Southeast China; all were EFL learners with similar backgrounds; one-way ANOVA confirmed no significant differences in English proficiency across classes before group assignment; classes were then randomly assigned to CG, EG1, and EG2.",
		"sample_size_and_effect (样本量及效应量)": "Total N = 104 Grade 11 EFL students; CG n = 35, EG1 n = 33, EG2 n = 36. Pretest total writing scores were similar (e.g. CG M = 8.31, SD = 1.68; EG1 M = 8.58, SD = 1.76; EG2 M = 8.90, SD = 1.92). Mixed ANOVA showed significant time × group interaction effects for total score (F = 43.795, p < .001, partial η² = .464), content (F = 14.969, p < .001, partial η² = .229), organisation (F = 43.435, p < .001, partial η² = .244), vocabulary (F = 12.779, p < .001, partial η² = .202), and grammar (F = 14.992, p < .001, partial η² = .229). Post-test pairwise comparisons indicated that both EG1 and EG2 significantly outperformed CG on all dimensions with large effect sizes (e.g. total score CG–EG1 Cohen’s d ≈ 1.39, CG–EG2 d ≈ 1.71), while differences between EG1 and EG2 were small and non-significant. Within-group improvements from pretest to post-test were large for EG1 (total score d = 1.92) and EG2 (total score d = 1.61), but non-significant for CG (p = .396, d = 0.42).",
		"theoretical_foundation (理论基础_理论框架)": "Grounded in the Output Hypothesis (Swain 1985, 2000) and the Noticing Hypothesis (Schmidt 2001), where learners identify gaps between their own output and model texts, notice target features, and incorporate them in subsequent revisions; model texts are conceptualised as positive evidence feedback (PEF) that supports form–meaning mapping and L2 development.",
		"data_collection_instrument (数据收集工具)": "Oxford Placement Test (OPT) to determine English proficiency; one argumentative writing task on whether environmental problems are too big for individuals to solve, used for both pretest and post-test (40 minutes each); one teacher-generated model text (Model 1) and one ChatGPT-generated model text (Model 2) aligned with the same prompt; comparison sheet for EG1 and EG2 to record similarities/differences and useful features; analytical writing rubric adapted from Kang (2024) rating content, organisation, vocabulary, and grammar (0–5 per dimension, summed to total); questionnaire with Likert-scale items adapted from Nguyen and Vu (2024) on perceived usefulness of the model for content, organisation, vocabulary, and grammar plus two open-ended questions on strengths/weaknesses and preference for teacher vs ChatGPT model texts.",
		"data_collection_validity_reliability (工具信度与效度)": "The writing task and topic were pilot-tested with another Grade 11 group at the same school to ensure appropriateness and to elicit preferred stance. Both model texts were pilot-tested with 20 similar students who rated comprehensibility on a 7-point Likert scale, showing suitable difficulty (M = 3.62, SD = 1.15 for the teacher model; M = 3.77, SD = 1.25 for the ChatGPT model). The analytical rubric was adopted from prior work (Kang 2024). Two authors independently rated all writing scripts; inter-rater reliability (Pearson r) was 0.89 for first drafts and 0.93 for rewritten essays. The questionnaire items were adapted from a validated instrument (Nguyen and Vu 2024). Content analysis of open-ended responses used a conventional inductive approach with two coders; intercoder agreement was substantial with Cohen’s kappa k = 0.84.",
		"data_analysis_method (数据分析方法)": "Descriptive statistics for writing scores and questionnaire responses; one-way ANOVA to check initial proficiency equivalence; mixed-design ANOVA (time: pretest vs post-test; group: CG vs EG1 vs EG2) for total writing scores and four analytical dimensions, with assumptions of normality, homogeneity of variance, and sphericity checked and partial eta squared calculated as effect size; post-hoc simple-effect analyses and paired-sample t-tests within groups with Bonferroni corrections and Cohen’s d reported; content analysis of open-ended questionnaire responses via inductive coding into themes of strengths, weaknesses, and comparisons of model types; textual analysis of EG2’s pretest and post-test essays to identify specific incorporations of model-derived features in content, organisation, vocabulary, and grammar with independent review by two researchers.",
		"unit_of_analysis (分析单位)": "Individual student texts and scores for quantitative analyses; individual students’ responses and essays for qualitative content and textual analyses; class/group (CG, EG1, EG2) used as between-subjects factor in ANOVA.",
		"group_assignment_method (组别分配方式_随机_非随机等)": "Three intact Grade 11 classes were first confirmed to have no significant differences in OPT proficiency; then one class was randomly assigned as the control group, one as EG1 (teacher-generated model), and one as EG2 (ChatGPT-generated model).",
		"power_analysis (功效分析与样本量论证)": "NR",
		"sampling_frame_and_representativeness (抽样框与样本代表性)": "Sampling frame consisted of three intact Grade 11 EFL classes from a single high school in Southeast China; authors did not explicitly discuss representativeness or generalisability beyond this context.",
		"scoring_procedure_and_rater_training (评分流程与评分者培训)": "Two authors independently scored all pretest and post-test essays using the analytical rubric (0–5 for each of content, organisation, vocabulary, grammar, summed to a total score); Pearson correlations of 0.89 (first drafts) and 0.93 (rewritten essays) indicated high inter-rater reliability; the final score for each participant was the average of the two raters’ scores; specific rater training sessions beyond authors’ expertise and prior agreement on rubric use were not reported."
	},
	"Intervention": {
		"duration (干预时长与频率)": "Four-week sequence: Week 1 information session, consent, and OPT; Week 2 Stage 1 pretest writing task (40 minutes) and self-reported writing problems; Week 3 Stage 2 comparison activity (30 minutes) with model texts for EG1 and EG2 and self-review without feedback for CG; Week 4 Stage 3 rewriting on the same prompt as post-test (40 minutes) plus questionnaire for EG2.",
		"llm_model_type (LLM模型类型_如ChatGPT_GPT4_Gemini等)": "ChatGPT (GPT-4o version)",
		"llm_model_configuration (LLM模型配置与版本_API_网页_参数等)": "The first author used the ChatGPT 4o version to generate a single argumentative model text via a contextual prompt specifying the pedagogical purpose (model for content, organisation, vocabulary, grammar), target learner profile (mainland Chinese Grade 11 students, CEFR B1–B2), topic, stance, and minimum length (150 words); ChatGPT access mode (web UI vs API), temperature or other parameters were not reported; no advanced prompt engineering techniques were intentionally used, to reflect typical teacher practice.",
		"llm_integration_mode (LLM整合模式_直接使用_教师中介_系统嵌入)": "Teacher-mediated integration: the researcher/teacher interacted with ChatGPT to generate a model text offline and provided the printed ChatGPT-generated model to EG2 students; learners did not directly use or converse with ChatGPT during the intervention.",
		"prompting_strategy (提示策略_提示工程与使用方式)": "A single contextual template prompt was used to elicit the ChatGPT model: the prompt instructed ChatGPT to write a model argumentative essay for learners to learn from in terms of content, organisation, vocabulary, and grammar; specified that target learners were mainland Chinese Grade 11 students with B1–B2 proficiency; stated the topic and required stance ('environmental problems are not too big for individuals to solve'); and requested at least 150 words; this prompt was chosen to emulate how practising EFL teachers with limited prompt-engineering training might typically formulate prompts; more complex or multi-step prompting strategies were not employed.",
		"training_support_llm_literacy (LLM素养与提示培训)": "No explicit training in ChatGPT use or prompt engineering was provided to students in this study; instead, the first author handled prompt creation and model generation; participants were already familiar with human- and AI-generated model texts as feedback instruments in regular instruction, but in the experiment the authorship of the model (teacher vs ChatGPT) was not disclosed during the comparison stage to avoid biasing student engagement.",
		"intervention_implementation (干预实施流程_步骤与任务)": "Stage 1 (Week 2): all three groups wrote an in-class 40-minute argumentative essay on whether environmental problems are too big for individuals to solve (pretest), and then briefly reported their writing problems. Stage 2 (Week 3): EG1 received their first draft, a teacher-generated model text (Model 1), and a comparison sheet; EG2 received their draft, the ChatGPT-generated model text (Model 2), and a comparison sheet; both experimental groups had 30 minutes to write similarities and differences between their writing and the model, and to identify useful features in the model in terms of content, organisation, vocabulary, and grammar, in English or Mandarin; the model’s authorship was not disclosed. The CG spent the same time reviewing their own drafts and was encouraged to identify and correct their own mistakes without any model or explicit feedback. Stage 3 (Week 4): all groups wrote a new version of the essay (rewritten essay/post-test) on the same prompt without any external support (no model texts or feedback); afterwards, EG2 completed a questionnaire (Likert items plus open-ended questions) about their perceptions of the ChatGPT-generated model text.",
		"experimental_group_intervention (实验组干预内容)": "EG1 (n = 33) received a teacher-generated model text written by their class teacher, revised by a native speaker, and reviewed by two other EFL teachers; EG2 (n = 36) received a ChatGPT-generated model text created via the contextual prompt with ChatGPT 4o; both EGs compared their own first drafts with the respective model, noted similarities/differences and useful features for content, organisation, vocabulary, and grammar, and then rewrote their essays without external support; EG2 additionally completed a perception questionnaire about the ChatGPT-generated model.",
		"control_group_intervention (对照组干预内容)": "The control group (CG, n = 35) completed the same pretest writing task and later the rewriting task but did not receive any model text or feedback between drafts; during Stage 2 they were simply encouraged to identify and correct their own mistakes on their first drafts within the same 30-minute time frame as the experimental groups.",
		"writing_stage (写作阶段_如生成_修改_反馈_重写等)": "The intervention targeted feedback and revision stages between initial composing and rewriting: students first composed an initial draft (generation), then received feedback via model texts (for EG1 and EG2) or self-review (CG), engaged in comparison and noticing, and finally rewrote their essays (revision and rewriting) without access to models.",
		"writing_genre (写作体裁)": "Argumentative essay",
		"writing_task_type (写作任务类型)": "Timed in-class argumentative essay task requiring at least 150 words on whether environmental problems are too big for individuals to solve.",
		"role_llm (LLM角色_如生成文本_给反馈_评分_对话等)": "ChatGPT’s role was to generate a single model argumentative essay that functioned as a model text feedback instrument (positive evidence feedback) for EG2; it provided an exemplar of content development, organisational structure, vocabulary use, and grammar, but did not directly give corrections, scores, or engage in interactive dialogue with students.",
		"role_instructor (教师角色与介入方式)": "Teachers and researchers designed the writing task, generated and vetted the teacher-written model text, created the prompt and used ChatGPT 4o to generate the AI model, pilot-tested the tasks and models, implemented the three-stage procedure in class, supervised writing and comparison tasks, ensured model authorship was not revealed during comparison, and scored the essays using the analytical rubric; they also conducted and coded the questionnaire and content analysis.",
		"setting (教学情境_学校类型_课程类型_线上线下)": "Face-to-face EFL writing instruction in three intact Grade 11 classes at a high school in Southeast China; tasks were completed in regular classroom settings under teacher supervision.",
		"ethical_consideration (伦理审查与知情同意)": "In Week 1, all participants were informed about the research and signed a consent form; they then completed the OPT; further details about institutional ethical review approval were not reported.",
		"llm_access_policy (LLM使用规范_允许与限制)": "Only the researchers interacted with ChatGPT to generate the model text for EG2; students did not access ChatGPT directly during the study; model authorship (teacher vs ChatGPT) was concealed during Stage 2 to prevent bias; participants had prior experience with AI-generated models in regular instruction, but in this experiment ChatGPT use was restricted to model generation by researchers.",
		"llm_safety_guardrails (LLM安全与内容过滤设置)": "No explicit technical safety or content-filter settings for ChatGPT were reported; potential issues such as bias, fabricated information, and contextual mismatch of AI-generated texts were discussed in the literature review and addressed conceptually rather than through specific system-level guardrails; model texts were pilot-tested for comprehensibility before use.",
		"key_findings (主要研究发现_与LLM写作干预相关)": "There were no significant differences between groups at pretest, but both experimental groups that received model texts (teacher-generated or ChatGPT-generated) significantly outperformed the control group on total text quality and all analytical dimensions (content, organisation, vocabulary, grammar) at post-test with large effect sizes. EG2, which received the ChatGPT-generated model, achieved the highest post-test scores, but differences between EG1 and EG2 were small and non-significant, indicating that ChatGPT-generated model texts were as effective as teacher-written models in improving text quality under comparable conditions. Textual analysis showed that EG2 students incorporated model-derived features such as additional elaboration and supporting evidence for arguments, richer and more topic-appropriate vocabulary, clearer multi-paragraph structure, and more complex grammatical constructions. Questionnaire data indicated that EG2 students generally perceived ChatGPT-generated model texts as helpful for improving overall writing and specific aspects (especially content and vocabulary), though they also reported concerns about potential constraints on creativity, formulaic and sometimes unnatural style, occasional difficulty with advanced vocabulary, and limited contextual and emotional depth compared with teacher-generated models."
	},
	"Outcome": {
		"application_effectiveness_overview (应用效果总体评价与测量工具)": "Effectiveness was evaluated by comparing pretest and post-test argumentative essays using an analytical rubric (content, organisation, vocabulary, grammar) and by collecting EG2 students’ perceptions via a Likert-scale questionnaire and open-ended questions; results showed that ChatGPT-generated model texts, when used as a feedback instrument between drafts, led to substantial and statistically significant gains in overall text quality and all analytic dimensions relative to a no-feedback control and performed comparably to teacher-generated models; students also reported generally positive perceptions of the ChatGPT models’ usefulness.",
		"writing_performance_measure (写作表现测量工具_量表或评分标准)": "Analytical writing rubric adopted from Kang (2024), assessing four dimensions—content, organisation, vocabulary, and grammar—each on a 0–5 scale, summed to a total score (0–20); essays were rated independently by two trained raters (the authors) with high inter-rater reliability (r = 0.89 for first drafts, r = 0.93 for rewritten essays).",
		"writing_performance_focus (写作表现关注维度_流利度_准确性_复杂度_体裁等)": "Text quality in terms of content (idea development, elaboration, evidence), organisation (paragraphing, logical sequencing, coherence), vocabulary (range, appropriacy, precision, topic-related lexis), and grammar (accuracy and variety of grammatical structures) in argumentative essays.",
		"affective_aspect_measure (情感因素测量工具_问卷_量表等)": "Questionnaire with 5-point Likert-scale items adapted from Nguyen and Vu (2024) assessing EG2 students’ perceived usefulness of the ChatGPT-generated model text for improving overall writing and specific aspects (content, organisation, vocabulary, grammar), plus two open-ended questions on strengths/weaknesses and preferences between teacher- and ChatGPT-generated models.",
		"affective_aspect_focus (情感因素维度_动机_态度_焦虑_自效感等)": "Students’ attitudes and perceptions regarding the usefulness and effectiveness of ChatGPT-generated model texts as feedback, including perceived help in clarifying reasoning, structuring ideas, enriching vocabulary, improving grammar, perceived convenience and efficiency, and concerns about unnatural style, lack of contextual relevance, and constraints on creativity.",
		"cognitive_aspect_measure (认知因素测量工具)": "No separate quantitative cognitive scale was administered; cognitive processes such as noticing and hypothesis-testing were inferred from students’ comparison sheets and from textual analysis of pretest and post-test essays for EG2, where revisions were cross-referenced with specific features in the ChatGPT-generated model text.",
		"cognitive_aspect_focus (认知因素维度_策略使用_元认知监控等)": "Learners’ noticing of gaps between their own writing and the model text in terms of content elaboration, organisational patterns, vocabulary use, and grammatical structures; subsequent hypothesis-testing and metalinguistic processing when incorporating model features into rewritten essays; understanding of how to develop arguments, provide illustrative examples, use topic-appropriate lexis, and employ more complex syntactic structures.",
		"behavioral_aspect_measure (行为因素测量工具_日志_平台日志等)": "Structured comparison sheets completed by EG1 and EG2 during the model comparison stage documenting similarities, differences, and useful features; analysis of changes between initial and rewritten essays; descriptive statistics on questionnaire responses reflecting engagement with the model text; no digital interaction logs with ChatGPT were collected since students did not directly use the tool.",
		"behavioral_aspect_focus (行为因素维度_使用频率_交互模式_坚持度等)": "Students’ engagement with the model texts during the comparison task (identifying useful features, articulating differences, and planning incorporations) and their revision behaviours as evidenced by the adoption of content, organisational, lexical, and grammatical features from the ChatGPT-generated model in their rewritten essays; potential tendencies toward relying on the model for ideas and language were also noted in students’ perceptions.",
		"other_outcomes_measure (其他结果测量工具)": "Open-ended responses in the questionnaire about perceived strengths and weaknesses of ChatGPT-generated model texts and comparisons with teacher-generated model texts; qualitative textual analysis of EG2 students’ initial and rewritten essays to illustrate specific model-based revisions and provide representative examples.",
		"other_outcomes_focus (其他结果维度说明)": "Students’ qualitative evaluations of ChatGPT-generated model texts (e.g. rich vocabulary, clarification of reasoning, provision of ideas, range of grammatical structures, convenience and efficiency) and perceived drawbacks (e.g. restriction of creativity, formulaic and monotonous language, occasional difficulty of vocabulary, limited emotional depth, illogical reasoning in some cases, need for advanced prompting skills); preferences for human vs AI models and suggestions for human–AI collaboration in generating model texts.",
		"assessment_timepoints (评估时间点_前测_后测_延迟测等)": "Pretest argumentative writing task in Week 2; model comparison and self-review activity in Week 3; post-test rewritten essay on the same prompt in Week 4; EG2 questionnaire administered after post-test; no delayed post-test or long-term follow-up.",
		"primary_outcome_variables (主要结果变量_因变量)": "Total analytical writing score and sub-scores for content, organisation, vocabulary, and grammar on pretest and post-test essays; EG2’s Likert-scale ratings of perceived usefulness of the ChatGPT-generated model text for overall writing, content, organisation, vocabulary, and grammar.",
		"independent_variables_and_factors (自变量与实验因素)": "Instructional condition with three levels: control group without model text, experimental group 1 with teacher-generated model text, and experimental group 2 with ChatGPT-generated model text; time with two levels (pretest vs post-test); group × time interaction was of primary interest; learner-level variables such as proficiency were controlled through OPT and random assignment but not treated as factors in the main analyses.",
		"followup_length_and_type (随访时长与类型)": "No longitudinal follow-up; outcomes were assessed in a one-shot pretest–post-test design over approximately four weeks, focusing on immediate effects of model-text feedback on rewriting of the same prompt.",
		"statistical_significance (统计显著性结果摘要)": "Mixed ANOVA showed significant main effects of time and group and significant time × group interactions for total score and all four analytical dimensions. For total scores, the time × group interaction was F = 43.795, p < .001, partial η² = .464. For content, F = 14.969, p < .001, partial η² = .229; organisation, F = 43.435, p < .001, partial η² = .244; vocabulary, F = 12.779, p < .001, partial η² = .202; grammar, F = 14.992, p < .001, partial η² = .229. Post-test pairwise comparisons indicated that both EG1 and EG2 significantly outperformed CG on total and sub-dimension scores with p < .01 and large Cohen’s d values (e.g. total score CG–EG1 MD = −2.138, p < .001, d ≈ 1.39; CG–EG2 MD = −2.632, p < .001, d ≈ 1.71); differences between EG1 and EG2 were not statistically significant. Within-group comparisons showed significant pretest–post-test gains for EG1 and EG2 (e.g. total score EG1 d = 1.92, EG2 d = 1.61, p < .001) but not for CG (p = .396).",
		"effect_size_summary (效应量摘要)": "Time × group interaction effects for total and dimension scores were large, with partial η² values around .20–.46 (e.g. total partial η² = .464; content partial η² = .229; organisation partial η² = .244; vocabulary partial η² = .202; grammar partial η² = .229). Post-test pairwise comparisons between the control group and each experimental group showed large between-group effects (Cohen’s d > 0.80) for total and all dimension scores, while differences between EG1 and EG2 were small (d values around 0.08–0.58) and non-significant. Within-group effect sizes from pretest to post-test were large for EG1 (total d = 1.92) and EG2 (total d = 1.61) and small-to-moderate and non-significant for CG (d = 0.42).",
		"llm_misuse_or_negative_effects (LLM滥用或负向效应)": "Students reported that ChatGPT-generated model texts could constrain content development and creative thinking in rewriting, as the AI-produced models sometimes appeared too strong or complete, leading to over-reliance; participants also perceived the language of the AI models as formulaic, unnatural, and monotonous at times, with some illogical reasoning and a lack of emotional depth, life experience, and nuanced thought; some found certain vocabulary too difficult, and a few noted that ChatGPT’s models did not give explicit directions for improvement; the study raised concerns about contextual irrelevance and potential biases in AI-generated content, highlighting the need for critical evaluation and human oversight.",
		"equity_and_subgroup_effects (公平性与亚组差异)": "No subgroup analyses were conducted by gender, proficiency band, years of English study, or first language (Mandarin vs Cantonese), and the authors did not report equity-focused analyses or differential effects across demographic groups.",
		"limitation (研究局限)": "The authors note that only one model text per condition was used, which may not provide sufficiently rich input for noticing and incorporation; the study employed a one-shot experiment focused on rewriting the same prompt, which may not capture sustained or transferable effects; the writing task was limited to a single argumentative topic and genre, so findings may not generalise to other topics or genres (e.g. narrative, expository); the study relied on rubric-based scoring of text quality and did not include more fine-grained CAF (complexity, accuracy, fluency) measures; only a single contextual template prompt was used to generate the ChatGPT model text, which may not represent the full potential of advanced prompting strategies; and the sample was restricted to three intact classes in one high school, limiting generalisability.",
		"challenge (实施挑战与风险)": "Implementation challenges and risks include the potential of ChatGPT-generated model texts to limit learners’ creativity and content development by encouraging imitation rather than original thinking, the model’s stylistic rigidity and sometimes unnatural or formulaic language, occasional lack of contextual relevance to local test requirements and students’ prior knowledge and cultural background, the need for relatively advanced prompting skills to generate pedagogically appropriate model texts, and the risk that teachers or students might over-rely on AI and underuse human judgement and contextual knowledge; the authors argue for a human-in-the-loop approach where teachers critically review, refine, and adapt ChatGPT-generated models.",
		"future_work (未来研究方向)": "Future research directions include using more than one model text to provide richer input; conducting longitudinal or multi-session studies to examine the long-term and transferable effects of ChatGPT-generated model-text feedback; testing whether gains transfer to new prompts and different topics; extending research to other genres such as narrative and expository writing; employing additional measures such as CAF indices for more nuanced analyses of written output; exploring diverse prompting formats and scenarios (e.g. role-based, zero-shot) to improve ChatGPT’s performance and genre alignment; and using within-subject designs to compare learner engagement with human-authored and AI-generated model texts.",
		"implication (理论与教学实践启示)": "The study suggests that ChatGPT-generated model texts can function as effective positive evidence feedback instruments comparable to teacher-generated models for improving EFL learners’ text quality, broadening the conceptualisation of AI-assisted feedback beyond written corrective feedback; it implies that teachers can combine model-text feedback and error correction strategically, using models after initial drafting to support self-editing before providing WCF on revised texts; it highlights the value of human–AI collaboration in which teachers leverage ChatGPT to efficiently produce multiple, level-appropriate model texts while applying their pedagogical content knowledge to ensure contextual relevance, emotional resonance, and alignment with curricular and assessment requirements; it also underscores the need for transparency with students about AI involvement, explicit training in critically evaluating and appropriately using AI-generated models, and development of AI literacy among teachers so that model texts are tailored to learners’ zones of proximal development and do not undermine learner autonomy or creativity."
	},
	"Study Quality and Reporting": {
		"funding_source (经费来源)": "NR",
		"conflict_of_interest (利益冲突声明)": "The article explicitly states that no potential conflict of interest was reported by the authors.",
		"risk_of_bias_randomization (偏倚风险_分配与随机化)": "Randomisation was applied at the class level: after confirming no significant differences in English proficiency via OPT, three intact Grade 11 classes were randomly assigned to control, teacher-model, and ChatGPT-model conditions; this reduces but does not eliminate selection bias, as randomisation was not at the individual student level.",
		"risk_of_bias_blinding (偏倚风险_盲法)": "During the comparison stage, the authorship of model texts (teacher vs ChatGPT) was not disclosed to the experimental groups, limiting expectancy bias in learners’ engagement with the model; however, there is no indication that essay raters were blind to group assignment or timepoint, and teachers knew which classes belonged to which condition, so full blinding was not implemented.",
		"attrition_and_missing_data (流失与缺失数据处理)": "The article does not report attrition rates, exclusion of participants, or specific procedures for handling missing data, implying that attrition and missingness were minimal or not explicitly addressed.",
		"reporting_transparency (报告透明度与可重复性)": "The study provides detailed descriptions of context, participants, design, instruments (including the exact ChatGPT prompt and rating rubric dimensions), procedures, and statistical analyses, along with main descriptive and inferential statistics and effect sizes; model texts, rubrics, and additional details are referenced in appendices; content analysis procedures and intercoder reliability are reported; a data availability statement notes that data will be made available on request.",
		"preregistration_or_protocol (预注册或研究方案)": "NR",
		"llm_version_reproducibility (LLM版本与可复现性)": "The authors specify that the ChatGPT 4o version was used and provide the exact prompt used to generate the model text, enhancing reproducibility of the AI component, though other configuration details (e.g. temperature settings, interface) are not reported and future updates to ChatGPT may affect exact replication.",
		"baseline_equivalence (基线等同性检验)": "Baseline equivalence was assessed with a one-way ANOVA on OPT proficiency scores across the three classes (F(2,101) = 1.292, p = .279), showing no significant differences; an additional one-way ANOVA on pretest writing scores indicated no significant differences among the three groups across the four dimensions and total scores, confirming comparable starting points.",
		"assumption_check_and_data_diagnostics (统计假设检验与数据诊断)": "The authors report that assumptions of normality, homogeneity of variance, and sphericity were checked and met for the mixed ANOVA analyses; specific diagnostic statistics or plots are not detailed.",
		"outlier_treatment_and_sensitivity_analysis (异常值处理与稳健性分析)": "NR",
		"data_preprocessing_and_transformation (数据预处理与转换)": "Writing scores from the analytical rubric were averaged across two raters to yield final scores; questionnaire responses were summarised with descriptive statistics; open-ended responses were transcribed and coded through inductive content analysis with intercoder reliability checks; textual analysis of EG2 essays involved identifying and categorising model-based revisions by dimension; no data transformations (such as scaling or normalisation) beyond basic scoring and averaging are reported."
	}
}